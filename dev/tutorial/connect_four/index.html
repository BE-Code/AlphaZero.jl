<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Learning to Play Connect Four · AlphaZero</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">AlphaZero</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Tutorial</span><ul><li><a class="tocitem" href="../alphazero_intro/">Introduction to AlphaZero</a></li><li class="is-active"><a class="tocitem" href>Learning to Play Connect Four</a><ul class="internal"><li><a class="tocitem" href="#Training-an-agent-on-your-machine-1"><span>Training an agent on your machine</span></a></li><li><a class="tocitem" href="#Training-iterations-1"><span>Training iterations</span></a></li><li><a class="tocitem" href="#Experimental-results-1"><span>Experimental results</span></a></li></ul></li></ul></li><li><span class="tocitem">Reference</span><ul><li><a class="tocitem" href="../../reference/params/">Training Parameters</a></li><li><a class="tocitem" href="../../reference/game_interface/">Game Interface</a></li><li><a class="tocitem" href="../../reference/mcts/">MCTS</a></li><li><a class="tocitem" href="../../reference/network/">Network Interface</a></li><li><a class="tocitem" href="../../reference/networks_library/">Networks Library</a></li><li><a class="tocitem" href="../../reference/player/">Players</a></li><li><a class="tocitem" href="../../reference/memory/">Memory Buffer</a></li><li><a class="tocitem" href="../../reference/environment/">Environment</a></li><li><a class="tocitem" href="../../reference/benchmark/">Benchmark</a></li><li><a class="tocitem" href="../../reference/reports/">Training Reports</a></li><li><a class="tocitem" href="../../reference/ui/">User Interface</a></li></ul></li><li><span class="tocitem">Contributing</span><ul><li><a class="tocitem" href="../../contributing/guide/">Contribution Guide</a></li><li><a class="tocitem" href="../../contributing/add_game/">Adding New Games</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorial</a></li><li class="is-active"><a href>Learning to Play Connect Four</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Learning to Play Connect Four</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/jonathan-laurent/AlphaZero.jl/blob/master/docs/src/tutorial/connect_four.md#L" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="connect_four-1"><a class="docs-heading-anchor" href="#connect_four-1">Learning to Play Connect Four</a><a class="docs-heading-anchor-permalink" href="#connect_four-1" title="Permalink"></a></h1><p>In this section, we discuss how to use <code>AlphaZero.jl</code> to train a <em>Connect Four</em> agent without any form of supervision or prior knowledge. Although the game has been <a href="https://connect4.gamesolver.org/">solved</a> exactly with Alpha-beta pruning using domain-specific heuristics and optimizations, it is still a great challenge for reinforcement learning.<sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup></p><h2 id="Training-an-agent-on-your-machine-1"><a class="docs-heading-anchor" href="#Training-an-agent-on-your-machine-1">Training an agent on your machine</a><a class="docs-heading-anchor-permalink" href="#Training-an-agent-on-your-machine-1" title="Permalink"></a></h2><p>To run the experiments in this tutorial, we recommend having a CUDA compatible GPU with 4GB of memory or more. A 2GB GPU should work fine but you may have to reduce batch size. Each training iteration took about one hour and a half on a standard desktop computer with an Intel Core i5 9600K processor and an 8GB Nvidia RTX 2070 GPU.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>To get optimal performances, it is also recommended to use <code>AlphaZero.jl</code> with Julia 1.5 (nightly), which includes a <a href="https://github.com/JuliaLang/julia/pull/33448">critical feature</a> that enables <code>CuArrays</code> to force incremental GC collections.</p></div></div><p>To download <code>AlphaZero.jl</code> and start a new training session, just run the following:</p><pre><code class="language-sh">git clone https://github.com/jonathan-laurent/AlphaZero.jl.git
cd AlphaZero.jl
julia --project -e &quot;import Pkg; Pkg.instantiate()&quot;
julia --project --color=yes scripts/alphazero.jl --game connect-four train</code></pre><p>Instead of using <code>scripts/alphazero.jl</code>, one can also run the following into the Julia REPL:</p><pre><code class="language-julia">ENV[&quot;CUARRAYS_MEMORY_POOL&quot;] = &quot;split&quot;

using AlphaZero

include(&quot;games/connect-four/main.jl&quot;)
using .ConnectFour: Game, Training

const SESSION_DIR = &quot;sessions/connect-four&quot;

session = AlphaZero.Session(
    Game,
    Training.Network{ConnectFour.Game},
    Training.params,
    Training.netparams,
    benchmark=Training.benchmark,
    dir=SESSION_DIR)

resume!(session)</code></pre><p>The first line configures CuArrays to use a splitting memory pool, which performs better than the default binned pool on AlphaZero&#39;s workload as it does not require to run the garbage collector as frequently. Then, a new AlphaZero <a href="../../reference/ui/#ui-1">session</a> is created with the following arguments:</p><table><tr><th style="text-align: left">Argument</th><th style="text-align: left">Description</th></tr><tr><td style="text-align: left"><code>Game</code></td><td style="text-align: left">Game type, which implements the <a href="../../reference/game_interface/#game_interface-1">game interface</a>.</td></tr><tr><td style="text-align: left"><code>Training.Network</code></td><td style="text-align: left">Network type, which implements the <a href="../../reference/network/#network_interface-1">network interface</a>.</td></tr><tr><td style="text-align: left"><code>Training.params</code></td><td style="text-align: left">AlphaZero <a href="../../reference/params/#params-1">parameters</a>.</td></tr><tr><td style="text-align: left"><code>Training.netparams</code></td><td style="text-align: left">Network <a href="../../reference/networks_library/#conv_resnet-1">hyperparameters</a>.</td></tr><tr><td style="text-align: left"><code>Training.benchmark</code></td><td style="text-align: left"><a href="../../reference/benchmark/#benchmark-1">Benchmark</a> that is run between training iterations.</td></tr><tr><td style="text-align: left"><code>SESSION_DIR</code></td><td style="text-align: left">Directory in which all session files are saved.</td></tr></table><p>The <code>ConnectFour.Training</code> module contains some default parameters and benchmarks for the Connect Four game. Its content can be examined in file <code>games/connect-four/params.jl</code>. We include it below for reference but each part will be discussed specifically in the rest of this tutorial.</p><h3 id="Full-training-configuration-1"><a class="docs-heading-anchor" href="#Full-training-configuration-1">Full training configuration</a><a class="docs-heading-anchor-permalink" href="#Full-training-configuration-1" title="Permalink"></a></h3><pre><code class="language-julia">Network = ResNet

netparams = ResNetHP(
  num_filters=64,
  num_blocks=7,
  conv_kernel_size=(3, 3),
  num_policy_head_filters=32,
  num_value_head_filters=32,
  batch_norm_momentum=0.1)

self_play = SelfPlayParams(
  num_games=4_000,
  reset_mcts_every=100,
  mcts=MctsParams(
    use_gpu=true,
    num_workers=64,
    num_iters_per_turn=600,
    cpuct=2.0,
    temperature=StepSchedule(
      start=1.0,
      change_at=[10],
      values=[0.5]),
    dirichlet_noise_ϵ=0.25,
    dirichlet_noise_α=1.0))

arena = ArenaParams(
  num_games=200,
  reset_mcts_every=nothing,
  flip_probability=0.5,
  update_threshold=0.1,
  mcts=MctsParams(
    self_play.mcts,
    temperature=StepSchedule(0.1),
    dirichlet_noise_ϵ=0.05))

learning = LearningParams(
  use_position_averaging=true,
  samples_weighing_policy=LOG_WEIGHT,
  batch_size=2048,
  loss_computation_batch_size=2048,
  optimiser=Adam(lr=1e-3),
  l2_regularization=1e-4,
  nonvalidity_penalty=1.,
  min_checkpoints_per_epoch=1,
  max_batches_per_checkpoint=1000,
  num_checkpoints=2)

params = Params(
  arena=arena,
  self_play=self_play,
  learning=learning,
  num_iters=80,
  ternary_rewards=true,
  use_symmetries=true,
  memory_analysis=MemAnalysisParams(
    num_game_stages=4),
  mem_buffer_size=PLSchedule(
  [      0,        60],
  [400_000, 2_000_000]))

baselines = [
  Benchmark.MctsRollouts(
    MctsParams(
      arena.mcts,
      num_iters_per_turn=1000,
      cpuct=1.)),
  Benchmark.MinMaxTS(depth=5, τ=0.2)]

make_duel(baseline) =
  Benchmark.Duel(
    Benchmark.Full(arena.mcts),
    baseline,
    num_games=200,
    flip_probability=0.5,
    color_policy=CONTENDER_WHITE)

benchmark = make_duel.(baselines)</code></pre><p>Note that, in addition to having standard keyword constructors, parameter types have constructors that implement the <em>record update</em> operation from functional languages. For example, <code>Params(p, num_iters=100)</code> builds a <code>Params</code> object that is identical to <code>p</code> for every field, except <code>num_iters</code> which is set to <code>100</code>.</p><h3 id="Session-initialization-1"><a class="docs-heading-anchor" href="#Session-initialization-1">Session initialization</a><a class="docs-heading-anchor-permalink" href="#Session-initialization-1" title="Permalink"></a></h3><p>After launching a training session for the first time, you should see the following:</p><p><img src="../../assets/img/ui-init.png" alt="Session CLI (init)"/></p><p>Before the first training iteration and between each iteration, the current AlphaZero agent is benchmarked against some baselines in a series of games (200 in this case) so as to provide a concrete measure of training progress. In this tutorial, we use two baselines:</p><ul><li>A <strong>vanilla MCTS</strong> baseline that uses rollouts to estimate the value of new nodes.</li><li>A <strong>minmax baseline</strong> that plans at depth 5 using a handcrafted heuristic.</li></ul><p>Comparing two deterministic players is challenging as deterministic players will always play the same game repeatedly given a unique initial state. To add randomization, players are instantiated with a small but nonzero move selection temperature. Nonetheless, the minmax baseline is guaranteed to play a winning move if it sees one (see <a href="../../reference/benchmark/#AlphaZero.MinMax.Player"><code>MinMax.Player</code></a>).</p><p>The <code>redundancy</code> indicator is helpful to diagnose a lack of randomization. It measures the quantity <span>$1 - n_u / n_d$</span> where <span>$n_u$</span> is the total number of unique states that have been encountered (excluding the initial state) and <span>$n_d$</span> is the total number of encountered states, excluding the initial state and counting duplicates (see   <a href="../../reference/benchmark/#AlphaZero.Benchmark.DuelOutcome"><code>Benchmark.DuelOutcome</code></a>).</p><div class="admonition is-info"><header class="admonition-header">On leveraging symmetries</header><div class="admonition-body"><p>Another trick that we use to add randomization is to leverage the symmetry of the Connect Four board with respect to its central vertical axis: at each turn, the board is <em>flipped</em> along its central vertical axis with a fixed probability (see <a href="tutorial/@ref"><code>flip_probability</code></a>).</p><p>This is one of two ways in which <code>AlphaZero.jl</code> takes advantage of board symmetries, the other one being data augmentation (see <a href="../../reference/params/#AlphaZero.Params"><code>use_symmetries</code></a>). Board symmetries can be declared for new games by overriding <a href="../../reference/game_interface/#AlphaZero.GameInterface.symmetries"><code>GameInterface.symmetries</code></a>.</p></div></div><p>As you can see, the AlphaZero agent can win a few games with a randomly initialized network, by relying on search alone for very short term tactical decisions.</p><h2 id="Training-iterations-1"><a class="docs-heading-anchor" href="#Training-iterations-1">Training iterations</a><a class="docs-heading-anchor-permalink" href="#Training-iterations-1" title="Permalink"></a></h2><p>After the initial benchmarks are done, the first training iteration can start. Each training iteration took between 60 and 90 minutes on our hardware. The first iterations are typically on the shorter end, as games of self-play terminate more quickly and the memory buffer has yet to reach its final size.</p><p><img src="../../assets/img/ui-first-iter.png" alt="Session CLI (first iteration)"/></p><p>Each training iteration is decomposed into a <strong>self-play phase</strong> and a <strong>learning phase</strong>. During the self-play phase, the AlphaZero agent plays a series of 4000 games against itself, running 600 MCTS simulations for each move.<sup class="footnote-reference"><a id="citeref-2" href="#footnote-2">[2]</a></sup> Doing so, it records training samples in the memory buffer. Then, during the learning phase, the neural network is updated to fit data in memory. The current neural network is evaluated periodically against the best one seen so far, and replaces it to generate self-play data if it achieves a sufficiently high win rate. For more details, see <a href="../../reference/params/#AlphaZero.SelfPlayParams"><code>SelfPlayParams</code></a>, <a href="../../reference/params/#AlphaZero.LearningParams"><code>LearningParams</code></a> and <a href="../../reference/params/#AlphaZero.ArenaParams"><code>ArenaParams</code></a>.</p><p>At the end of every iteration, benchmarks are run, summary plots are generated and the state of the current environment is saved on disk. This way, if training is interrupted for any reason, it can be resumed from the last saved state by simply rerunning <code>scripts/alphazero.jl</code>.</p><h2 id="Experimental-results-1"><a class="docs-heading-anchor" href="#Experimental-results-1">Experimental results</a><a class="docs-heading-anchor-permalink" href="#Experimental-results-1" title="Permalink"></a></h2><p><img src="../../assets/img/connect-four/training/benchmark_won_games.png" alt="Win rate evolution (AlphaZero)"/> <img src="../../assets/img/connect-four/net-only/benchmark_won_games.png" alt="Win rate evolution (network only)"/> <img src="../../assets/img/connect-four/training/arena.png" alt="Arena results"/> <img src="../../assets/img/connect-four/training/loss.png" alt="Loss on full memory"/> <img src="../../assets/img/connect-four/training/exploration_depth.png" alt="Exploration depth"/> <img src="../../assets/img/connect-four/training/entropies.png" alt="Policy entropy"/> <img src="../../assets/img/connect-four/training/nsamples.png" alt="Number of training samples"/> <img src="../../assets/img/connect-four/training/loss_last_batch.png" alt="Loss on last batch"/> <img src="../../assets/img/connect-four/training/loss_per_stage.png" alt="Loss per game stage"/></p><h3 id="Per-iter-1"><a class="docs-heading-anchor" href="#Per-iter-1">Per iter</a><a class="docs-heading-anchor-permalink" href="#Per-iter-1" title="Permalink"></a></h3><p><img src="../../assets/img/connect-four/first-iter/loss.png" alt="Loss evolution during first iteration"/> <img src="../../assets/img/connect-four/first-iter/perfs.png" alt="Profiling for first teration"/> <img src="../../assets/img/connect-four/first-iter/summary.png" alt="Summary of first iteration"/></p><h3 id="Pons-Benchmark-1"><a class="docs-heading-anchor" href="#Pons-Benchmark-1">Pons Benchmark</a><a class="docs-heading-anchor-permalink" href="#Pons-Benchmark-1" title="Permalink"></a></h3><p><img src="../../assets/img/connect-four/pons-benchmark-results.png" alt="Pons benchark"/></p><h3 id="Async-Profiling-1"><a class="docs-heading-anchor" href="#Async-Profiling-1">Async Profiling</a><a class="docs-heading-anchor-permalink" href="#Async-Profiling-1" title="Permalink"></a></h3><p><img src="../assets/img/connect-four/async-profiling/mcts_speed.png" alt="Async speedup"/> <img src="../assets/img/connect-four/async-profiling/inference_time_ratio.png" alt="Time ratio spent doing inference"/></p><h3 id="Explorer-1"><a class="docs-heading-anchor" href="#Explorer-1">Explorer</a><a class="docs-heading-anchor-permalink" href="#Explorer-1" title="Permalink"></a></h3><p><img src="../../assets/img/explorer.png" alt="Explorer"/></p><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>To the best of our knowledge, none of the many existing Python implementations of AlphaZero are able to learn a player that beats a minmax baseline that plans at depth 2 (on a single desktop computer).</li><li class="footnote" id="footnote-2"><a class="tag is-link" href="#citeref-2">2</a>Compare those numbers with those of a popular <a href="https://github.com/suragnair/alpha-zero-general">Python implementation</a>, which achieves iterations of similar duration when training its Othello agent but only runs 100 games and 25 MCTS simulations per move.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../alphazero_intro/">« Introduction to AlphaZero</a><a class="docs-footer-nextpage" href="../../reference/params/">Training Parameters »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Tuesday 24 March 2020 22:26">Tuesday 24 March 2020</span>. Using Julia version 1.3.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
