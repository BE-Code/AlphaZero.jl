<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Training a Connect Four Agent · AlphaZero</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">AlphaZero</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Tutorial</span><ul><li><a class="tocitem" href="../alphazero_intro/">Introduction to AlphaZero</a></li><li class="is-active"><a class="tocitem" href>Training a Connect Four Agent</a><ul class="internal"><li><a class="tocitem" href="#Experimental-results-1"><span>Experimental results</span></a></li><li><a class="tocitem" href="#c4-config-1"><span>Full training configuration</span></a></li></ul></li></ul></li><li><span class="tocitem">Reference</span><ul><li><a class="tocitem" href="../../reference/params/">Training Parameters</a></li><li><a class="tocitem" href="../../reference/game_interface/">Game Interface</a></li><li><a class="tocitem" href="../../reference/mcts/">MCTS</a></li><li><a class="tocitem" href="../../reference/network/">Network Interface</a></li><li><a class="tocitem" href="../../reference/networks_library/">Networks Library</a></li><li><a class="tocitem" href="../../reference/player/">Players</a></li><li><a class="tocitem" href="../../reference/memory/">Memory Buffer</a></li><li><a class="tocitem" href="../../reference/environment/">Environment</a></li><li><a class="tocitem" href="../../reference/benchmark/">Benchmark</a></li><li><a class="tocitem" href="../../reference/reports/">Training Reports</a></li><li><a class="tocitem" href="../../reference/ui/">User Interface</a></li></ul></li><li><span class="tocitem">Contributing</span><ul><li><a class="tocitem" href="../../contributing/guide/">Contribution Guide</a></li><li><a class="tocitem" href="../../contributing/add_game/">Adding New Games</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorial</a></li><li class="is-active"><a href>Training a Connect Four Agent</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Training a Connect Four Agent</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/jonathan-laurent/AlphaZero.jl/blob/master/docs/src/tutorial/connect_four.md#L" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="connect_four-1"><a class="docs-heading-anchor" href="#connect_four-1">Training a Connect Four Agent</a><a class="docs-heading-anchor-permalink" href="#connect_four-1" title="Permalink"></a></h1><p>In this section, we demonstrate <code>AlphaZero.jl</code> by training a <em>Connect Four</em> agent without any form of supervision or prior knowledge. Although the game has been <a href="https://connect4.gamesolver.org/">solved</a> exactly with Alpha-beta pruning using domain-specific heuristics and optimizations, it is still a great challenge for reinforcement learning.<sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup></p><h3 id="Setup-1"><a class="docs-heading-anchor" href="#Setup-1">Setup</a><a class="docs-heading-anchor-permalink" href="#Setup-1" title="Permalink"></a></h3><p>To run the experiments in this tutorial, we recommend having a CUDA compatible GPU with 4GB of memory or more. A 2GB GPU should work fine but you may have to reduce batch size. Each training iteration took about one hour and a half on a standard desktop computer with an Intel Core i5 9600K processor and an 8GB Nvidia RTX 2070 GPU.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>To get optimal performances, it is also recommended to use <code>AlphaZero.jl</code> with Julia 1.5 (nightly), which includes a <a href="https://github.com/JuliaLang/julia/pull/33448">critical feature</a> that enables <code>CuArrays</code> to force incremental GC collections.</p></div></div><p>To download <code>AlphaZero.jl</code> and start a new training session, just run the following:</p><pre><code class="language-sh">git clone https://github.com/jonathan-laurent/AlphaZero.jl.git
cd AlphaZero.jl
julia --project -e &quot;import Pkg; Pkg.instantiate()&quot;
julia --project --color=yes scripts/alphazero.jl --game connect-four train</code></pre><p>Instead of using <code>scripts/alphazero.jl</code>, one can also run the following using the Julia REPL:</p><pre><code class="language-julia">ENV[&quot;CUARRAYS_MEMORY_POOL&quot;] = &quot;split&quot;

using AlphaZero

include(&quot;games/connect-four/main.jl&quot;)
using .ConnectFour: Game, Training

const SESSION_DIR = &quot;sessions/connect-four&quot;

session = AlphaZero.Session(
    Game,
    Training.Network{ConnectFour.Game},
    Training.params,
    Training.netparams,
    benchmark=Training.benchmark,
    dir=SESSION_DIR)

resume!(session)</code></pre><p>The first line configures CuArrays to use a splitting memory pool, which performs better than the default binned pool on AlphaZero&#39;s workload as it does not require to run the garbage collector as frequently. Then, a new AlphaZero <a href="../../reference/ui/#ui-1">session</a> is created with the following arguments:</p><table><tr><th style="text-align: left">Argument</th><th style="text-align: left">Description</th></tr><tr><td style="text-align: left"><code>Game</code></td><td style="text-align: left">Game type, which implements the <a href="../../reference/game_interface/#game_interface-1">game interface</a>.</td></tr><tr><td style="text-align: left"><code>Training.Network</code></td><td style="text-align: left">Network type, which implements the <a href="../../reference/network/#network_interface-1">network interface</a>.</td></tr><tr><td style="text-align: left"><code>Training.params</code></td><td style="text-align: left">AlphaZero <a href="../../reference/params/#params-1">parameters</a>.</td></tr><tr><td style="text-align: left"><code>Training.netparams</code></td><td style="text-align: left">Network <a href="../../reference/networks_library/#conv_resnet-1">hyperparameters</a>.</td></tr><tr><td style="text-align: left"><code>Training.benchmark</code></td><td style="text-align: left"><a href="../../reference/benchmark/#benchmark-1">Benchmark</a> that is run between training iterations.</td></tr><tr><td style="text-align: left"><code>SESSION_DIR</code></td><td style="text-align: left">Directory in which all session files are saved.</td></tr></table><p>The <code>ConnectFour.Training</code> module specifies some default parameters and benchmarks for the Connect Four game. Its content can be examined in file <code>games/connect-four/params.jl</code>. We copy it <a href="#c4-config-1">here</a> for reference but the most important parts will be discussed specifically in the rest of this tutorial.</p><h3 id="Initial-benchmarks-1"><a class="docs-heading-anchor" href="#Initial-benchmarks-1">Initial benchmarks</a><a class="docs-heading-anchor-permalink" href="#Initial-benchmarks-1" title="Permalink"></a></h3><p>After launching the training script for the first time, you should see the following:</p><p><img src="../../assets/img/ui-init.png" alt="Session CLI (init)"/></p><p>Before the first training iteration and between each iteration, the current AlphaZero agent is benchmarked against some baselines in a series of games (200 in this case) so as to provide a concrete measure of training progress. In this tutorial, we use two baselines:</p><ul><li>A <strong>vanilla MCTS</strong> baseline that uses rollouts to estimate the value of new nodes.</li><li>A <strong>minmax baseline</strong> that plans at depth 5 using a handcrafted heuristic.</li></ul><p>Comparing two deterministic players is challenging as deterministic players will always play the same game repeatedly given a unique initial state. To add randomization, all players are instantiated with a small but nonzero move selection temperature.<sup class="footnote-reference"><a id="citeref-2" href="#footnote-2">[2]</a></sup></p><p>The <code>redundancy</code> indicator is helpful to diagnose a lack of randomization. It measures the quantity <span>$1 - u / n$</span> where <span>$u$</span> is the total number of unique states that have been encountered (excluding the initial state) and <span>$n$</span> is the total number of encountered states, excluding the initial state and counting duplicates (see   <a href="../../reference/benchmark/#AlphaZero.Benchmark.DuelOutcome"><code>Benchmark.DuelOutcome</code></a>).</p><div class="admonition is-info"><header class="admonition-header">On leveraging symmetries</header><div class="admonition-body"><p>Another trick that we use to add randomization is to leverage the symmetry of the Connect Four board with respect to its central vertical axis: at each turn, the board is <em>flipped</em> along its central vertical axis with a fixed probability (see <a href="../../reference/params/#AlphaZero.Params"><code>flip_probability</code></a>).</p><p>This is one of two ways in which <code>AlphaZero.jl</code> takes advantage of board symmetries, the other one being data augmentation (see <a href="../../reference/params/#AlphaZero.Params"><code>use_symmetries</code></a>). Board symmetries can be declared for new games by implementing the <a href="../../reference/game_interface/#AlphaZero.GameInterface.symmetries"><code>GameInterface.symmetries</code></a> function.</p></div></div><p>As you can see, the AlphaZero agent can still win some games with a randomly initialized network, by relying on search alone for short term tactical decisions.</p><h3 id="Training-1"><a class="docs-heading-anchor" href="#Training-1">Training</a><a class="docs-heading-anchor-permalink" href="#Training-1" title="Permalink"></a></h3><p>After the initial benchmarks are done, the first training iteration can start. Each training iteration took between 60 and 90 minutes on our hardware. The first iterations are typically on the shorter end, as games of self-play terminate more quickly and the memory buffer has yet to reach its final size.</p><p><img src="../../assets/img/ui-first-iter.png" alt="Session CLI (first iteration)"/></p><p>Each training iteration is composed of a <strong>self-play phase</strong> and of a <strong>learning phase</strong>. During the self-play phase, the AlphaZero agent plays a series of 4000 games against itself, running 600 MCTS simulations for each move.<sup class="footnote-reference"><a id="citeref-3" href="#footnote-3">[3]</a></sup> Doing so, it records training samples in the memory buffer. Then, during the learning phase, the neural network is updated to fit data in memory. The current neural network is evaluated periodically against the best one seen so far, and replaces it for generating self-play data if it achieves a sufficiently high win rate. For more details, see <a href="../../reference/params/#AlphaZero.SelfPlayParams"><code>SelfPlayParams</code></a>, <a href="../../reference/params/#AlphaZero.LearningParams"><code>LearningParams</code></a> and <a href="../../reference/params/#AlphaZero.ArenaParams"><code>ArenaParams</code></a> respectively.</p><p>Between the self-play and learning phase, we perform an <strong>analysis of the memory buffer</strong> by partitioning samples according to how many moves remained until the end of the game when they were taken. This is useful to monitor how well the neural network performs at different game stages. Separate statistics are also computed for the last batch of collected samples. A description of all measured metrics can be found in <a href="../../reference/reports/#reports-1">Training Reports</a>.</p><p>At the end of every iteration, benchmarks are run, summary plots are generated and the state of the current environment is saved on disk. This way, if training is interrupted for any reason, it can be resumed from the last saved state by simply running <code>scripts/alphazero.jl</code> again.</p><h3 id="Examining-the-current-agent-1"><a class="docs-heading-anchor" href="#Examining-the-current-agent-1">Examining the current agent</a><a class="docs-heading-anchor-permalink" href="#Examining-the-current-agent-1" title="Permalink"></a></h3><p>At any time during training, you can start an <a href="../../reference/ui/#explorer-1">interactive command interpreter</a> to investigate the current agent:</p><pre><code class="language-none">julia --project --color=yes scripts/alphazero.jl --game connect-four explore</code></pre><p><img src="../../assets/img/explorer.png" alt="Explorer"/></p><p>If you just want to play, use the <code>play</code> mode instead:</p><pre><code class="language-none">julia --project --color=yes scripts/alphazero.jl --game connect-four play</code></pre><h2 id="Experimental-results-1"><a class="docs-heading-anchor" href="#Experimental-results-1">Experimental results</a><a class="docs-heading-anchor-permalink" href="#Experimental-results-1" title="Permalink"></a></h2><h3 id="Training-plots-1"><a class="docs-heading-anchor" href="#Training-plots-1">Training plots</a><a class="docs-heading-anchor-permalink" href="#Training-plots-1" title="Permalink"></a></h3><p><img src="../../assets/img/connect-four/plots/benchmark_won_games.png" alt="Win rate evolution (AlphaZero)"/> <img src="../../assets/img/connect-four/net-only/benchmark_won_games.png" alt="Win rate evolution (network only)"/> <img src="../../assets/img/connect-four/plots/arena.png" alt="Arena results"/> <img src="../../assets/img/connect-four/plots/loss.png" alt="Loss on full memory"/> <img src="../../assets/img/connect-four/plots/exploration_depth.png" alt="Exploration depth"/> <img src="../../assets/img/connect-four/plots/entropies.png" alt="Policy entropy"/> <img src="../../assets/img/connect-four/plots/nsamples.png" alt="Number of training samples"/> <img src="../../assets/img/connect-four/plots/loss_last_batch.png" alt="Loss on last batch"/> <img src="../../assets/img/connect-four/plots/loss_per_stage.png" alt="Loss per game stage"/></p><p>All summary plots generated during the training of our agent can be downloaded <a href="../../assets/download/c4-plots.zip">here</a>. Also, you can use</p><h3 id="async_bench-1"><a class="docs-heading-anchor" href="#async_bench-1">Asynchronous MCTS speedup</a><a class="docs-heading-anchor-permalink" href="#async_bench-1" title="Permalink"></a></h3><p>Most of the time spent training an AlphaZero agent is spent within MCTS, either generating self-play data or pitting networks against one another. For example, looking at the generated profiling report for iteration 80, we can see that a bit more than 7/8th of training is spent in MCTS.</p><p><img src="../../assets/img/connect-four/plots/iter_perfs/80.png" alt="Perfs at iter 80"/></p><p>A key MCTS optimization that is implemented in <code>AlphaZero.jl</code> is to allow several workers to explore the search tree asynchronously. This is a huge win even on a single machine and with no parallelism, as it enables to perform neural-network inference on large batches rather than evaluating board positions separately, thereby maximizing the GPU utilization.</p><p>To evaluate the resulting performance gain, we plot the resulting speedup on self-play data generation as a function of the number of asynchronous workers. Using <code>scripts/profile/async_mcts.jl</code> on our machine, we get the following:</p><p><img src="../../assets/img/connect-four/async-profiling/mcts_speed.png" alt="Async speedup"/></p><p>As you can see, we measure a 20x performance gain for 64 workers.</p><h3 id="Pons-Benchmark-1"><a class="docs-heading-anchor" href="#Pons-Benchmark-1">Pons Benchmark</a><a class="docs-heading-anchor-permalink" href="#Pons-Benchmark-1" title="Permalink"></a></h3><p><img src="../../assets/img/connect-four/pons-benchmark-results.png" alt="Pons benchark"/></p><h2 id="c4-config-1"><a class="docs-heading-anchor" href="#c4-config-1">Full training configuration</a><a class="docs-heading-anchor-permalink" href="#c4-config-1" title="Permalink"></a></h2><p>Here, we copy the full content of the configuration file <code>games/connect-four/params.jl</code> for reference.</p><p>Note that, in addition to having standard keyword constructors, parameter types have constructors that implement the <em>record update</em> operation from functional languages. For example, <code>Params(p, num_iters=100)</code> builds a <code>Params</code> object that is identical to <code>p</code> for every field, except <code>num_iters</code> which is set to <code>100</code>.</p><pre><code class="language-julia">Network = ResNet

netparams = ResNetHP(
  num_filters=64,
  num_blocks=7,
  conv_kernel_size=(3, 3),
  num_policy_head_filters=32,
  num_value_head_filters=32,
  batch_norm_momentum=0.1)

self_play = SelfPlayParams(
  num_games=4_000,
  reset_mcts_every=100,
  mcts=MctsParams(
    use_gpu=true,
    num_workers=64,
    num_iters_per_turn=600,
    cpuct=2.0,
    temperature=StepSchedule(
      start=1.0,
      change_at=[10],
      values=[0.5]),
    dirichlet_noise_ϵ=0.25,
    dirichlet_noise_α=1.0))

arena = ArenaParams(
  num_games=200,
  reset_mcts_every=nothing,
  flip_probability=0.5,
  update_threshold=0.1,
  mcts=MctsParams(
    self_play.mcts,
    temperature=StepSchedule(0.1),
    dirichlet_noise_ϵ=0.05))

learning = LearningParams(
  use_position_averaging=true,
  samples_weighing_policy=LOG_WEIGHT,
  batch_size=2048,
  loss_computation_batch_size=2048,
  optimiser=Adam(lr=1e-3),
  l2_regularization=1e-4,
  nonvalidity_penalty=1.,
  min_checkpoints_per_epoch=1,
  max_batches_per_checkpoint=1000,
  num_checkpoints=2)

params = Params(
  arena=arena,
  self_play=self_play,
  learning=learning,
  num_iters=80,
  ternary_rewards=true,
  use_symmetries=true,
  memory_analysis=MemAnalysisParams(
    num_game_stages=4),
  mem_buffer_size=PLSchedule(
  [      0,        60],
  [400_000, 2_000_000]))

baselines = [
  Benchmark.MctsRollouts(
    MctsParams(
      arena.mcts,
      num_iters_per_turn=1000,
      cpuct=1.)),
  Benchmark.MinMaxTS(depth=5, τ=0.2)]

make_duel(baseline) =
  Benchmark.Duel(
    Benchmark.Full(arena.mcts),
    baseline,
    num_games=200,
    flip_probability=0.5,
    color_policy=CONTENDER_WHITE)

benchmark = make_duel.(baselines)</code></pre><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>To the best of our knowledge, none of the many existing Python implementations of AlphaZero are able to learn a player that beats a minmax baseline that plans at depth 2 (on a single desktop computer).</li><li class="footnote" id="footnote-2"><a class="tag is-link" href="#citeref-2">2</a>Note, however, that the minmax baseline is guaranteed to play a winning move whenever it sees one and to avoid moves it can prove to be losing within 5 steps (see <a href="../../reference/benchmark/#AlphaZero.MinMax.Player"><code>MinMax.Player</code></a>).</li><li class="footnote" id="footnote-3"><a class="tag is-link" href="#citeref-3">3</a>Compare those numbers with those of a popular <a href="https://github.com/suragnair/alpha-zero-general">Python implementation</a>, which achieves iterations of similar duration when training its Othello agent but only runs 100 games and 25 MCTS simulations per move.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../alphazero_intro/">« Introduction to AlphaZero</a><a class="docs-footer-nextpage" href="../../reference/params/">Training Parameters »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Wednesday 25 March 2020 17:15">Wednesday 25 March 2020</span>. Using Julia version 1.3.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
