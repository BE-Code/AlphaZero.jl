<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Training a Connect Four Agent · AlphaZero</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="AlphaZero logo"/></a><div class="docs-package-name"><span class="docs-autofit">AlphaZero</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Guided Tour</span><ul><li><a class="tocitem" href="../alphazero_intro/">Introduction to AlphaZero</a></li><li><a class="tocitem" href="../package_overview/">Package Overview</a></li><li class="is-active"><a class="tocitem" href>Training a Connect Four Agent</a><ul class="internal"><li><a class="tocitem" href="#Setup-and-Training"><span>Setup and Training</span></a></li><li><a class="tocitem" href="#Experimental-Results"><span>Experimental Results</span></a></li><li><a class="tocitem" href="#c4-config"><span>Full Training Configuration</span></a></li></ul></li></ul></li><li><span class="tocitem">Reference</span><ul><li><a class="tocitem" href="../../reference/params/">Training Parameters</a></li><li><a class="tocitem" href="../../reference/game_interface/">Game Interface</a></li><li><a class="tocitem" href="../../reference/mcts/">MCTS</a></li><li><a class="tocitem" href="../../reference/network/">Network Interface</a></li><li><a class="tocitem" href="../../reference/networks_library/">Networks Library</a></li><li><a class="tocitem" href="../../reference/player/">Players</a></li><li><a class="tocitem" href="../../reference/memory/">Memory Buffer</a></li><li><a class="tocitem" href="../../reference/environment/">Environment</a></li><li><a class="tocitem" href="../../reference/benchmark/">Benchmark</a></li><li><a class="tocitem" href="../../reference/reports/">Training Reports</a></li><li><a class="tocitem" href="../../reference/ui/">User Interface</a></li></ul></li><li><span class="tocitem">Contributing</span><ul><li><a class="tocitem" href="../../contributing/guide/">Contribution Guide</a></li><li><a class="tocitem" href="../../contributing/add_game/">Adding New Games</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Guided Tour</a></li><li class="is-active"><a href>Training a Connect Four Agent</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Training a Connect Four Agent</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/jonathan-laurent/AlphaZero.jl/blob/master/docs/src/tutorial/connect_four.md#L" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="connect_four"><a class="docs-heading-anchor" href="#connect_four">Training a Connect Four Agent</a><a id="connect_four-1"></a><a class="docs-heading-anchor-permalink" href="#connect_four" title="Permalink"></a></h1><p>In this tutorial, we demonstrate AlphaZero.jl by training a <em>Connect Four</em> agent without any form of supervision or prior knowledge. Although the game has been <a href="https://connect4.gamesolver.org/">solved</a> exactly with Alpha-beta pruning using domain-specific heuristics and optimizations, it is still a great challenge for reinforcement learning.<sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup></p><h2 id="Setup-and-Training"><a class="docs-heading-anchor" href="#Setup-and-Training">Setup and Training</a><a id="Setup-and-Training-1"></a><a class="docs-heading-anchor-permalink" href="#Setup-and-Training" title="Permalink"></a></h2><p>To run the experiments in this tutorial, we recommend having a CUDA compatible GPU with 4GB of memory or more. A 2GB GPU should work fine but you may have to reduce batch size. Each training iteration took between one and two hours on a desktop computer with an Intel Core i5 9600K processor and an 8GB Nvidia RTX 2070 GPU.</p><p>To download AlphaZero.jl and start a new training session, just run the following:</p><pre><code class="language-sh">git clone --branch v0.3.0 https://github.com/jonathan-laurent/AlphaZero.jl.git
cd AlphaZero.jl
julia --project -e &quot;import Pkg; Pkg.instantiate()&quot;
julia --project --color=yes scripts/alphazero.jl --game connect-four train</code></pre><p>Instead of using <code>scripts/alphazero.jl</code>, one can also run the following using the Julia REPL:</p><pre><code class="language-julia">ENV[&quot;CUARRAYS_MEMORY_POOL&quot;] = &quot;split&quot;

using AlphaZero

include(&quot;games/connect-four/main.jl&quot;)
using .ConnectFour: Game, Training

const SESSION_DIR = &quot;sessions/connect-four&quot;

session = AlphaZero.Session(
    Game,
    Training.Network{ConnectFour.Game},
    Training.params,
    Training.netparams,
    benchmark=Training.benchmark,
    dir=SESSION_DIR)

resume!(session)</code></pre><p>The first line configures CuArrays to use a splitting memory pool, which performs better than the default binned pool on AlphaZero&#39;s workload as it does not require to run the garbage collector as frequently. Then, a new AlphaZero <a href="../../reference/ui/#ui">session</a> is created with the following arguments:</p><table><tr><th style="text-align: left">Argument</th><th style="text-align: left">Description</th></tr><tr><td style="text-align: left"><code>Game</code></td><td style="text-align: left">Game type, which implements the <a href="../../reference/game_interface/#game_interface">game interface</a>.</td></tr><tr><td style="text-align: left"><code>Training.Network</code></td><td style="text-align: left">Network type, which implements the <a href="../../reference/network/#network_interface">network interface</a>.</td></tr><tr><td style="text-align: left"><code>Training.params</code></td><td style="text-align: left">AlphaZero <a href="../../reference/params/#params">hyperparameters</a>.</td></tr><tr><td style="text-align: left"><code>Training.netparams</code></td><td style="text-align: left">Network <a href="../../reference/networks_library/#conv_resnet">hyperparameters</a>.</td></tr><tr><td style="text-align: left"><code>Training.benchmark</code></td><td style="text-align: left"><a href="../../reference/benchmark/#benchmark">Benchmark</a> that is run between training iterations.</td></tr><tr><td style="text-align: left"><code>SESSION_DIR</code></td><td style="text-align: left">Directory in which all session files are saved.</td></tr></table><p>The <code>ConnectFour.Training</code> module specifies the hyperparameters and benchmarks that are used in this tutorial. Its content can be examined in file <code>games/connect-four/params.jl</code>. We copy it for reference <a href="#c4-config">at the end of this tutorial</a>. Here are some highlights:</p><ul><li>We use a <a href="../../reference/networks_library/#conv_resnet">two-headed convolutional ResNet</a> similar to the one introduced in the AlphaGo Zero paper, although much smaller. Its tower consists of 5 residual blocks with 64 convolutional filters per layer, for a total of about 470K parameters (in contrast, the neural network from the AlphaGo Zero paper has about 100M parameters).</li><li>During each iteration, the current agent plays 1000 games against itself, running 600 MCTS simulations to plan each move.<sup class="footnote-reference"><a id="citeref-2" href="#footnote-2">[2]</a></sup> The move selection temperature is set to 1.0 during the first ten moves of every game and then decreased to 0.5.</li><li>Self-play data is accumulated in a memory buffer whose capacity grows from 200K samples (initially) to 1M samples (at iteration 60). For reference, assuming an average game duration of 35 moves, about 35 x 1000 = 35K new samples are generated at each iteration.</li></ul><h3 id="Initial-Benchmarks"><a class="docs-heading-anchor" href="#Initial-Benchmarks">Initial Benchmarks</a><a id="Initial-Benchmarks-1"></a><a class="docs-heading-anchor-permalink" href="#Initial-Benchmarks" title="Permalink"></a></h3><p>After launching the training script for the first time, you should see the following:</p><p><img src="../../assets/img/ui-init.png" alt="Session CLI (init)"/></p><p>Before the first training iteration and between each iteration, the current AlphaZero agent is benchmarked against some baselines in a series of games (200 in this case) so as to provide a concrete measure of training progress. In this tutorial, we use two baselines:</p><ul><li>A <strong>vanilla MCTS</strong> baseline that uses rollouts to estimate the value of new nodes.</li><li>A <strong>minmax baseline</strong> that plans at depth 5 using a handcrafted heuristic.</li></ul><p>Comparing two deterministic players is challenging as deterministic players will always play the same game repeatedly given a unique initial state. To add randomization, all players are instantiated with a small but nonzero move selection temperature.<sup class="footnote-reference"><a id="citeref-3" href="#footnote-3">[3]</a></sup></p><p>The <code>redundancy</code> indicator is helpful to diagnose a lack of randomization. It measures the quantity <span>$1 - u / n$</span> where <span>$u$</span> is the total number of unique states that have been encountered (excluding the initial state) and <span>$n$</span> is the total number of encountered states, excluding the initial state and counting duplicates (see <a href="../../reference/benchmark/#AlphaZero.Benchmark.DuelOutcome"><code>Benchmark.DuelOutcome</code></a>).</p><div class="admonition is-info"><header class="admonition-header">On leveraging symmetries</header><div class="admonition-body"><p>Another trick that we use to add randomization is to leverage the symmetry of the Connect Four board with respect to its central vertical axis: at each turn, the board is <em>flipped</em> along its central vertical axis with a fixed probability (see <a href="../../reference/params/#AlphaZero.Params"><code>flip_probability</code></a>).</p><p>This is one of two ways in which <code>AlphaZero.jl</code> takes advantage of board symmetries, the other one being data augmentation (see <a href="../../reference/params/#AlphaZero.Params"><code>use_symmetries</code></a>). Board symmetries can be declared for new games by implementing the <a href="../../reference/game_interface/#AlphaZero.GameInterface.symmetries"><code>GameInterface.symmetries</code></a> function.</p></div></div><p>As you can see, the AlphaZero agent can still win some games with a randomly initialized network, by relying on search alone for short term tactical decisions.</p><h3 id="Training"><a class="docs-heading-anchor" href="#Training">Training</a><a id="Training-1"></a><a class="docs-heading-anchor-permalink" href="#Training" title="Permalink"></a></h3><p>After the initial benchmarks are done, the first training iteration can start. Each training iteration took between 30 and 50 minutes on our hardware. The first iterations are typically on the shorter end, as games of self-play terminate more quickly and the memory buffer has yet to reach its final size.</p><p><img src="../../assets/img/ui-first-iter.png" alt="Session CLI (first iteration)"/></p><p>Between the self-play and learning phase of each iteration, we perform an <strong>analysis of the memory buffer</strong> by partitioning samples according to how many moves remained until the end of the game when they were recorded. This is useful to monitor how well the neural network performs at different game stages. Separate statistics are also computed for the last batch of collected samples.</p><p>A description of all reported metrics can be found in <a href="../../reference/reports/#reports">Training Reports</a>.</p><p>At the end of every iteration, benchmarks are run, summary plots are generated and the state of the current environment is saved on disk. This way, if training is interrupted for any reason, it can be resumed from the last saved state by simply running <code>scripts/alphazero.jl</code> again.</p><p>All summary plots generated during the training of our agent can be downloaded <a href="../../assets/download/c4-plots.zip">here</a>.</p><h3 id="Examining-the-current-agent"><a class="docs-heading-anchor" href="#Examining-the-current-agent">Examining the current agent</a><a id="Examining-the-current-agent-1"></a><a class="docs-heading-anchor-permalink" href="#Examining-the-current-agent" title="Permalink"></a></h3><p>At any time during training, you can start an <a href="../../reference/ui/#explorer">interactive command interpreter</a> to investigate the current agent:</p><pre><code class="language-none">julia --project --color=yes scripts/alphazero.jl --game connect-four explore</code></pre><p><img src="../../assets/img/explorer.png" alt="Explorer"/></p><p>If you just want to play and not be bothered with metrics, you can substitute <code>explore</code> by <code>play</code> in the command above.</p><h2 id="Experimental-Results"><a class="docs-heading-anchor" href="#Experimental-Results">Experimental Results</a><a id="Experimental-Results-1"></a><a class="docs-heading-anchor-permalink" href="#Experimental-Results" title="Permalink"></a></h2><p>We plot below the evolution of the win rate of our AlphaZero agent against our two baselines:</p><p><img src="../../assets/img/connect-four/plots/benchmark_won_games.png" alt="Win rate evolution (AlphaZero)"/></p><p>It is important to note that the AlphaZero agent is never exposed to those baselines during training and therefore cannot learn from them.</p><p>We also evaluate the performances of the neural network alone against the same baselines: instead of plugging it into MCTS, we just play the action that is assigned the highest prior probability at each state.</p><p><img src="../../assets/img/connect-four/net-only/benchmark_won_games.png" alt="Win rate evolution (network only)"/></p><p>Unsurprisingly, the network alone is initially unable to win a single game. However, it ends up significantly stronger than the minmax baseline despite not being able to perform any search.</p><h3 id="Benchmark-against-a-perfect-solver"><a class="docs-heading-anchor" href="#Benchmark-against-a-perfect-solver">Benchmark against a perfect solver</a><a id="Benchmark-against-a-perfect-solver-1"></a><a class="docs-heading-anchor-permalink" href="#Benchmark-against-a-perfect-solver" title="Permalink"></a></h3><p>Finally, we benchmark our AlphaZero agent against a perfect Connect Four solver. To do so, we evaluate the rate at which it makes mistakes on different test datasets available <a href="http://blog.gamesolver.org/solving-connect-four/02-test-protocol/">here</a>. Each dataset gathers positions of similar game depth and level of difficulty. More specificaly:</p><ul><li>A position is labelled <strong>&quot;Beginning&quot;</strong> if it is at most 14 moves away from the beginning of the game. It is labelled <strong>&quot;End&quot;</strong> if it is more than 28 moves away. Otherwise, it is labelled <strong>&quot;Middle&quot;</strong>.</li><li>A position is labelled <strong>&quot;Easy&quot;</strong> if it is less than 14 moves away from the end of the game given perfect play from both players. It is labelled <strong>&quot;Hard&quot;</strong> if it is more than 28 moves away. Otherwise, it is labelled <strong>&quot;Medium&quot;</strong>.</li></ul><p>On each dataset, we measure the mistake rate of our AlphaZero agent at different stages of training. An action suggested by the agent is said to be <em>a mistake</em> if another action is available with a strictly higher optimal Q-value (which can be either -1, 0 or 1). For example, the agent makes a mistake if it plays an action that results in a losing or draw state (assuming perfect play from both player) when another action exists that would have led to a winning state.</p><p>We plot the results below. The blue curves describe the mistake rate of our AlphaZero agent as a function of the number of training iterations on different datasets. The orange lines indicate the mistake rate of the minmax baseline. This experiment can be replicated using the script at <code>games/connect-four/scripts/pons_benchmark.jl</code>.</p><p><img src="../../assets/img/connect-four/pons-benchmark-results.png" alt="Pons benchark"/></p><p>As you can see, while our AlphaZero agent makes few mistakes that could be detected by planning up to 14 moves ahead, it is still imperfect at making longer term strategical decisions and playing overtures.</p><h3 id="You-can-do-better!"><a class="docs-heading-anchor" href="#You-can-do-better!">You can do better!</a><a id="You-can-do-better!-1"></a><a class="docs-heading-anchor-permalink" href="#You-can-do-better!" title="Permalink"></a></h3><p>The AlphaZero agent that is demonstrated in this tutorial went through little hyperparameters tuning and it can certainly be improved significantly. We encourage you to make your own tuning experiments and <a href="https://gitter.im/alphazero-jl/community?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge">share</a> the results.</p><p>The <a href="https://medium.com/oracledevs/lessons-from-alpha-zero-part-6-hyperparameter-tuning-b1cfcbe4ca9a">Oracle series</a> discusses hyperparameters tuning for a Connect Four agent. However, their configuration is optimized for more powerful hardware than is targeted by this tutorial <sup class="footnote-reference"><a id="citeref-4" href="#footnote-4">[4]</a></sup> In particular, they use a network that is about ten times larger and generate twice as much training data per iteration.</p><p>Our current configuration results from an attempt at downscaling Oracle&#39;s setup. Doing so is not trivial as hyperparameters are interrelated in a complex fashion. For example, we found that reducing exploration slightly results in faster training for our downscaled agent. Also:</p><ul><li>We chose to use the Adam optimizer instead of SGD with cyclical rates, as it introduces less hyperparameters and is generally more forgiving.</li><li>Raising the number of MCTS simulations per move from 300 to its current value of 600 resulted in faster learning, despite the additional computational cost per iteration.</li><li>To tune the MCTS hyperparameters, we trained a first version of AlphaZero based on an initial guess. Then, we ran a grid-search by plugging different MCTS parameters into the resulting agent and optimizing its score against the minmax baseline. We iterated again with a second training session and a second grid-search to get the current parameters. Notably, tuning the <a href="../../reference/params/#AlphaZero.MctsParams"><code>prior_temperature</code></a> parameter (which does not exist in the original version of AlphaZero and was introduced by LeelaZero) resulted in a significant improvement.</li></ul><p>Apart from that, most hyperparameters are the result of a single guess. We are looking forward to seeing how you can improve our Connect Four agent, with or without the help of better hardware!</p><h2 id="c4-config"><a class="docs-heading-anchor" href="#c4-config">Full Training Configuration</a><a id="c4-config-1"></a><a class="docs-heading-anchor-permalink" href="#c4-config" title="Permalink"></a></h2><p>Here, we copy the full content of the configuration file <code>games/connect-four/params.jl</code> for reference.</p><p>Note that, in addition to having standard keyword constructors, parameter types have constructors that implement the <em>record update</em> operation from functional languages. For example, <code>Params(p, num_iters=100)</code> builds a <code>Params</code> object that is identical to <code>p</code> for every field, except <code>num_iters</code> which is set to <code>100</code>.</p><pre><code class="language-julia"># Hyperparameters

Network = ResNet

netparams = ResNetHP(
  num_filters=64,
  num_blocks=5,
  conv_kernel_size=(3, 3),
  num_policy_head_filters=32,
  num_value_head_filters=32,
  batch_norm_momentum=0.1)

self_play = SelfPlayParams(
  num_games=2000,
  reset_mcts_every=50,
  mcts=MctsParams(
    use_gpu=true,
    num_workers=32,
    num_iters_per_turn=600,
    cpuct=3.0,
    prior_temperature=0.7,
    temperature=PLSchedule([0, 20], [1.0, 0.3]),
    dirichlet_noise_ϵ=0.2,
    dirichlet_noise_α=1.0))

arena = ArenaParams(
  num_games=100,
  reset_mcts_every=50,
  flip_probability=0.5,
  update_threshold=0.1,
  mcts=MctsParams(
    self_play.mcts,
    temperature=ConstSchedule(0.2),
    dirichlet_noise_ϵ=0.05))

learning = LearningParams(
  use_gpu=true,
  use_position_averaging=true,
  samples_weighing_policy=LOG_WEIGHT,
  batch_size=2048,
  loss_computation_batch_size=2048,
  optimiser=Adam(lr=1e-3),
  l2_regularization=1e-4,
  nonvalidity_penalty=1.,
  min_checkpoints_per_epoch=1,
  max_batches_per_checkpoint=1000,
  num_checkpoints=1)

params = Params(
  arena=arena,
  self_play=self_play,
  learning=learning,
  num_iters=50,
  ternary_rewards=true,
  use_symmetries=true,
  memory_analysis=MemAnalysisParams(
    num_game_stages=4),
  mem_buffer_size=PLSchedule(
  [      0,        60],
  [200_000, 1_000_000]))

# Benchmarks

mcts_baseline =
  Benchmark.MctsRollouts(
    MctsParams(
      arena.mcts,
      num_iters_per_turn=1000,
      num_workers=1,
      cpuct=1.))

minmax_baseline = Benchmark.MinMaxTS(depth=5, amplify_rewards=true, τ=0.2)

players = [
  Benchmark.Full(arena.mcts),
  Benchmark.Full(arena.mcts),
  Benchmark.NetworkOnly(τ=0.5, use_gpu=true)]

baselines = [
  mcts_baseline,
  minmax_baseline,
  mcts_baseline]

make_duel(player, baseline) =
  Benchmark.Duel(
    player,
    baseline,
    num_games=50,
    flip_probability=0.5,
    color_policy=CONTENDER_WHITE)

benchmark = [make_duel(p, b) for (p, b) in zip(players, baselines)]</code></pre><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>To the best of our knowledge, none of the many existing Python implementations of AlphaZero are able to learn a player that beats a minmax baseline that plans at depth 2 (on a single desktop computer).</li><li class="footnote" id="footnote-2"><a class="tag is-link" href="#citeref-2">2</a>Compare those numbers with those of a popular <a href="https://github.com/suragnair/alpha-zero-general">Python implementation</a>, which achieves iterations of similar duration when training its Othello agent but only runs 100 games and 25 MCTS simulations per move.</li><li class="footnote" id="footnote-3"><a class="tag is-link" href="#citeref-3">3</a>Note, however, that the minmax baseline is guaranteed to play a winning move whenever it sees one and to avoid moves it can prove to be losing within 5 steps (see <a href="../../reference/benchmark/#AlphaZero.MinMax.Player"><code>MinMax.Player</code></a>).</li><li class="footnote" id="footnote-4"><a class="tag is-link" href="#citeref-4">4</a>Their closed-source C++ implementation of AlphaZero is also faster. A big part of the difference is apparently coming from them using Int8 quantization to accelerate network inference. See <a href="../../contributing/guide/#contributions_guide">Contributions Guide</a>.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../package_overview/">« Package Overview</a><a class="docs-footer-nextpage" href="../../reference/params/">Training Parameters »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Thursday 9 July 2020 22:52">Thursday 9 July 2020</span>. Using Julia version 1.4.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
