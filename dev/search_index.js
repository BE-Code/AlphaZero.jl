var documenterSearchIndex = {"docs":
[{"location":"reference/networks_library/#networks_library","page":"Networks Library","title":"Networks Library","text":"","category":"section"},{"location":"reference/networks_library/","page":"Networks Library","title":"Networks Library","text":"CurrentModule = AlphaZero","category":"page"},{"location":"reference/networks_library/","page":"Networks Library","title":"Networks Library","text":"For convenience, we provide a library of standard networks implementing the neural network interface.","category":"page"},{"location":"reference/networks_library/","page":"Networks Library","title":"Networks Library","text":"These networks are contained in the AlphaZero.NetLib module, which is resolved to either AlphaZero.KnetLib or AlphaZero.FluxLib during precompilation depending on the value of the ALPHAZERO_DEFAULT_DL_FRAMEWORK environment variable (Knet is recommended and used by default).","category":"page"},{"location":"reference/networks_library/#conv_resnet","page":"Networks Library","title":"Convolutional ResNet","text":"","category":"section"},{"location":"reference/networks_library/","page":"Networks Library","title":"Networks Library","text":"NetLib.ResNet\nNetLib.ResNetHP","category":"page"},{"location":"reference/networks_library/#AlphaZero.FluxLib.ResNet","page":"Networks Library","title":"AlphaZero.FluxLib.ResNet","text":"ResNet <: TwoHeadNetwork\n\nThe convolutional residual network architecture that is used in the original AlphaGo Zero paper.\n\n\n\n\n\n","category":"type"},{"location":"reference/networks_library/#AlphaZero.FluxLib.ResNetHP","page":"Networks Library","title":"AlphaZero.FluxLib.ResNetHP","text":"ResNetHP\n\nHyperparameters for the convolutional resnet architecture.\n\nParameter Type Default\nnum_blocks Int -\nnum_filters Int -\nconv_kernel_size Tuple{Int, Int} -\nnum_policy_head_filters Int 2\nnum_value_head_filters Int 1\nbatch_norm_momentum Float32 0.6f0\n\nThe trunk of the two-head network consists of num_blocks consecutive blocks. Each block features two convolutional layers with num_filters filters and with kernel size conv_kernel_size. Note that both kernel dimensions must be odd.\n\nDuring training, the network is evaluated in training mode on the whole dataset to compute the loss before it is switched to test model, using big batches. Therefore, it makes sense to use a high batch norm momentum (put a lot of weight on the latest measurement).\n\nAlphaGo Zero Parameters\n\nThe network in the original paper from Deepmind features 20 blocks with 256 filters per convolutional layer.\n\n\n\n\n\n","category":"type"},{"location":"reference/networks_library/#simplenet","page":"Networks Library","title":"Simple Network","text":"","category":"section"},{"location":"reference/networks_library/","page":"Networks Library","title":"Networks Library","text":"NetLib.SimpleNet\nNetLib.SimpleNetHP","category":"page"},{"location":"reference/networks_library/#AlphaZero.FluxLib.SimpleNet","page":"Networks Library","title":"AlphaZero.FluxLib.SimpleNet","text":"SimpleNet <: TwoHeadNetwork\n\nA simple two-headed architecture with only dense layers.\n\n\n\n\n\n","category":"type"},{"location":"reference/networks_library/#AlphaZero.FluxLib.SimpleNetHP","page":"Networks Library","title":"AlphaZero.FluxLib.SimpleNetHP","text":"SimpleNetHP\n\nHyperparameters for the simplenet architecture.\n\nParameter Description\nwidth :: Int Number of neurons on each dense layer\ndepth_common :: Int Number of dense layers in the trunk\ndepth_phead = 1 Number of hidden layers in the actions head\ndepth_vhead = 1 Number of hidden layers in the value  head\nuse_batch_norm = false Use batch normalization between each layer\nbatch_norm_momentum = 0.6f0 Momentum of batch norm statistics updates\n\n\n\n\n\n","category":"type"},{"location":"reference/benchmark/#benchmark","page":"Benchmark","title":"Benchmark","text":"","category":"section"},{"location":"reference/benchmark/","page":"Benchmark","title":"Benchmark","text":"CurrentModule = AlphaZero","category":"page"},{"location":"reference/benchmark/","page":"Benchmark","title":"Benchmark","text":"Benchmark","category":"page"},{"location":"reference/benchmark/#AlphaZero.Benchmark","page":"Benchmark","title":"AlphaZero.Benchmark","text":"Utilities to evaluate players against one another.\n\nTypically, between each training iteration, different players that possibly depend on the current neural network compete against a set of baselines.\n\n\n\n\n\n","category":"module"},{"location":"reference/benchmark/#Evaluations","page":"Benchmark","title":"Evaluations","text":"","category":"section"},{"location":"reference/benchmark/","page":"Benchmark","title":"Benchmark","text":"Benchmark.Evaluation\nBenchmark.Single\nBenchmark.Duel\nBenchmark.run","category":"page"},{"location":"reference/benchmark/#AlphaZero.Benchmark.Evaluation","page":"Benchmark","title":"AlphaZero.Benchmark.Evaluation","text":"Evaluation\n\nAbstract type for a benchmark item specification.\n\n\n\n\n\n","category":"type"},{"location":"reference/benchmark/#AlphaZero.Benchmark.Single","page":"Benchmark","title":"AlphaZero.Benchmark.Single","text":"Single <: Evaluation\n\nEvaluating a single player in a one-player game.\n\n\n\n\n\n","category":"type"},{"location":"reference/benchmark/#AlphaZero.Benchmark.Duel","page":"Benchmark","title":"AlphaZero.Benchmark.Duel","text":"Duel <: Evaluation\n\nEvaluating a player by pitting it against a baseline player in a two-player game.\n\n\n\n\n\n","category":"type"},{"location":"reference/benchmark/#AlphaZero.Benchmark.run","page":"Benchmark","title":"AlphaZero.Benchmark.run","text":"Benchmark.run(env::Env, duel::Benchmark.Evaluation, progress=nothing)\n\nRun a benchmark duel and return a Report.Evaluation.\n\nIf a progress is provided, next!(progress) is called after each simulated game.\n\n\n\n\n\n","category":"function"},{"location":"reference/benchmark/#Players","page":"Benchmark","title":"Players","text":"","category":"section"},{"location":"reference/benchmark/","page":"Benchmark","title":"Benchmark","text":"Benchmark.Player\nBenchmark.Full\nBenchmark.NetworkOnly\nBenchmark.MctsRollouts\nBenchmark.MinMaxTS","category":"page"},{"location":"reference/benchmark/#AlphaZero.Benchmark.Player","page":"Benchmark","title":"AlphaZero.Benchmark.Player","text":"Benchmark.Player\n\nAbstract type to specify a player that can be featured in a benchmark duel.\n\nSubtypes must implement the following functions:\n\nBenchmark.instantiate(player, nn): instantiate the player specification   into an AbstractPlayer given a neural network\nBenchmark.name(player): return a String describing the player\n\n\n\n\n\n","category":"type"},{"location":"reference/benchmark/#AlphaZero.Benchmark.Full","page":"Benchmark","title":"AlphaZero.Benchmark.Full","text":"Benchmark.Full(params) <: Benchmark.Player\n\nFull AlphaZero player that combines MCTS with the learnt network.\n\nArgument params has type MctsParams.\n\n\n\n\n\n","category":"type"},{"location":"reference/benchmark/#AlphaZero.Benchmark.NetworkOnly","page":"Benchmark","title":"AlphaZero.Benchmark.NetworkOnly","text":"Benchmark.NetworkOnly(;τ=1.0) <: Benchmark.Player\n\nPlayer that uses the policy output by the learnt network directly, instead of relying on MCTS.\n\n\n\n\n\n","category":"type"},{"location":"reference/benchmark/#AlphaZero.Benchmark.MctsRollouts","page":"Benchmark","title":"AlphaZero.Benchmark.MctsRollouts","text":"Benchmark.MctsRollouts(params) <: Benchmark.Player\n\nPure MCTS baseline that uses rollouts to evaluate new positions.\n\nArgument params has type MctsParams.\n\n\n\n\n\n","category":"type"},{"location":"reference/benchmark/#AlphaZero.Benchmark.MinMaxTS","page":"Benchmark","title":"AlphaZero.Benchmark.MinMaxTS","text":"Benchmark.MinMaxTS(;depth, τ=0.) <: Benchmark.Player\n\nMinmax baseline, which relies on MinMax.Player.\n\n\n\n\n\n","category":"type"},{"location":"reference/benchmark/#Minmax-Baseline","page":"Benchmark","title":"Minmax Baseline","text":"","category":"section"},{"location":"reference/benchmark/","page":"Benchmark","title":"Benchmark","text":"MinMax\nMinMax.Player","category":"page"},{"location":"reference/benchmark/#AlphaZero.MinMax","page":"Benchmark","title":"AlphaZero.MinMax","text":"A simple implementation of the minmax tree search algorithm, to be used as a baseline against AlphaZero. Heuristic board values are provided by the GameInterface.heuristic_value function.\n\n\n\n\n\n","category":"module"},{"location":"reference/benchmark/#AlphaZero.MinMax.Player","page":"Benchmark","title":"AlphaZero.MinMax.Player","text":"MinMax.Player <: AbstractPlayer\n\nA stochastic minmax player, to be used as a baseline.\n\nMinMax.Player(;depth, amplify_rewards, τ=0.)\n\nThe minmax player explores the game tree exhaustively at depth depth to build an estimate of the Q-value of each available action. Then, it chooses an action as follows:\n\nIf there are winning moves (with value Inf), one of them is picked uniformly at random.\nIf all moves are losing (with value -Inf), one of them is picked uniformly at random.\n\nOtherwise,\n\nIf the temperature τ is zero, a move is picked uniformly among those with maximal Q-value (there is usually only one choice).\nIf the temperature τ is nonzero, the probability of choosing action a is proportional to e^fracq_aCτ where q_a is the Q value of action a and C is the maximum absolute value of all finite Q values, making the decision invariant to rescaling of GameInterface.heuristic_value.\n\nIf the amplify_rewards option is set to true, every received positive reward is converted to  and every negative reward is converted to -.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/own_game/#own_game","page":"Solving Your Own Games","title":"Solving Your Own Games","text":"","category":"section"},{"location":"tutorial/own_game/","page":"Solving Your Own Games","title":"Solving Your Own Games","text":"CurrentModule = AlphaZero","category":"page"},{"location":"tutorial/own_game/","page":"Solving Your Own Games","title":"Solving Your Own Games","text":"Here are some recommended steps for using AlphaZero.jl on your own game.","category":"page"},{"location":"tutorial/own_game/","page":"Solving Your Own Games","title":"Solving Your Own Games","text":"Implement the Game Interface for your game or wrap a CommonRLInterface environment. You can look at the examples in the games directory for inspiration.\nOnce you have defined an AbstractGameSpec, you can use Scripts.test_game to run a series of sanity checks on your implementation.\nDefine your neural network architecture or reuse one from the Networks Library.\nThink about your training hyperparameters and define an Experiment. Once again, you may draw inspiration from the provided examples.\nBefore running a complete training session, you can use Scripts.dummy_run to check the absence of potential runtime errors in your code. You can also use Scripts.test_grad_updates to run a gradient update step on dummy data in order to check that your GPU has enough memory to accomodate your current choice of hyperparameters.\nRun a training session using Scripts.train. You can interrupt training at any moment and resume it later since the current state of your agent is saved automatically in the sessions folder after each iteration.\nLook at the generated plots and reports and adapt your hyperparameters if necessary. Also, you can visualize your current agent using Scripts.explore.","category":"page"},{"location":"tutorial/own_game/","page":"Solving Your Own Games","title":"Solving Your Own Games","text":"If you've managed to train a successful agent, please consider sharing your experience with us by opening an issue on Github or posting on the Julia Discourse forum.","category":"page"},{"location":"reference/network/#network_interface","page":"Network Interface","title":"Network Interface","text":"","category":"section"},{"location":"reference/network/","page":"Network Interface","title":"Network Interface","text":"CurrentModule = AlphaZero.Network","category":"page"},{"location":"reference/network/","page":"Network Interface","title":"Network Interface","text":"Network","category":"page"},{"location":"reference/network/#AlphaZero.Network","page":"Network Interface","title":"AlphaZero.Network","text":"A generic, framework agnostic interface for neural networks.\n\n\n\n\n\n","category":"module"},{"location":"reference/network/#Mandatory-Interface","page":"Network Interface","title":"Mandatory Interface","text":"","category":"section"},{"location":"reference/network/","page":"Network Interface","title":"Network Interface","text":"AbstractNetwork\nHyperParams\nhyperparams\ngame_spec\nforward\ntrain!\nset_test_mode!\nparams\nregularized_params","category":"page"},{"location":"reference/network/#AlphaZero.Network.AbstractNetwork","page":"Network Interface","title":"AlphaZero.Network.AbstractNetwork","text":"AbstractNetwork\n\nAbstract base type for a neural network.\n\nConstructor\n\nAny subtype Network must implement Base.copy along with the following constructor:\n\nNetwork(game_spec, hyperparams)\n\nwhere the expected type of hyperparams is given by HyperParams(Network).\n\n\n\n\n\n","category":"type"},{"location":"reference/network/#AlphaZero.Network.HyperParams","page":"Network Interface","title":"AlphaZero.Network.HyperParams","text":"HyperParams(::Type{<:AbstractNetwork})\n\nReturn the hyperparameter type associated with a given network type.\n\n\n\n\n\n","category":"function"},{"location":"reference/network/#AlphaZero.Network.hyperparams","page":"Network Interface","title":"AlphaZero.Network.hyperparams","text":"hyperparams(::AbstractNetwork)\n\nReturn the hyperparameters of a network.\n\n\n\n\n\n","category":"function"},{"location":"reference/network/#AlphaZero.Network.game_spec","page":"Network Interface","title":"AlphaZero.Network.game_spec","text":"game_spec(::AbstractNetwork)\n\nReturn the game specification that was passed to the network's constructor.\n\n\n\n\n\n","category":"function"},{"location":"reference/network/#AlphaZero.Network.forward","page":"Network Interface","title":"AlphaZero.Network.forward","text":"forward(::AbstractNetwork, states)\n\nCompute the forward pass of a network on a batch of inputs.\n\nExpect a Float32 tensor states whose batch dimension is the last one.\n\nReturn a (P, V) triple where:\n\nP is a matrix of size (num_actions, batch_size). It is allowed to put weight on invalid actions (see evaluate).\nV is a row vector of size (1, batch_size)\n\n\n\n\n\n","category":"function"},{"location":"reference/network/#AlphaZero.Network.train!","page":"Network Interface","title":"AlphaZero.Network.train!","text":"train!(callback, ::AbstractNetwork, opt::OptimiserSpec, loss, batches, n)\n\nUpdate a given network to fit some data.\n\nopt specifies which optimiser to use.\nloss is a function that maps a batch of samples to a tracked real.\ndata is an iterator over minibatches.\nn is the number of minibatches. If length is defined on data,  we must have length(data) == n. However, not all finite  iterators implement length and thus this argument is needed.\ncallback(i, loss) is called at each step with the batch number i  and the loss on last batch.\n\n\n\n\n\n","category":"function"},{"location":"reference/network/#AlphaZero.Network.set_test_mode!","page":"Network Interface","title":"AlphaZero.Network.set_test_mode!","text":"set_test_mode!(mode=true)\n\nPut a network in test mode or in training mode. This is relevant for networks featuring layers such as batch normalization layers.\n\n\n\n\n\n","category":"function"},{"location":"reference/network/#AlphaZero.Network.params","page":"Network Interface","title":"AlphaZero.Network.params","text":"params(::AbstractNetwork)\n\nReturn the collection of trainable parameters of a network.\n\n\n\n\n\n","category":"function"},{"location":"reference/network/#AlphaZero.Network.regularized_params","page":"Network Interface","title":"AlphaZero.Network.regularized_params","text":"regularized_params(::AbstractNetwork)\n\nReturn the collection of regularized parameters of a network. This usually excludes neuron's biases.\n\n\n\n\n\n","category":"function"},{"location":"reference/network/#Conversion-and-Copy","page":"Network Interface","title":"Conversion and Copy","text":"","category":"section"},{"location":"reference/network/","page":"Network Interface","title":"Network Interface","text":"to_gpu\nto_cpu\non_gpu\nconvert_input\nconvert_output","category":"page"},{"location":"reference/network/#AlphaZero.Network.to_gpu","page":"Network Interface","title":"AlphaZero.Network.to_gpu","text":"to_gpu(::AbstractNetwork)\n\nReturn a copy of the given network that has been transferred to the GPU if one is available. Otherwise, return the given network untouched.\n\n\n\n\n\n","category":"function"},{"location":"reference/network/#AlphaZero.Network.to_cpu","page":"Network Interface","title":"AlphaZero.Network.to_cpu","text":"to_cpu(::AbstractNetwork)\n\nReturn a copy of the given network that has been transferred to the CPU or return the given network untouched if it is already on CPU.\n\n\n\n\n\n","category":"function"},{"location":"reference/network/#AlphaZero.Network.on_gpu","page":"Network Interface","title":"AlphaZero.Network.on_gpu","text":"on_gpu(::AbstractNetwork) :: Bool\n\nTest whether or not a network is located on GPU.\n\n\n\n\n\n","category":"function"},{"location":"reference/network/#AlphaZero.Network.convert_input","page":"Network Interface","title":"AlphaZero.Network.convert_input","text":"convert_input(::AbstractNetwork, input)\n\nConvert an array (or number) to the right format so that it can be used as an input by a given network.\n\n\n\n\n\n","category":"function"},{"location":"reference/network/#AlphaZero.Network.convert_output","page":"Network Interface","title":"AlphaZero.Network.convert_output","text":"convert_output(::AbstractNetwork, output)\n\nConvert an array (or number) produced by a neural network to a standard CPU array (or number) type.\n\n\n\n\n\n","category":"function"},{"location":"reference/network/#Misc","page":"Network Interface","title":"Misc","text":"","category":"section"},{"location":"reference/network/","page":"Network Interface","title":"Network Interface","text":"gc","category":"page"},{"location":"reference/network/#AlphaZero.Network.gc","page":"Network Interface","title":"AlphaZero.Network.gc","text":"gc(::AbstractNetwork)\n\nPerform full garbage collection and empty the GPU memory pool.\n\n\n\n\n\n","category":"function"},{"location":"reference/network/#Derived-Functions","page":"Network Interface","title":"Derived Functions","text":"","category":"section"},{"location":"reference/network/#Evaluation-Functions","page":"Network Interface","title":"Evaluation Functions","text":"","category":"section"},{"location":"reference/network/","page":"Network Interface","title":"Network Interface","text":"forward_normalized\nevaluate\nevaluate_batch","category":"page"},{"location":"reference/network/#AlphaZero.Network.forward_normalized","page":"Network Interface","title":"AlphaZero.Network.forward_normalized","text":"forward_normalized(network::AbstractNetwork, states, actions_mask)\n\nEvaluate a batch of vectorized states. This function is a wrapper on forward that puts a zero weight on invalid actions.\n\nArguments\n\nstates is a tensor whose last dimension has size bach_size\nactions_mask is a binary matrix of size (num_actions, batch_size)\n\nReturned value\n\nReturn a (P, V, Pinv) triple where:\n\nP is a matrix of size (num_actions, batch_size).\nV is a row vector of size (1, batch_size).\nPinv is a row vector of size (1, batch_size)  that indicates the total probability weight put by the network  on invalid actions for each sample.\n\nAll tensors manipulated by this function have elements of type Float32.\n\n\n\n\n\n","category":"function"},{"location":"reference/network/#AlphaZero.Network.evaluate","page":"Network Interface","title":"AlphaZero.Network.evaluate","text":"evaluate(::AbstractNetwork, state)\n\n(nn::AbstractNetwork)(state) = evaluate(nn, state)\n\nEvaluate the neural network as an MCTS oracle on a single state.\n\nNote, however, that evaluating state positions once at a time is slow and so you may want to use a BatchedOracle along with an inference server that uses evaluate_batch.\n\n\n\n\n\n","category":"function"},{"location":"reference/network/#AlphaZero.Network.evaluate_batch","page":"Network Interface","title":"AlphaZero.Network.evaluate_batch","text":"evaluate_batch(::AbstractNetwork, batch)\n\nEvaluate the neural network as an MCTS oracle on a batch of states at once.\n\nTake a list of states as input and return a list of (P, V) pairs as defined in the MCTS oracle interface.\n\n\n\n\n\n","category":"function"},{"location":"reference/network/#Misc-2","page":"Network Interface","title":"Misc","text":"","category":"section"},{"location":"reference/network/","page":"Network Interface","title":"Network Interface","text":"num_parameters\nnum_regularized_parameters\nmean_weight\ncopy(::AbstractNetwork)","category":"page"},{"location":"reference/network/#AlphaZero.Network.num_parameters","page":"Network Interface","title":"AlphaZero.Network.num_parameters","text":"num_parameters(::AbstractNetwork)\n\nReturn the total number of parameters of a network.\n\n\n\n\n\n","category":"function"},{"location":"reference/network/#AlphaZero.Network.num_regularized_parameters","page":"Network Interface","title":"AlphaZero.Network.num_regularized_parameters","text":"num_regularized_parameters(::AbstractNetwork)\n\nReturn the total number of regularized parameters of a network.\n\n\n\n\n\n","category":"function"},{"location":"reference/network/#AlphaZero.Network.mean_weight","page":"Network Interface","title":"AlphaZero.Network.mean_weight","text":"mean_weight(::AbstractNetwork)\n\nReturn the mean absolute value of the regularized parameters of a network.\n\n\n\n\n\n","category":"function"},{"location":"reference/network/#AlphaZero.Network.copy-Tuple{AbstractNetwork}","page":"Network Interface","title":"AlphaZero.Network.copy","text":"copy(::AbstractNetwork; on_gpu, test_mode)\n\nA copy function that also handles CPU/GPU transfers and test/train mode switches.\n\n\n\n\n\n","category":"method"},{"location":"reference/network/#Optimiser-Specification","page":"Network Interface","title":"Optimiser Specification","text":"","category":"section"},{"location":"reference/network/","page":"Network Interface","title":"Network Interface","text":"OptimiserSpec\nCyclicNesterov\nAdam","category":"page"},{"location":"reference/network/#AlphaZero.Network.OptimiserSpec","page":"Network Interface","title":"AlphaZero.Network.OptimiserSpec","text":"OptimiserSpec\n\nAbstract type for an optimiser specification.\n\n\n\n\n\n","category":"type"},{"location":"reference/network/#AlphaZero.Network.CyclicNesterov","page":"Network Interface","title":"AlphaZero.Network.CyclicNesterov","text":"CyclicNesterov(; lr_base, lr_high, lr_low, momentum_low, momentum_high)\n\nSGD optimiser with a cyclic learning rate and cyclic Nesterov momentum.\n\nDuring an epoch, the learning rate goes from lr_low to lr_high and then back to lr_low.\nThe momentum evolves in the opposite way, from high values to low values and then back to high values.\n\n\n\n\n\n","category":"type"},{"location":"reference/network/#AlphaZero.Network.Adam","page":"Network Interface","title":"AlphaZero.Network.Adam","text":"Adam(;lr)\n\nAdam optimiser.\n\n\n\n\n\n","category":"type"},{"location":"reference/omitted/#Miscellaneous","page":"Miscellaneous","title":"Miscellaneous","text":"","category":"section"},{"location":"reference/omitted/","page":"Miscellaneous","title":"Miscellaneous","text":"We gather on this page the documentation of internal utility functions that are omitted from the manual for brievety.","category":"page"},{"location":"reference/omitted/","page":"Miscellaneous","title":"Miscellaneous","text":"CurrentModule = AlphaZero","category":"page"},{"location":"reference/omitted/#Batchifying-oracles","page":"Miscellaneous","title":"Batchifying oracles","text":"","category":"section"},{"location":"reference/omitted/","page":"Miscellaneous","title":"Miscellaneous","text":"Batchifier\nBatchifier.launch_server\nBatchifier.client_done!\nBatchifier.BatchedOracle","category":"page"},{"location":"reference/omitted/#AlphaZero.Batchifier","page":"Miscellaneous","title":"AlphaZero.Batchifier","text":"Utilities for batchifying oracle evaluation.\n\nThe typical workflow should be the following:\n\nLaunch an inference server using launch_server to obtain a request channel.\nCreate several BatchedOracle that send queries along this channel.\nWhen a client to the server is done, it calls client_done! so that the server does not have to wait for its next query anymore.\n\n\n\n\n\n","category":"module"},{"location":"reference/omitted/#AlphaZero.Batchifier.launch_server","page":"Miscellaneous","title":"AlphaZero.Batchifier.launch_server","text":"launch_server(f; num_workers, batch_size)\n\nLaunch an inference requests server.\n\nArguments\n\nf is the function to be evaluated (e.g. a neural network). It takes a batch of inputs and returns a batch of results of similar size.\nnum_workers is the number of workers that are expected to query the server.\nbatch_size corresponds to the batch size that is used to evaluate f. Note that one must have batch_size <= num_workers\n\nHow to use\n\nThis function returns a channel along which workers can send queries. A query can be either:\n\n- `:none` when a worker is done and about to terminate\n- a named tuple with fields `query` (the input to be given to `f`) and\n  `answer_channel` (the channel the sever must use to return its answer).\n\nThe server stops automatically after all workers send :none.\n\n\n\n\n\n","category":"function"},{"location":"reference/omitted/#AlphaZero.Batchifier.client_done!","page":"Miscellaneous","title":"AlphaZero.Batchifier.client_done!","text":"client_done!(reqc)\n\nThis function is to be called every time a client to the inference server identified by request channel reqc is done and won't send queries anymore.\n\nIt should be called n times in total, where n is the number of workers that was indicated to launch_server.\n\nnote: Note\nThis function does not take a BatchedOracle as an argument as there can be several oracles per worker (e.g. a worker can simulate games between two players that each rely on a neural network).\n\n\n\n\n\n","category":"function"},{"location":"reference/omitted/#AlphaZero.Batchifier.BatchedOracle","page":"Miscellaneous","title":"AlphaZero.Batchifier.BatchedOracle","text":"BatchedOracle(reqc, preprocess=(x->x))\n\nCreate an oracle that delegates its job to an inference server.\n\nWhen called on a state, this oracle sends a query to the server identified by request channel reqc. This call is blocking until every other active worker also sends its query.\nA preprocess function can be provided to ransform the passed state before it is sent to the server as a query.\n\n\n\n\n\n","category":"type"},{"location":"reference/omitted/#KnetLib-and-FluxLib","page":"Miscellaneous","title":"KnetLib and FluxLib","text":"","category":"section"},{"location":"reference/omitted/","page":"Miscellaneous","title":"Miscellaneous","text":"#KnetLib\n#KnetLib.KNetwork\n#KnetLib.TwoHeadNetwork\nFluxLib\nFluxLib.FluxNetwork\nFluxLib.TwoHeadNetwork","category":"page"},{"location":"reference/omitted/#AlphaZero.FluxLib","page":"Miscellaneous","title":"AlphaZero.FluxLib","text":"This module provides utilities to build neural networks with Flux, along with a library of standard architectures.\n\n\n\n\n\n","category":"module"},{"location":"reference/omitted/#AlphaZero.FluxLib.FluxNetwork","page":"Miscellaneous","title":"AlphaZero.FluxLib.FluxNetwork","text":"FluxNetwork <: AbstractNetwork\n\nAbstract type for neural networks implemented using the Flux framework.\n\nThe regularized_params_ function must be overrided for all layers containing parameters that are subject to regularization.\n\nProvided that the above holds, FluxNetwork implements the full network interface with the following exceptions: Network.HyperParams, Network.hyperparams, Network.forward and Network.on_gpu.\n\n\n\n\n\n","category":"type"},{"location":"reference/omitted/#AlphaZero.FluxLib.TwoHeadNetwork","page":"Miscellaneous","title":"AlphaZero.FluxLib.TwoHeadNetwork","text":"TwoHeadNetwork <: FluxNetwork\n\nAn abstract type for two-head neural networks implemented with Flux.\n\nSubtypes are assumed to have fields hyper, gspec, common, vhead and phead. Based on those, an implementation is provided for Network.hyperparams, Network.game_spec, Network.forward and Network.on_gpu, leaving only Network.HyperParams to be implemented.\n\n\n\n\n\n","category":"type"},{"location":"reference/omitted/#Utilities","page":"Miscellaneous","title":"Utilities","text":"","category":"section"},{"location":"reference/omitted/","page":"Miscellaneous","title":"Miscellaneous","text":"Modules = [AlphaZero.Util]","category":"page"},{"location":"reference/omitted/#AlphaZero.Util.apply_temperature-Tuple{Any, Any}","page":"Miscellaneous","title":"AlphaZero.Util.apply_temperature","text":"apply_temperature(π, τ)\n\nApply temperature τ to probability distribution π. Handle the limit case where τ=0.\n\n\n\n\n\n","category":"method"},{"location":"reference/omitted/#AlphaZero.Util.cycle_iterator-Tuple{Any}","page":"Miscellaneous","title":"AlphaZero.Util.cycle_iterator","text":"cycle_iterator(iterator)\n\nGenerate an infinite cycle from an iterator.\n\n\n\n\n\n","category":"method"},{"location":"reference/omitted/#AlphaZero.Util.fix_probvec-Tuple{Any}","page":"Miscellaneous","title":"AlphaZero.Util.fix_probvec","text":"fix_probvec(π)\n\nConvert probability vector π to type Vector{Float32} and renormalize it if necessary.\n\nThis is useful as Distributions.isprobvec can be picky about its input when it does not sum to one due to numerical errors.\n\n\n\n\n\n","category":"method"},{"location":"reference/omitted/#AlphaZero.Util.generate_update_constructor-Tuple{Any}","page":"Miscellaneous","title":"AlphaZero.Util.generate_update_constructor","text":"generate_update_constructor(T)\n\nGenerate a new constructor for immutable type T that enables copying an existing structure while only updating a subset of its fields.\n\nFor example, given the following struct:\n\nstruct Point\n  x :: Int\n  y :: Int\nend\n\nThe generated code is equivalent to:\n\nPoint(pt; x=pt.x, y=pt.y) = Point(x, y)\n\n** This function may be deprecated in favor of Setfield.jl in the future.**\n\n\n\n\n\n","category":"method"},{"location":"reference/omitted/#AlphaZero.Util.mapreduce-NTuple{5, Any}","page":"Miscellaneous","title":"AlphaZero.Util.mapreduce","text":"mapreduce(make_worker, args, num_workers, combine, init)\n\nIn spirit, this computes reduce(combine, map(f, args); init) on num_workers where f is defined below.\n\nThe make_worker function must create a worker w with two fields:\n\na process function such that w.process(x) evaluates to f(x)\na terminate function to be called with no argument when the worker terminates.\n\nThis function only spawns workers on background threads (with id greater or equal than 1).\n\nnote: Note\nThis function makes one call to combine per computed element and so it should only be used when the associated synchronization cost is small compared to the cost of computing individual elements.\n\n\n\n\n\n","category":"method"},{"location":"reference/omitted/#AlphaZero.Util.momentum_smoothing-Tuple{Any, Any}","page":"Miscellaneous","title":"AlphaZero.Util.momentum_smoothing","text":"Same smoothing function that is used by Tensorboard to smooth time series.\n\n\n\n\n\n","category":"method"},{"location":"reference/omitted/#AlphaZero.Util.rand_categorical-Tuple{Any}","page":"Miscellaneous","title":"AlphaZero.Util.rand_categorical","text":"Draw a sample from a categorical distribution represented as a probability vector. See fix_probvec.\n\n\n\n\n\n","category":"method"},{"location":"reference/omitted/#AlphaZero.Util.@printing_errors-Tuple{Any}","page":"Miscellaneous","title":"AlphaZero.Util.@printing_errors","text":"@printing_errors expr\n\nEvaluate expression expr while printing any uncaught exception on stderr. This is useful to avoid silent falure of concurrent tasks, as explained in this issue.\n\n\n\n\n\n","category":"macro"},{"location":"tutorial/connect_four/#connect_four","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"","category":"section"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"In this tutorial, we demonstrate AlphaZero.jl by training a Connect Four agent without any form of supervision or prior knowledge. Although the game has been solved exactly with Alpha-beta pruning using domain-specific heuristics and optimizations, it is still a great challenge for reinforcement learning.[1]","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"[1]: To the best of our knowledge, none of the many existing Python implementations of AlphaZero are able to learn a player that beats a minmax baseline that plans at depth 2 (on a single desktop computer).","category":"page"},{"location":"tutorial/connect_four/#Setup-and-Training","page":"Training a Connect Four Agent","title":"Setup and Training","text":"","category":"section"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"AlphaZero.jl requires Julia version 1.6 or higher.","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"To run the experiments in this tutorial, we recommend having a CUDA compatible GPU with 4GB of memory or more. A 2GB GPU should work fine but you may have to reduce the batch size or the size of the neural network. Each training iteration took between one and two hours on a desktop computer with an Intel Core i5 9600K processor and an 8GB Nvidia RTX 2070 GPU.","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"To download AlphaZero.jl and start a new training session, just run the following:","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"git clone --branch v0.5.0 https://github.com/jonathan-laurent/AlphaZero.jl.git\ncd AlphaZero.jl\njulia --project -e 'import Pkg; Pkg.instantiate()'\njulia --project -e 'using AlphaZero; Scripts.train(\"connect-four\")'","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"Instead of using Scripts.train, one can do things more manually and run the following inside the Julia REPL:","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"using AlphaZero\nexperiment = Examples.experiments[\"connect-four\"]\nsession = Session(experiment, dir=\"sessions/connect-four\")\nresume!(session)","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"The second line loads a predefined experiment from the AlphaZero.Examples module. An experiment bundles together all the information necessary to spawn a training session and it is defined by the followng fields:","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"Field Description\ngspec Game to be played, which implements the game interface.\nparams AlphaZero hyperparameters.\nmknet Constructor for a neural network implementing the network interface.\nnetparams Network hyperparameters.\nbenchmark Benchmark that is run between training iterations.","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"The connect-four experiment is defined in file games/connect-four/params.jl. We copy it for reference at the end of this tutorial. Here are some highlights:","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"We use a two-headed convolutional ResNet similar to the one introduced in the AlphaGo Zero paper, although much smaller. Its tower consists of 5 residual blocks with 128 convolutional filters per layer, for a total of about 1.6M parameters (in contrast, the neural network from the AlphaGo Zero paper has about 100M parameters).\nDuring each iteration, the current agent plays 5000 games against itself, running 600 MCTS simulations to plan each move.[2] The move selection temperature is set to 1.0 during the first twenty moves of every game and then decreased progressively to 0.3.\nSelf-play data is accumulated in a memory buffer whose capacity grows from 400K samples (initially) to 1M samples (at iteration 15). For reference, assuming an average game duration of 35 moves, about 35 x 5000 = 175K new samples are generated at each iteration.","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"[2]: Compare those numbers with those of a popular Python implementation, which achieves iterations of similar duration when training its Othello agent but only runs 100 games and 25 MCTS simulations per move.","category":"page"},{"location":"tutorial/connect_four/#Initial-Benchmarks","page":"Training a Connect Four Agent","title":"Initial Benchmarks","text":"","category":"section"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"After launching the training script for the first time, you should see something like this:","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"(Image: Session CLI (init))","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"Before the first training iteration and between each iteration, the current AlphaZero agent is benchmarked against some baselines in a series of games (200 in this case) so as to provide a concrete measure of training progress. In this tutorial, we use two baselines:","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"A vanilla MCTS baseline that uses rollouts to estimate the value of new nodes.\nA minmax baseline that plans at depth 5 using a handcrafted heuristic.","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"Comparing two deterministic players is challenging as deterministic players will always play the same game repeatedly given a unique initial state. To add randomization, all players are instantiated with a small but nonzero move selection temperature.[3]","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"[3]: Note, however, that the minmax baseline is guaranteed to play a winning move whenever it sees one and to avoid moves it can prove to be losing within 5 steps (see MinMax.Player).","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"The redundancy indicator is helpful to diagnose a lack of randomization. It measures the quantity 1 - u  n where u is the total number of unique states that have been encountered (excluding the initial state) and n is the total number of encountered states, excluding the initial state and counting duplicates (see Report.Evaluation).","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"note: On leveraging symmetries\nAnother trick that we use to add randomization is to leverage the symmetry of the Connect Four board with respect to its central vertical axis: at each turn, the board is flipped along its central vertical axis with a fixed probability (see flip_probability).This is one of two ways in which AlphaZero.jl takes advantage of board symmetries, the other one being data augmentation (see use_symmetries). Board symmetries can be declared for new games by implementing the GameInterface.symmetries function.","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"As you can see, the AlphaZero agent can still win some games with a randomly initialized network, by relying on search alone for short term tactical decisions.","category":"page"},{"location":"tutorial/connect_four/#Training","page":"Training a Connect Four Agent","title":"Training","text":"","category":"section"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"After the initial benchmarks are done, the first training iteration can start. Each training iteration took about an hour on our hardware. The first iterations are typically on the shorter end, as games of self-play terminate more quickly and the memory buffer has yet to reach its final size.","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"(Image: Session CLI (first iteration))","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"Between the self-play and learning phase of each iteration, we perform an analysis of the memory buffer by partitioning samples according to how many moves remained until the end of the game when they were recorded. This is useful to monitor how well the neural network performs at different game stages. Separate statistics are also computed for the last batch of collected samples.","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"A description of all reported metrics can be found in Training Reports.","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"At the end of every iteration, benchmarks are run, summary plots are generated and the state of the current environment is saved on disk. This way, if training is interrupted for any reason, it can be resumed from the last saved state by simply running Scripts.train(\"connect-four\") again.","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"All summary plots generated during the training of our agent can be downloaded here.","category":"page"},{"location":"tutorial/connect_four/#Examining-the-current-agent","page":"Training a Connect Four Agent","title":"Examining the current agent","text":"","category":"section"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"At any time during training, you can start an interactive command interpreter to investigate the current agent:","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"julia --project -e 'using AlphaZero; Scripts.explore(\"connect-four\")'","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"(Image: Explorer)","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"If you just want to play and not be bothered with metrics, you can use Scripts.play instead of Scripts.explore.","category":"page"},{"location":"tutorial/connect_four/#Experimental-Results","page":"Training a Connect Four Agent","title":"Experimental Results","text":"","category":"section"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"We plot below the evolution of the win rate of our AlphaZero agent against our two baselines.","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"(Image: Win rate evolution (AlphaZero))","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"It is important to note that the AlphaZero agent is never exposed to those baselines during training and therefore cannot learn from them.","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"We also evaluate the performances of the neural network alone against the same baselines: instead of plugging it into MCTS, we just play the action that is assigned the highest prior probability at each state.","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"(Image: Win rate evolution (network only))","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"Unsurprisingly, the network alone is initially unable to win a single game. However, it ends up significantly stronger than the minmax baseline despite not being able to perform any search.","category":"page"},{"location":"tutorial/connect_four/#Benchmark-against-a-perfect-solver","page":"Training a Connect Four Agent","title":"Benchmark against a perfect solver","text":"","category":"section"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"Finally, we benchmark our AlphaZero agent against a perfect Connect Four solver. To do so, we evaluate the rate at which it makes mistakes on different test datasets available here. Each dataset gathers positions of similar game depth and level of difficulty. More specificaly:","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"A position is labelled \"Beginning\" if it is at most 14 moves away from the beginning of the game. It is labelled \"End\" if it is more than 28 moves away. Otherwise, it is labelled \"Middle\".\nA position is labelled \"Easy\" if it is less than 14 moves away from the end of the game given perfect play from both players. It is labelled \"Hard\" if it is more than 28 moves away. Otherwise, it is labelled \"Medium\".","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"On each dataset, we measure the mistake rate of our AlphaZero agent at different stages of training. An action suggested by the agent is said to be a mistake if another action is available with a strictly higher optimal Q-value (which can be either -1, 0 or 1). For example, the agent makes a mistake if it plays an action that results in a losing or draw state (assuming perfect play from both player) when another action exists that would have led to a winning state.","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"We plot the results below. The blue curves describe the mistake rate of our AlphaZero agent as a function of the number of training iterations on different datasets. The orange lines indicate the mistake rate of the minmax baseline. This experiment can be replicated using the script at games/connect-four/scripts/pons_benchmark.jl.","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"(Image: Pons benchark)","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"As you can see, while our AlphaZero agent makes few mistakes that could be detected by planning up to 14 moves ahead, it is still imperfect at making longer term strategical decisions and playing overtures.","category":"page"},{"location":"tutorial/connect_four/#You-can-do-better!","page":"Training a Connect Four Agent","title":"You can do better!","text":"","category":"section"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"The AlphaZero agent that is demonstrated in this tutorial went through little hyperparameters tuning and it can certainly be improved significantly. We encourage you to make your own tuning experiments and share the results.","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"The Oracle series discusses hyperparameters tuning for a Connect Four agent. However, their configuration is optimized for more powerful hardware than is targeted by this tutorial [4] In particular, they use a network that is about ten times larger and generate twice as much training data per iteration.","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"Our current configuration results from an attempt at downscaling Oracle's setup. Doing so is not trivial as hyperparameters are interrelated in a complex fashion. For example, we found that reducing exploration slightly results in faster training for our downscaled agent. Also:","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"We chose to use the Adam optimizer instead of SGD with cyclical rates, as it introduces less hyperparameters and is generally more forgiving.\nRaising the number of MCTS simulations per move from 300 to its current value of 600 resulted in faster learning, despite the additional computational cost per iteration.\nTo tune the MCTS hyperparameters, we trained a first version of AlphaZero based on an initial guess. Then, we ran a grid-search by plugging different MCTS parameters into the resulting agent and optimizing its score against the minmax baseline. We iterated again with a second training session and a second grid-search to get the current parameters. Notably, tuning the prior_temperature parameter (which does not exist in the original version of AlphaZero and was introduced by LeelaZero) resulted in a significant improvement.","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"Apart from that, most hyperparameters are the result of a single guess. We are looking forward to seeing how you can improve our Connect Four agent, with or without the help of better hardware!","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"[4]: Their closed-source C++ implementation of AlphaZero is also faster. A big part of the difference is apparently coming from them using Int8 quantization to accelerate network inference. See Contributions Guide.","category":"page"},{"location":"tutorial/connect_four/#c4-config","page":"Training a Connect Four Agent","title":"Full Training Configuration","text":"","category":"section"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"Here, we copy the full definition of the connect-four experiment from games/connect-four/params.jl.","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"Note that, in addition to having standard keyword constructors, parameter types have constructors that implement the record update operation from functional languages. For example, Params(p, num_iters=100) builds a Params object that is identical to p for every field, except num_iters which is set to 100.","category":"page"},{"location":"tutorial/connect_four/","page":"Training a Connect Four Agent","title":"Training a Connect Four Agent","text":"#####\n##### Training hyperparameters\n#####\n\nNetwork = NetLib.ResNet\n\nnetparams = NetLib.ResNetHP(\n  num_filters=128,\n  num_blocks=5,\n  conv_kernel_size=(3, 3),\n  num_policy_head_filters=32,\n  num_value_head_filters=32,\n  batch_norm_momentum=0.1)\n\nself_play = SelfPlayParams(\n  sim=SimParams(\n    num_games=5000,\n    num_workers=128,\n    batch_size=64,\n    use_gpu=true,\n    reset_every=2,\n    flip_probability=0.,\n    alternate_colors=false),\n  mcts=MctsParams(\n    num_iters_per_turn=600,\n    cpuct=2.0,\n    prior_temperature=1.0,\n    temperature=PLSchedule([0, 20, 30], [1.0, 1.0, 0.3]),\n    dirichlet_noise_ϵ=0.25,\n    dirichlet_noise_α=1.0))\n\narena = ArenaParams(\n  sim=SimParams(\n    num_games=128,\n    num_workers=128,\n    batch_size=128,\n    use_gpu=true,\n    reset_every=2,\n    flip_probability=0.5,\n    alternate_colors=true),\n  mcts=MctsParams(\n    self_play.mcts,\n    temperature=ConstSchedule(0.2),\n    dirichlet_noise_ϵ=0.05),\n  update_threshold=0.05)\n\nlearning = LearningParams(\n  use_gpu=true,\n  use_position_averaging=true,\n  samples_weighing_policy=LOG_WEIGHT,\n  batch_size=1024,\n  loss_computation_batch_size=1024,\n  optimiser=Adam(lr=2e-3),\n  l2_regularization=1e-4,\n  nonvalidity_penalty=1.,\n  min_checkpoints_per_epoch=1,\n  max_batches_per_checkpoint=2000,\n  num_checkpoints=1)\n\nparams = Params(\n  arena=arena,\n  self_play=self_play,\n  learning=learning,\n  num_iters=15,\n  ternary_rewards=true,\n  use_symmetries=true,\n  memory_analysis=nothing,\n  mem_buffer_size=PLSchedule(\n  [      0,        15],\n  [400_000, 1_000_000]))\n\n#####\n##### Evaluation benchmark\n#####\n\nmcts_baseline =\n  Benchmark.MctsRollouts(\n    MctsParams(\n      arena.mcts,\n      num_iters_per_turn=1000,\n      cpuct=1.))\n\nminmax_baseline = Benchmark.MinMaxTS(\n  depth=5,\n  τ=0.2,\n  amplify_rewards=true)\n\nalphazero_player = Benchmark.Full(arena.mcts)\n\nnetwork_player = Benchmark.NetworkOnly(τ=0.5)\n\nbenchmark_sim = SimParams(\n  arena.sim;\n  num_games=256,\n  num_workers=256,\n  batch_size=256,\n  alternate_colors=false)\n\nbenchmark = [\n  Benchmark.Duel(alphazero_player, mcts_baseline,   benchmark_sim),\n  Benchmark.Duel(alphazero_player, minmax_baseline, benchmark_sim),\n  Benchmark.Duel(network_player,   mcts_baseline,   benchmark_sim),\n  Benchmark.Duel(network_player,   minmax_baseline, benchmark_sim)\n]\n\n#####\n##### Wrapping up in an experiment\n#####\n\nexperiment = Experiment(\"connect-four\",\n  GameSpec(), params, Network, netparams, benchmark)","category":"page"},{"location":"reference/params/#params","page":"Training Parameters","title":"Training Parameters","text":"","category":"section"},{"location":"reference/params/","page":"Training Parameters","title":"Training Parameters","text":"CurrentModule = AlphaZero","category":"page"},{"location":"reference/params/#General","page":"Training Parameters","title":"General","text":"","category":"section"},{"location":"reference/params/","page":"Training Parameters","title":"Training Parameters","text":"Params","category":"page"},{"location":"reference/params/#AlphaZero.Params","page":"Training Parameters","title":"AlphaZero.Params","text":"Params\n\nThe AlphaZero training hyperparameters.\n\nParameter Type Default\nself_play SelfPlayParams -\nlearning LearningParams -\narena Union{Nothing, ArenaParams} -\nmemory_analysis Union{Nothing, MemAnalysisParams} nothing\nnum_iters Int -\nuse_symmetries Bool false\nternary_rewards Bool false\nmem_buffer_size PLSchedule{Int} -\n\nExplanation\n\nThe AlphaZero training process consists in num_iters iterations. Each iteration can be decomposed into a self-play phase (see SelfPlayParams) and a learning phase (see LearningParams).\n\nternary_rewards: set to true if the rewards issued by  the game environment always belong to -1 0 1 so that  the logging and profiling tools can take advantage of this property.\nuse_symmetries: if set to true, board symmetries are used for  data augmentation before learning.\nmem_buffer_size: size schedule of the memory buffer, in terms of number  of samples. It is typical to start with a small memory buffer that is grown  progressively so as to wash out the initial low-quality self-play data  more quickly.\nmemory_analysis: parameters for the memory analysis step that is  performed at each iteration (see MemAnalysisParams), or  nothing if no analysis is to be performed.\n\nAlphaGo Zero Parameters\n\nIn the original AlphaGo Zero paper:\n\nAbout 5 millions games of self-play are played across 200 iterations.\nThe memory buffer contains 500K games, which makes about 100M samples as an average game of Go lasts about 200 turns.\n\n\n\n\n\n","category":"type"},{"location":"reference/params/#Self-Play","page":"Training Parameters","title":"Self-Play","text":"","category":"section"},{"location":"reference/params/","page":"Training Parameters","title":"Training Parameters","text":"SelfPlayParams","category":"page"},{"location":"reference/params/#AlphaZero.SelfPlayParams","page":"Training Parameters","title":"AlphaZero.SelfPlayParams","text":"SelfPlayParams\n\nParameters governing self-play.\n\nParameter Type Default\nmcts MctsParams -\nsim SimParams -\n\nAlphaGo Zero Parameters\n\nIn the original AlphaGo Zero paper, sim.num_games=25_000 (5 millions games of self-play across 200 iterations).\n\n\n\n\n\n","category":"type"},{"location":"reference/params/#Learning","page":"Training Parameters","title":"Learning","text":"","category":"section"},{"location":"reference/params/","page":"Training Parameters","title":"Training Parameters","text":"LearningParams\nSamplesWeighingPolicy","category":"page"},{"location":"reference/params/#AlphaZero.LearningParams","page":"Training Parameters","title":"AlphaZero.LearningParams","text":"LearningParams\n\nParameters governing the learning phase of a training iteration, where the neural network is updated to fit the data in the memory buffer.\n\nParameter Type Default\nuse_gpu Bool false\nuse_position_averaging Bool true\nsamples_weighing_policy SamplesWeighingPolicy -\noptimiser OptimiserSpec -\nl2_regularization Float32 -\nrewards_renormalization Float32 1f0\nnonvalidity_penalty Float32 1f0\nbatch_size Int -\nloss_computation_batch_size Int -\nmin_checkpoints_per_epoch Float64 -\nmax_batches_per_checkpoint Int -\nnum_checkpoints Int -\n\nDescription\n\nThe neural network goes through num_checkpoints series of n updates using batches of size batch_size drawn from memory, where n is defined as follows:\n\nn = min(max_batches_per_checkpoint, ntotal ÷ min_checkpoints_per_epoch)\n\nwith ntotal the total number of batches in memory. Between each series, the current network is evaluated against the best network so far (see ArenaParams).\n\nnonvalidity_penalty is the multiplicative constant of a loss term that  corresponds to the average probability weight that the network puts on  invalid actions.\nbatch_size is the batch size used for gradient descent.\nloss_computation_batch_size is the batch size that is used to compute the loss between each epochs.\nAll rewards are divided by rewards_renormalization before the MSE loss is computed.\nIf use_position_averaging is set to true, samples in memory that correspond to the same board position are averaged together. The merged sample is reweighted according to samples_weighing_policy.\n\nAlphaGo Zero Parameters\n\nIn the original AlphaGo Zero paper:\n\nThe batch size for gradient updates is 2048.\nThe L2 regularization parameter is set to 10^-4.\nCheckpoints are produced every 1000 training steps, which corresponds to seeing about 20% of the samples in the memory buffer: (1000  2048)  10^7   02.\nIt is unclear how many checkpoints are taken or how many training steps are performed in total.\n\n\n\n\n\n","category":"type"},{"location":"reference/params/#AlphaZero.SamplesWeighingPolicy","page":"Training Parameters","title":"AlphaZero.SamplesWeighingPolicy","text":"SamplesWeighingPolicy\n\nDuring self-play, early board positions are possibly encountered many times across several games. The corresponding samples can be merged together and given a weight W that is a nondecreasing function of the number n of merged samples:\n\nCONSTANT_WEIGHT: W(n) = 1\nLOG_WEIGHT: W(n) = log_2(n) + 1\nLINEAR_WEIGHT: W(n) = n\n\n\n\n\n\n","category":"type"},{"location":"reference/params/#Arena","page":"Training Parameters","title":"Arena","text":"","category":"section"},{"location":"reference/params/","page":"Training Parameters","title":"Training Parameters","text":"ArenaParams","category":"page"},{"location":"reference/params/#AlphaZero.ArenaParams","page":"Training Parameters","title":"AlphaZero.ArenaParams","text":"ArenaParams\n\nParameters that govern the evaluation process that compares the current neural network with the best one seen so far (which is used to generate data).\n\nParameter Type Default\nmcts MctsParams -\nsim SimParams -\nupdate_threshold Float64 -\n\nExplanation (two-player games)\n\nThe two competing networks are instantiated into two MCTS players of parameter mcts and then play sim.num_games games.\nThe evaluated network replaces the current best one if its average collected reward is greater or equal than update_threshold.\n\nExplanation (single-player games)\n\nThe two competing networks play sim.num_games games each.\nThe evaluated network replaces the current best one if its average collected rewards exceeds the average collected reward of the old one by update_threshold at least. \n\nRemarks\n\nSee necessary_samples to make an informed choice for sim.num_games.\n\nAlphaGo Zero Parameters\n\nIn the original AlphaGo Zero paper, 400 games are played to evaluate a network and the update_threshold parameter is set to a value that corresponds to a 55% win rate.\n\n\n\n\n\n","category":"type"},{"location":"reference/params/#Memory-Analysis","page":"Training Parameters","title":"Memory Analysis","text":"","category":"section"},{"location":"reference/params/","page":"Training Parameters","title":"Training Parameters","text":"MemAnalysisParams","category":"page"},{"location":"reference/params/#AlphaZero.MemAnalysisParams","page":"Training Parameters","title":"AlphaZero.MemAnalysisParams","text":"MemAnalysisParams\n\nParameters governing the analysis of the memory buffer (for debugging and profiling purposes).\n\nParameter Type Default\nnum_game_stages Int -\n\nExplanation\n\nThe memory analysis consists in partitioning the memory buffer in num_game_stages parts of equal size, according to the number of remaining moves until the end of the game for each sample. Then, the quality of the predictions of the current neural network is evaluated on each subset (see Report.Memory).\n\nThis is useful to get an idea of how the neural network performance varies depending on the game stage (typically, good value estimates for endgame board positions are available earlier in the training process than good values for middlegame positions).\n\n\n\n\n\n","category":"type"},{"location":"reference/params/#MCTS","page":"Training Parameters","title":"MCTS","text":"","category":"section"},{"location":"reference/params/","page":"Training Parameters","title":"Training Parameters","text":"MctsParams","category":"page"},{"location":"reference/params/#AlphaZero.MctsParams","page":"Training Parameters","title":"AlphaZero.MctsParams","text":"Parameters of an MCTS player.\n\nParameter Type Default\nnum_iters_per_turn Int -\ngamma Float64 1.\ncpuct Float64 1.\ntemperature AbstractSchedule{Float64} ConstSchedule(1.)\ndirichlet_noise_ϵ Float64 -\ndirichlet_noise_α Float64 -\nprior_temperature Float64 1.\n\nExplanation\n\nAn MCTS player picks an action as follows. Given a game state, it launches num_iters_per_turn MCTS iterations, with UCT exploration constant cpuct. Rewards are discounted using the gamma factor.\n\nThen, an action is picked according to the distribution π where π_i  n_i^1τ with n_i the number of times that the i^textth action was visited and τ the temperature parameter.\n\nIt is typical to use a high value of the temperature parameter τ during the first moves of a game to increase exploration and then switch to a small value. Therefore, temperature is am AbstractSchedule.\n\nFor information on parameters cpuct, dirichlet_noise_ϵ, dirichlet_noise_α and prior_temperature, see MCTS.Env.\n\nAlphaGo Zero Parameters\n\nIn the original AlphaGo Zero paper:\n\nThe discount factor gamma is set to 1.\nThe number of MCTS iterations per move is 1600, which corresponds to 0.4s of computation time.\nThe temperature is set to 1 for the 30 first moves and then to an infinitesimal value.\nThe ϵ parameter for the Dirichlet noise is set to 025 and the α parameter to 003, which is consistent with the heuristic of using α = 10n with n the maximum number of possibles moves, which is 19  19 + 1 = 362 in the case of Go.\n\n\n\n\n\n","category":"type"},{"location":"reference/params/#Simulations","page":"Training Parameters","title":"Simulations","text":"","category":"section"},{"location":"reference/params/","page":"Training Parameters","title":"Training Parameters","text":"SimParams","category":"page"},{"location":"reference/params/#AlphaZero.SimParams","page":"Training Parameters","title":"AlphaZero.SimParams","text":"SimParams\n\nParameters for parallel game simulations.\n\nThese parameters are common to self-play data generation, neural network evaluation and benchmarking.\n\nParameter Type Default\nnum_games Int -\nnum_workers Int -\nbatch_size Int -\nuse_gpu Bool false\nfill_batches Bool true\nflip_probability Float64 0.\nreset_every Union{Nothing, Int} 1\nalternate_colors Float64 false\n\nExplanations\n\nOn each machine (process), num_workers simulation tasks are spawned. Inference requests are processed by an inference server by batch of size batch_size. Note that we must have batch_size <= num_workers.\nIf fill_batches is set to true, we make sure that batches sent to the neural network for inference have constant size.\nBoth players are reset (e.g. their MCTS trees are emptied) every reset_every games (or never if nothing is passed).\nTo add randomization and before every game turn, the game board is \"flipped\" according to a symmetric transformation with probability flip_probability.\nIn the case of (symmetric) two-player games and if alternate_colors is set totrue, then the colors of both players are swapped between each simulated game.\n\n\n\n\n\n","category":"type"},{"location":"reference/params/#Utilities","page":"Training Parameters","title":"Utilities","text":"","category":"section"},{"location":"reference/params/","page":"Training Parameters","title":"Training Parameters","text":"necessary_samples\nAbstractSchedule\nStepSchedule\nPLSchedule\nCyclicSchedule","category":"page"},{"location":"reference/params/#AlphaZero.necessary_samples","page":"Training Parameters","title":"AlphaZero.necessary_samples","text":"necessary_samples(ϵ, β) = log(1 / β) / (2 * ϵ^2)\n\nCompute the number of times N that a random variable X sim textBer(p) has to be sampled so that if the empirical average of X is greather than 12 + ϵ, then p  12 with probability at least 1-β.\n\nThis bound is based on Hoeffding's inequality .\n\n\n\n\n\n","category":"function"},{"location":"reference/params/#AlphaZero.AbstractSchedule","page":"Training Parameters","title":"AlphaZero.AbstractSchedule","text":"AbstractSchedule{R}\n\nAbstract type for a parameter schedule, which represents a function from nonnegative integers to numbers of type R. Subtypes must implement the getindex(s::AbstractSchedule, i::Int) operator.\n\n\n\n\n\n","category":"type"},{"location":"reference/params/#AlphaZero.StepSchedule","page":"Training Parameters","title":"AlphaZero.StepSchedule","text":"StepSchedule{R} <: AbstractSchedule{R}\n\nType for step function schedules.\n\nConstructor\n\nStepSchedule(;start, change_at, values)\n\nReturn a schedule that has initial value start. For all i, the schedule takes value values[i] at step change_at[i].\n\n\n\n\n\n","category":"type"},{"location":"reference/params/#AlphaZero.PLSchedule","page":"Training Parameters","title":"AlphaZero.PLSchedule","text":"PLSchedule{R} <: AbstractSchedule{R}\n\nType for piecewise linear schedules.\n\nConstructors\n\nPLSchedule(cst)\n\nReturn a schedule with a constant value cst.\n\nPLSchedule(xs, ys)\n\nReturn a piecewise linear schedule such that:\n\nFor all i, (xs[i], ys[i]) belongs to the schedule's graph.\nBefore xs[1], the schedule has value ys[1].\nAfter xs[end], the schedule has value ys[end].\n\n\n\n\n\n","category":"type"},{"location":"reference/params/#AlphaZero.CyclicSchedule","page":"Training Parameters","title":"AlphaZero.CyclicSchedule","text":"CyclicSchedule(base, mid, term; n, xmid=0.45, xback=0.90)\n\nReturn the PLSchedule that is typically used for cyclic learning rate scheduling.\n\n\n\n\n\n","category":"function"},{"location":"reference/reports/#reports","page":"Training Reports","title":"Training Reports","text":"","category":"section"},{"location":"reference/reports/","page":"Training Reports","title":"Training Reports","text":"CurrentModule = AlphaZero","category":"page"},{"location":"reference/reports/","page":"Training Reports","title":"Training Reports","text":"Report","category":"page"},{"location":"reference/reports/#AlphaZero.Report","page":"Training Reports","title":"AlphaZero.Report","text":"Analytical reports generated during training, for debugging and hyperparameters tuning.\n\n\n\n\n\n","category":"module"},{"location":"reference/reports/","page":"Training Reports","title":"Training Reports","text":"Report.Initial\nReport.Iteration\nReport.Perfs","category":"page"},{"location":"reference/reports/#AlphaZero.Report.Initial","page":"Training Reports","title":"AlphaZero.Report.Initial","text":"Report.Initial\n\nReport summarizing the configuration of an agent before training starts.\n\nnum_network_parameters: see Network.num_parameters\nnum_network_regularized_parameters:   see Network.num_regularized_parameters\nmcts_footprint_per_node: see MCTS.memory_footprint_per_node\n\n\n\n\n\n","category":"type"},{"location":"reference/reports/#AlphaZero.Report.Iteration","page":"Training Reports","title":"AlphaZero.Report.Iteration","text":"Report.Iteration\n\nReport generated after each training iteration.\n\nFields self_play, memory, learning have types Report.SelfPlay,   Report.SelfPlay and Report.Learning respectively\nFields perfs_self_play, perfs_memory_analysis and perfs_learning are   performance reports for the different phases of the iteration,   with type Report.Perfs\n\n\n\n\n\n","category":"type"},{"location":"reference/reports/#AlphaZero.Report.Perfs","page":"Training Reports","title":"AlphaZero.Report.Perfs","text":"Report.Perfs\n\nPerformances report for a subroutine.\n\ntime: total time spent, in seconds\nallocated: amount of memory allocated, in bytes\ngc_time: total amount of time spent in the garbage collector\n\n\n\n\n\n","category":"type"},{"location":"reference/reports/#Self-Play-Phase","page":"Training Reports","title":"Self-Play Phase","text":"","category":"section"},{"location":"reference/reports/","page":"Training Reports","title":"Training Reports","text":"Report.SelfPlay","category":"page"},{"location":"reference/reports/#AlphaZero.Report.SelfPlay","page":"Training Reports","title":"AlphaZero.Report.SelfPlay","text":"Report.SelfPlay\n\nReport generated after the self-play phase of an iteration.\n\nsamples_gen_speed: average number of samples generated per second\naverage_exploration_depth: see MCTS.average_exploration_depth\nmcts_memory_footprint: estimation of the maximal memory footprint of the   MCTS tree during self-play, as computed by   MCTS.approximate_memory_footprint\nmemory_size: number of samples in the memory buffer at the end of the   self-play phase\nmemory_num_distinct_boards: number of distinct board positions in the   memory buffer at the end of the self-play phase\n\n\n\n\n\n","category":"type"},{"location":"reference/reports/#Memory-Analysis-Phase","page":"Training Reports","title":"Memory Analysis Phase","text":"","category":"section"},{"location":"reference/reports/","page":"Training Reports","title":"Training Reports","text":"Report.Memory\nReport.Samples\nReport.StageSamples","category":"page"},{"location":"reference/reports/#AlphaZero.Report.Memory","page":"Training Reports","title":"AlphaZero.Report.Memory","text":"Report.Memory\n\nReport generated by the memory analysis phase of an iteration. It features statistics for\n\nthe whole memory buffer (all_samples::Report.Samples)\nthe samples collected during the last self-play iteration  (latest_batch::Report.Samples)\nthe subsets of the memory buffer corresponding to different game stages:  (per_game_stage::Vector{Report.StageSamples})\n\nSee MemAnalysisParams.\n\n\n\n\n\n","category":"type"},{"location":"reference/reports/#AlphaZero.Report.Samples","page":"Training Reports","title":"AlphaZero.Report.Samples","text":"Report.Samples\n\nStatistics about a set of samples, as collected during memory analysis.\n\nnum_samples: total number of samples\nnum_boards: number of distinct board positions\nWtot: total weight of the samples\nstatus: Report.LearningStatus statistics of the current network   on the samples\n\n\n\n\n\n","category":"type"},{"location":"reference/reports/#AlphaZero.Report.StageSamples","page":"Training Reports","title":"AlphaZero.Report.StageSamples","text":"Report.StageSamples\n\nStatistics for the samples corresponding to a particular game stage, as collected during memory analysis.\n\nThe samples whose statistics are collected in the samples_stats field correspond to historical positions where the number of remaining moves until the end of the game was in the range defined by the min_remaining_length and max_remaining_length fields.\n\n\n\n\n\n","category":"type"},{"location":"reference/reports/#Learning-Phase","page":"Training Reports","title":"Learning Phase","text":"","category":"section"},{"location":"reference/reports/","page":"Training Reports","title":"Training Reports","text":"Report.Learning\nReport.Checkpoint\nReport.LearningStatus\nReport.Loss","category":"page"},{"location":"reference/reports/#AlphaZero.Report.Learning","page":"Training Reports","title":"AlphaZero.Report.Learning","text":"Report.Learning\n\nReport generated at the end of the learning phase of an iteration.\n\ntime_convert, time_loss, time_train and time_eval are the   amounts of time (in seconds) spent at converting the samples,   computing losses, performing gradient updates and evaluating checkpoints   respectively\ninitial_status: status before the learning phase, as an object of type   Report.LearningStatus\nlosses: loss value on each minibatch\ncheckpoints: vector of Report.Checkpoint reports\nnn_replaced: true if the best neural network was replaced\n\n\n\n\n\n","category":"type"},{"location":"reference/reports/#AlphaZero.Report.Checkpoint","page":"Training Reports","title":"AlphaZero.Report.Checkpoint","text":"Report.Checkpoint\n\nReport generated after a checkpoint evaluation.\n\nbatch_id: number of batches after which the checkpoint was computed\nevaluation: evaluation report from the arena, of type Report.Evaluation\nstatus_after: learning status at the checkpoint, as an object of type  Report.LearningStatus\nnn_replaced: true if the current best neural network was updated after  the checkpoint\n\n\n\n\n\n","category":"type"},{"location":"reference/reports/#AlphaZero.Report.LearningStatus","page":"Training Reports","title":"AlphaZero.Report.LearningStatus","text":"Report.LearningStatus\n\nStatistics about the performance of the neural network on a subset of the memory buffer.\n\nloss: detailed loss on the samples, as an object of type   Report.Loss\nHp: average entropy of the π component of samples (MCTS policy);   this quantity is independent of the network and therefore constant   during a learning iteration\nHpnet: average entropy of the network's prescribed policy on the samples\n\n\n\n\n\n","category":"type"},{"location":"reference/reports/#AlphaZero.Report.Loss","page":"Training Reports","title":"AlphaZero.Report.Loss","text":"Report.Loss\n\nDecomposition of the loss in a sum of terms (all have type Float32).\n\nL is the total loss: L == Lp + Lv + Lreg + Linv\nLp is the policy cross-entropy loss term\nLv is the average value mean square error\nLreg is the L2 regularization loss term\nLinv is the loss term penalizing the average weight put by the network on invalid actions\n\n\n\n\n\n","category":"type"},{"location":"reference/reports/#Evaluatons-and-benchmarks","page":"Training Reports","title":"Evaluatons and benchmarks","text":"","category":"section"},{"location":"reference/reports/","page":"Training Reports","title":"Training Reports","text":"Report.Evaluation\nReport.Benchmark","category":"page"},{"location":"reference/reports/#AlphaZero.Report.Evaluation","page":"Training Reports","title":"AlphaZero.Report.Evaluation","text":"Report.Evaluation\n\nThe outcome of evaluating a player against a baseline player.\n\nTwo-player Games\n\nrewards is the sequence of rewards collected by the evaluated player\navgr is the average reward collected by the evaluated player\nbaseline_rewards is nothing\n\nSingle-player Games\n\nrewards is the sequence of rewards collected by the evaluated player\nbaseline_rewards is the sequence of rewards collected by the baseline player\navgr is equal to mean(rewards) - mean(baseline_rewards)\n\nCommon Fields\n\nlegend is a string describing the evaluation\nredundancy is the ratio of duplicate positions encountered during the  evaluation, not counting the initial position. If this number is too high,  you may want to increase the move selection temperature.\ntime is the computing time spent running the evaluation, in seconds\n\n\n\n\n\n","category":"type"},{"location":"reference/reports/#AlphaZero.Report.Benchmark","page":"Training Reports","title":"AlphaZero.Report.Benchmark","text":"const Report.Benchmark = Vector{Report.Evaluation}\n\nA benchmark report is a vector of Evaluation objects.\n\n\n\n\n\n","category":"type"},{"location":"reference/environment/#environment","page":"Environment","title":"Environment","text":"","category":"section"},{"location":"reference/environment/","page":"Environment","title":"Environment","text":"CurrentModule = AlphaZero","category":"page"},{"location":"reference/environment/","page":"Environment","title":"Environment","text":"Env\nHandlers\nget_experience(::Env)\ninitial_report\ntrain!(env::Env)","category":"page"},{"location":"reference/environment/#AlphaZero.Env","page":"Environment","title":"AlphaZero.Env","text":"Env\n\nType for an AlphZero environment.\n\nThe environment features the current neural network, the best neural network seen so far that is used for data generation, a memory buffer and an iteration counter.\n\nConstructor\n\nEnv(game_spec, params, curnn, bestnn=copy(curnn), experience=[], itc=0)\n\nConstruct a new AlphaZero environment:\n\ngame_spec specified the game being played\nparams has type Params\ncurnn is the current neural network and has type AbstractNetwork\nbestnn is the best neural network so far, which is used for data generation\nexperience is the initial content of the memory buffer  as a vector of TrainingSample\nitc is the value of the iteration counter (0 at the start of training)\n\n\n\n\n\n","category":"type"},{"location":"reference/environment/#AlphaZero.Handlers","page":"Environment","title":"AlphaZero.Handlers","text":"Handlers\n\nNamespace for the callback functions that are used during training. This enables logging, saving and plotting to be implemented separately. An example handler object is Session.\n\nAll callback functions take a handler object h as their first argument and sometimes a second argment r that consists in a report.\n\nCallback Comment\niteration_started(h) called at the beggining of an iteration\nself_play_started(h) called once per iter before self play starts\ngame_played(h) called after each game of self play\nself_play_finished(h, r) sends report: Report.SelfPlay\nmemory_analyzed(h, r) sends report: Report.Memory\nlearning_started(h) called at the beginning of the learning phase\nupdates_started(h, r) sends report: Report.LearningStatus\nupdates_finished(h, r) sends report: Report.LearningStatus\ncheckpoint_started(h) called before a checkpoint evaluation starts\ncheckpoint_game_played(h) called after each arena game\ncheckpoint_finished(h, r) sends report: Report.Checkpoint\nlearning_finished(h, r) sends report: Report.Learning\niteration_finished(h, r) sends report: Report.Iteration\ntraining_finished(h) called once at the end of training\n\n\n\n\n\n","category":"module"},{"location":"reference/environment/#AlphaZero.get_experience-Tuple{Env}","page":"Environment","title":"AlphaZero.get_experience","text":"get_experience(env::Env)\n\nReturn the content of the agent's memory as a vector of TrainingSample.\n\n\n\n\n\n","category":"method"},{"location":"reference/environment/#AlphaZero.initial_report","page":"Environment","title":"AlphaZero.initial_report","text":"initial_report(env::Env)\n\nReturn a report summarizing the configuration of agent before training starts, as an object of type Report.Initial.\n\n\n\n\n\n","category":"function"},{"location":"reference/environment/#AlphaZero.train!-Tuple{Env}","page":"Environment","title":"AlphaZero.train!","text":"train!(env::Env, handler=nothing)\n\nStart or resume the training of an AlphaZero agent.\n\nA handler object can be passed that implements a subset of the callback functions defined in Handlers.\n\n\n\n\n\n","category":"method"},{"location":"reference/mcts/#mcts","page":"MCTS","title":"MCTS","text":"","category":"section"},{"location":"reference/mcts/","page":"MCTS","title":"MCTS","text":"CurrentModule = AlphaZero.MCTS","category":"page"},{"location":"reference/mcts/","page":"MCTS","title":"MCTS","text":"MCTS","category":"page"},{"location":"reference/mcts/#AlphaZero.MCTS","page":"MCTS","title":"AlphaZero.MCTS","text":"A generic, standalone implementation of Monte Carlo Tree Search. It can be used on any game that implements GameInterface and with any external oracle.\n\nOracle Interface\n\nAn oracle can be any function or callable object.\n\noracle(state)\n\nevaluates a single state from the current player's perspective and returns  a pair (P, V) where:\n\nP is a probability vector on GI.available_actions(GI.init(gspec, state))\nV is a scalar estimating the value or win probability for white.\n\n\n\n\n\n","category":"module"},{"location":"reference/mcts/#Standard-Oracles","page":"MCTS","title":"Standard Oracles","text":"","category":"section"},{"location":"reference/mcts/","page":"MCTS","title":"MCTS","text":"RolloutOracle","category":"page"},{"location":"reference/mcts/#AlphaZero.MCTS.RolloutOracle","page":"MCTS","title":"AlphaZero.MCTS.RolloutOracle","text":"MCTS.RolloutOracle(game_spec::AbstractGameSpec, γ=1.) <: Function\n\nThis oracle estimates the value of a position by simulating a random game from it (a rollout). Moreover, it puts a uniform prior on available actions. Therefore, it can be used to implement the \"vanilla\" MCTS algorithm.\n\n\n\n\n\n","category":"type"},{"location":"reference/mcts/#Environment","page":"MCTS","title":"Environment","text":"","category":"section"},{"location":"reference/mcts/","page":"MCTS","title":"MCTS","text":"Env\nexplore!\npolicy\nreset!","category":"page"},{"location":"reference/mcts/#AlphaZero.MCTS.Env","page":"MCTS","title":"AlphaZero.MCTS.Env","text":"MCTS.Env(game_spec::AbstractGameSpec, oracle; <keyword args>)\n\nCreate and initialize an MCTS environment with a given oracle.\n\nKeyword Arguments\n\ngamma=1.: the reward discount factor\ncpuct=1.: exploration constant in the UCT formula\nnoise_ϵ=0., noise_α=1.: parameters for the dirichlet exploration noise  (see below)\nprior_temperature=1.: temperature to apply to the oracle's output  to get the prior probability vector used by MCTS.\n\nDirichlet Noise\n\nA naive way to ensure exploration during training is to adopt an ϵ-greedy policy, playing a random move at every turn instead of using the policy prescribed by MCTS.policy with probability ϵ. The problem with this naive strategy is that it may lead the player to make terrible moves at critical moments, thereby biasing the policy evaluation mechanism.\n\nA superior alternative is to add a random bias to the neural prior for the root node during MCTS exploration: instead of considering the policy p output by the neural network in the UCT formula, one uses (1-ϵ)p + ϵη where η is drawn once per call to MCTS.explore! from a Dirichlet distribution of parameter α.\n\n\n\n\n\n","category":"type"},{"location":"reference/mcts/#AlphaZero.MCTS.explore!","page":"MCTS","title":"AlphaZero.MCTS.explore!","text":"MCTS.explore!(env, game, nsims)\n\nRun nsims MCTS simulations from the current state.\n\n\n\n\n\n","category":"function"},{"location":"reference/mcts/#AlphaZero.MCTS.policy","page":"MCTS","title":"AlphaZero.MCTS.policy","text":"MCTS.policy(env, game)\n\nReturn the recommended stochastic policy on the current state.\n\nA call to this function must always be preceded by a call to MCTS.explore!.\n\n\n\n\n\n","category":"function"},{"location":"reference/mcts/#AlphaZero.MCTS.reset!","page":"MCTS","title":"AlphaZero.MCTS.reset!","text":"MCTS.reset!(env)\n\nEmpty the MCTS tree.\n\n\n\n\n\n","category":"function"},{"location":"reference/mcts/#Profiling-Utilities","page":"MCTS","title":"Profiling Utilities","text":"","category":"section"},{"location":"reference/mcts/","page":"MCTS","title":"MCTS","text":"memory_footprint_per_node\napproximate_memory_footprint\naverage_exploration_depth","category":"page"},{"location":"reference/mcts/#AlphaZero.MCTS.memory_footprint_per_node","page":"MCTS","title":"AlphaZero.MCTS.memory_footprint_per_node","text":"MCTS.memory_footprint_per_node(gspec)\n\nReturn an estimate of the memory footprint of a single MCTS node for the given game (in bytes).\n\n\n\n\n\n","category":"function"},{"location":"reference/mcts/#AlphaZero.MCTS.approximate_memory_footprint","page":"MCTS","title":"AlphaZero.MCTS.approximate_memory_footprint","text":"MCTS.approximate_memory_footprint(env)\n\nReturn an estimate of the memory footprint of the MCTS tree (in bytes).\n\n\n\n\n\n","category":"function"},{"location":"reference/mcts/#AlphaZero.MCTS.average_exploration_depth","page":"MCTS","title":"AlphaZero.MCTS.average_exploration_depth","text":"MCTS.average_exploration_depth(env)\n\nReturn the average number of nodes that are traversed during an MCTS simulation, not counting the root.\n\n\n\n\n\n","category":"function"},{"location":"tutorial/package_overview/#Package-Overview","page":"Package Overview","title":"Package Overview","text":"","category":"section"},{"location":"tutorial/package_overview/","page":"Package Overview","title":"Package Overview","text":"The philosophy of this project is to provide an implementation of AlphaZero that is simple enough to be widely accessible for students and researchers, while also being sufficiently powerful and fast to enable meaningful experiments on limited computing resources.","category":"page"},{"location":"tutorial/package_overview/","page":"Package Overview","title":"Package Overview","text":"On this page, we describe some key features of AlphaZero.jl:","category":"page"},{"location":"tutorial/package_overview/","page":"Package Overview","title":"Package Overview","text":"A standalone and straightforward MCTS implementation\nSome facilities for asynchronous and distributed simulation\nA series of opt-in optimizations to increase training efficiency\nGeneric interfaces for games and neural networks\nA simple user interface to get started quickly and diagnose problems","category":"page"},{"location":"tutorial/package_overview/#Asynchronous-and-Distributed-Simulations","page":"Package Overview","title":"Asynchronous and Distributed Simulations","text":"","category":"section"},{"location":"tutorial/package_overview/","page":"Package Overview","title":"Package Overview","text":"A key aspect of generating self-play data efficiently is to simulate a large number of games asynchronously and batch requests to the neural network across all simulations. Indeed, evaluating board positions one at a time would be terribly slow and lead to low GPU utilization.","category":"page"},{"location":"tutorial/package_overview/","page":"Package Overview","title":"Package Overview","text":"Thanks to Julia's great abstractions for asynchronous programming, the complexity of dealing with asynchronous simulations and batching is factored out in a single place and does not affect most of the codebase. In particular, the standalone MCTS module does not have to deal with batching and can be implemented in a straightforward, textbook fashion.","category":"page"},{"location":"tutorial/package_overview/","page":"Package Overview","title":"Package Overview","text":"Moreover, leveraging Julia's Distributed module, simulations are automatically distributed over all available Julia processes. This makes it possible to train an agent on a cluster of machines as easily as on a single computer, without writing any additional code. This capability was demonstrated during Julia Computing's sponsor talk at JuliaCon 2020.","category":"page"},{"location":"tutorial/package_overview/#Training-Optimizations","page":"Package Overview","title":"Training Optimizations","text":"","category":"section"},{"location":"tutorial/package_overview/","page":"Package Overview","title":"Package Overview","text":"AlphaZero.jl has out-of-the-box support for many of the optimizations introduced in Oracle's series and also implements new ones. These include:","category":"page"},{"location":"tutorial/package_overview/","page":"Package Overview","title":"Package Overview","text":"Position averaging\nMemory buffer with growing window\nCyclical learning rates\nSymmetry-based data augmentation and game randomization","category":"page"},{"location":"tutorial/package_overview/","page":"Package Overview","title":"Package Overview","text":"All these optimizations are documented in the Training Parameters section of the manual.","category":"page"},{"location":"tutorial/package_overview/#Game-Interface","page":"Package Overview","title":"Game Interface","text":"","category":"section"},{"location":"tutorial/package_overview/","page":"Package Overview","title":"Package Overview","text":"You can use AlphaZero.jl on the game of your choice by simply implementing the Game Interface. Currently, there is support for two-players, zero-sum games with finite action spaces and perfect information. Support for Markov Decision Processes will be added in a forthcoming release.","category":"page"},{"location":"tutorial/package_overview/","page":"Package Overview","title":"Package Overview","text":"Please see here for recommendations on how to use AlphaZero.jl on your own game.","category":"page"},{"location":"tutorial/package_overview/#Network-Interface","page":"Package Overview","title":"Network Interface","text":"","category":"section"},{"location":"tutorial/package_overview/","page":"Package Overview","title":"Package Overview","text":"AlphaZero.jl is agnostic to the choice of deep learning framework and allows you to plug any neural network that implements the Network Interface. For convenience, we provide a library of standard networks based on Knet. Right now, it features templates for two-headed multi-layer perceptrons and convolutional resnets.","category":"page"},{"location":"tutorial/package_overview/#User-Interface-and-Utilities","page":"Package Overview","title":"User Interface and Utilities","text":"","category":"section"},{"location":"tutorial/package_overview/","page":"Package Overview","title":"Package Overview","text":"AlphaZero.jl comes with batteries included. It features a simple user interface along with utilities for session management, logging, profiling, benchmarking and model exploration.","category":"page"},{"location":"tutorial/package_overview/","page":"Package Overview","title":"Package Overview","text":"A session management system makes it easy to interrupt and resume training.\nAn interactive command interpreter can be used to explore the behavior of AlphaZero agents.\nCommon tasks can be executed in a single line thanks to the Scripts module.\nReports are generated automatically after each training iteration to help diagnosing problems and tuning hyperparameters. An extensive documentation of collected metrics can be found in Training Reports and Benchmarks.","category":"page"},{"location":"tutorial/package_overview/","page":"Package Overview","title":"Package Overview","text":"Finally, because the user interface is implemented separately from the core algorithm, it can be extended or replaced easily.","category":"page"},{"location":"tutorial/package_overview/","page":"Package Overview","title":"Package Overview","text":"<div>\n<img src=\"../../assets/img/connect-four/plots/benchmark_won_games.png\" width=\"24%\" />\n<img src=\"../../assets/img/connect-four/plots/arena.png\" width=\"24%\" />\n<img src=\"../../assets/img/connect-four/plots/exploration_depth.png\" width=\"24%\" />\n<img src=\"../../assets/img/connect-four/plots/entropies.png\" width=\"24%\" />\n<img src=\"../../assets/img/connect-four/plots/loss.png\" width=\"24%\" />\n<img src=\"../../assets/img/connect-four/plots/loss_per_stage.png\" width=\"24%\"/>\n<img src=\"../../assets/img/connect-four/plots/iter_perfs/1.png\" width=\"24%\"/>\n<img src=\"../../assets/img/connect-four/plots/iter_loss/1.png\" width=\"24%\" />\n<img src=\"../../assets/img/ui-first-iter-cut.png\" width=\"48%\" />\n<img src=\"../../assets/img/explorer.png\" width=\"48%\" />\n<!--<img src=\"../../assets/img/connect-four/plots/iter_summary/1.png\" width=\"24%\" />-->\n</div>","category":"page"},{"location":"reference/game_interface/#game_interface","page":"Game Interface","title":"Game Interface","text":"","category":"section"},{"location":"reference/game_interface/","page":"Game Interface","title":"Game Interface","text":"CurrentModule = AlphaZero","category":"page"},{"location":"reference/game_interface/","page":"Game Interface","title":"Game Interface","text":"GameInterface","category":"page"},{"location":"reference/game_interface/#AlphaZero.GameInterface","page":"Game Interface","title":"AlphaZero.GameInterface","text":"A generic interface for single-player games and two-player zero-sum games.\n\nStochastic games and intermediate rewards are supported. By convention, rewards are expressed from the point of view of the player called white. In two-player zero-sum games, we call black the player trying to minimize the reward.\n\n\n\n\n\n","category":"module"},{"location":"reference/game_interface/","page":"Game Interface","title":"Game Interface","text":"A test suite is provided in the AlphaZero.Scripts to check the compliance of your environment with this interface.","category":"page"},{"location":"reference/game_interface/","page":"Game Interface","title":"Game Interface","text":"CurrentModule = AlphaZero.GameInterface","category":"page"},{"location":"reference/game_interface/#Mandatory-Interface","page":"Game Interface","title":"Mandatory Interface","text":"","category":"section"},{"location":"reference/game_interface/","page":"Game Interface","title":"Game Interface","text":"The game interface of AlphaZero.jl differs from many standard RL interfaces by making a distinction between a game specification and a game environment:","category":"page"},{"location":"reference/game_interface/","page":"Game Interface","title":"Game Interface","text":"A specification holds all static information about a game, which does not depend on the current state (e.g. the world dimensions in a grid world environment)\nIn contrast, an environment holds information about the current state of the game (e.g. the player's position in a grid-world environment).","category":"page"},{"location":"reference/game_interface/#Game-Specifications","page":"Game Interface","title":"Game Specifications","text":"","category":"section"},{"location":"reference/game_interface/","page":"Game Interface","title":"Game Interface","text":"AbstractGameSpec\ntwo_players\nactions\nvectorize_state","category":"page"},{"location":"reference/game_interface/#AlphaZero.GameInterface.AbstractGameSpec","page":"Game Interface","title":"AlphaZero.GameInterface.AbstractGameSpec","text":"AbstractGameSpec\n\nAbstract type for a game specification.\n\nThe specification holds all static information about a game, which does not depend on the current state.\n\n\n\n\n\n","category":"type"},{"location":"reference/game_interface/#AlphaZero.GameInterface.two_players","page":"Game Interface","title":"AlphaZero.GameInterface.two_players","text":"two_players(::AbstractGameSpec) :: Bool\n\nReturn whether or not a game is a two-players game.\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#AlphaZero.GameInterface.actions","page":"Game Interface","title":"AlphaZero.GameInterface.actions","text":"actions(::AbstractGameSpec)\n\nReturn the vector of all game actions.\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#AlphaZero.GameInterface.vectorize_state","page":"Game Interface","title":"AlphaZero.GameInterface.vectorize_state","text":"vectorize_state(::AbstractGameSpec, state) :: Array{Float32}\n\nReturn a vectorized representation of a given state.\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#Game-Environments","page":"Game Interface","title":"Game Environments","text":"","category":"section"},{"location":"reference/game_interface/","page":"Game Interface","title":"Game Interface","text":"AbstractGameEnv\ninit\nspec\nset_state!\ncurrent_state\ngame_terminated\nwhite_playing\nactions_mask\nplay!\nwhite_reward","category":"page"},{"location":"reference/game_interface/#AlphaZero.GameInterface.AbstractGameEnv","page":"Game Interface","title":"AlphaZero.GameInterface.AbstractGameEnv","text":"AbstractGameEnv\n\nAbstract base type for a game environment.\n\nIntuitively, a game environment holds a game specification and a current state.\n\n\n\n\n\n","category":"type"},{"location":"reference/game_interface/#AlphaZero.GameInterface.init","page":"Game Interface","title":"AlphaZero.GameInterface.init","text":"init(::AbstractGameSpec) :: AbstractGameEnv\n\nCreate a new game environment in a (possibly random) initial state.\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#AlphaZero.GameInterface.spec","page":"Game Interface","title":"AlphaZero.GameInterface.spec","text":"spec(game::AbstractGameEnv) :: AbstractGameSpec\n\nReturn the game specification of an environment.\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#AlphaZero.GameInterface.set_state!","page":"Game Interface","title":"AlphaZero.GameInterface.set_state!","text":"set_state!(game::AbstractGameEnv, state)\n\nModify the state of a game environment in place.\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#AlphaZero.GameInterface.current_state","page":"Game Interface","title":"AlphaZero.GameInterface.current_state","text":"current_state(game::AbstractGameEnv)\n\nReturn the game state.\n\nwarn: Warn\nThe state returned by this function may be stored (e.g. in the MCTS tree) and must therefore either be fresh or persistent. If in doubt, you should make a copy.\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#AlphaZero.GameInterface.game_terminated","page":"Game Interface","title":"AlphaZero.GameInterface.game_terminated","text":"game_terminated(::AbstractGameEnv)\n\nReturn a boolean indicating whether or not the game is in a terminal state.\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#AlphaZero.GameInterface.white_playing","page":"Game Interface","title":"AlphaZero.GameInterface.white_playing","text":"white_playing(::AbstractGameEnv) :: Bool\n\nReturn true if white is to play and false otherwise.\n\nFor a one-player game, this function must always return true.\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#AlphaZero.GameInterface.actions_mask","page":"Game Interface","title":"AlphaZero.GameInterface.actions_mask","text":"actions_mask(::AbstractGameEnv)\n\nReturn a boolean mask indicating what actions are available.\n\nThe following identities must hold:\n\ngame_terminated(game) || any(actions_mask(game))\nlength(actions_mask(game)) == length(actions(spec(game)))\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#AlphaZero.GameInterface.play!","page":"Game Interface","title":"AlphaZero.GameInterface.play!","text":"play!(game::AbstractGameEnv, action)\n\nUpdate the game environment by making the current player perform action. Note that this function does not have to be deterministic.\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#AlphaZero.GameInterface.white_reward","page":"Game Interface","title":"AlphaZero.GameInterface.white_reward","text":"white_reward(game::AbstractGameEnv)\n\nReturn the intermediate reward obtained by the white player after the last transition step. The result is undetermined when called at an initial state.\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#Optional-Interface","page":"Game Interface","title":"Optional Interface","text":"","category":"section"},{"location":"reference/game_interface/#Interface-for-Interactive-Tools","page":"Game Interface","title":"Interface for Interactive Tools","text":"","category":"section"},{"location":"reference/game_interface/","page":"Game Interface","title":"Game Interface","text":"These functions are required for the default User Interface to work well.","category":"page"},{"location":"reference/game_interface/","page":"Game Interface","title":"Game Interface","text":"action_string\nparse_action\nread_state\nrender","category":"page"},{"location":"reference/game_interface/#AlphaZero.GameInterface.action_string","page":"Game Interface","title":"AlphaZero.GameInterface.action_string","text":"action_string(::AbstractGameSpec, action) :: String\n\nReturn a human-readable string representing the provided action.\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#AlphaZero.GameInterface.parse_action","page":"Game Interface","title":"AlphaZero.GameInterface.parse_action","text":"parse_action(::AbstractGameSpec, str::String)\n\nReturn the action described by string str or nothing if str does not denote a valid action.\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#AlphaZero.GameInterface.read_state","page":"Game Interface","title":"AlphaZero.GameInterface.read_state","text":"read_state(game_spec::AbstractGameSpec)\n\nRead a state from the standard input. Return the corresponding state (with type state_type(game_spec)) or nothing in case of an invalid input.\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#AlphaZero.GameInterface.render","page":"Game Interface","title":"AlphaZero.GameInterface.render","text":"render(game::AbstractGameEnv)\n\nPrint the game state on the standard output.\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#Other-Optional-Functions","page":"Game Interface","title":"Other Optional Functions","text":"","category":"section"},{"location":"reference/game_interface/","page":"Game Interface","title":"Game Interface","text":"heuristic_value\nsymmetries","category":"page"},{"location":"reference/game_interface/#AlphaZero.GameInterface.heuristic_value","page":"Game Interface","title":"AlphaZero.GameInterface.heuristic_value","text":"heuristic_value(game::AbstractGameEnv)\n\nReturn a heuristic estimate of the state value for the current player.\n\nThe given state must be nonfinal and returned values must belong to the (- ) interval.\n\nThis function is not needed by AlphaZero but it is useful for building baselines such as minmax players.\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#AlphaZero.GameInterface.symmetries","page":"Game Interface","title":"AlphaZero.GameInterface.symmetries","text":"symmetries(::AbstractGameSpec, state)\n\nReturn the vector of all pairs (s, σ) where:\n\ns is the image of state by a nonidentical symmetry\nσ is the associated actions permutation, as an integer vector of  size num_actions(game).\n\nA default implementation is provided that returns an empty vector.\n\nNote that the current state of the passed environment is ignored by this function.\n\nExample\n\nIn the game of tic-tac-toe, there are eight symmetries that can be obtained by composing reflexions and rotations of the board (including the identity symmetry).\n\nProperty\n\nIf (s2, σ) is a symmetry for state s1, then mask2 == mask1[σ] must hold where mask1 and mask2 are the available action masks for s1 and s2 respectively.\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#Derived-Functions","page":"Game Interface","title":"Derived Functions","text":"","category":"section"},{"location":"reference/game_interface/#Operations-on-Specifications","page":"Game Interface","title":"Operations on Specifications","text":"","category":"section"},{"location":"reference/game_interface/","page":"Game Interface","title":"Game Interface","text":"state_type\nstate_dim\nstate_memsize\naction_type\nnum_actions\ninit(::AbstractGameSpec, state)","category":"page"},{"location":"reference/game_interface/#AlphaZero.GameInterface.state_type","page":"Game Interface","title":"AlphaZero.GameInterface.state_type","text":"state_type(::AbstractGameSpec)\n\nReturn the state type associated to a game.\n\nState objects must be persistent or appear as such as they are stored into the MCTS tree without copying. They also have to be comparable and hashable.\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#AlphaZero.GameInterface.state_dim","page":"Game Interface","title":"AlphaZero.GameInterface.state_dim","text":"state_dim(::AbstractGameSpec)\n\nReturn a tuple that indicates the shape of a vectorized state representation.\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#AlphaZero.GameInterface.state_memsize","page":"Game Interface","title":"AlphaZero.GameInterface.state_memsize","text":"state_memsize(::AbstractGameSpec)\n\nReturn the memory footprint occupied by a state of the given game.\n\nThe computation is based on a random initial state, assuming that all states have an identical footprint.\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#AlphaZero.GameInterface.action_type","page":"Game Interface","title":"AlphaZero.GameInterface.action_type","text":"action_type(::AbstractGameSpec)\n\nReturn the action type associated to a game.\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#AlphaZero.GameInterface.num_actions","page":"Game Interface","title":"AlphaZero.GameInterface.num_actions","text":"num_actions(::AbstractGameSpec)\n\nReturn the total number of actions associated with a game.\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#AlphaZero.GameInterface.init-Tuple{AbstractGameSpec, Any}","page":"Game Interface","title":"AlphaZero.GameInterface.init","text":"init(::AbstractGameSpec, state) :: AbstractGameEnv\n\nCreate a new game environment, initialized in a given state.\n\n\n\n\n\n","category":"method"},{"location":"reference/game_interface/#Operations-on-Environments","page":"Game Interface","title":"Operations on Environments","text":"","category":"section"},{"location":"reference/game_interface/","page":"Game Interface","title":"Game Interface","text":"clone\navailable_actions\napply_random_symmetry!","category":"page"},{"location":"reference/game_interface/#AlphaZero.GameInterface.clone","page":"Game Interface","title":"AlphaZero.GameInterface.clone","text":"clone(::AbstractGameEnv)\n\nReturn an independent copy of the given environment.\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#AlphaZero.GameInterface.available_actions","page":"Game Interface","title":"AlphaZero.GameInterface.available_actions","text":"available_actions(::AbstractGameEnv)\n\nReturn the vector of all available actions.\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#AlphaZero.GameInterface.apply_random_symmetry!","page":"Game Interface","title":"AlphaZero.GameInterface.apply_random_symmetry!","text":"apply_random_symmetry!(::AbstractGameEnv)\n\nUpdate a game environment by applying a random symmetry to the current state (see symmetries).\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#common_rl_intf","page":"Game Interface","title":"Wrapper for CommonRLInterface.jl","text":"","category":"section"},{"location":"reference/game_interface/","page":"Game Interface","title":"Game Interface","text":"CurrentModule = AlphaZero","category":"page"},{"location":"reference/game_interface/","page":"Game Interface","title":"Game Interface","text":"CommonRLInterfaceWrapper\nCommonRLInterfaceWrapper.Env\nCommonRLInterfaceWrapper.Spec","category":"page"},{"location":"reference/game_interface/#AlphaZero.CommonRLInterfaceWrapper","page":"Game Interface","title":"AlphaZero.CommonRLInterfaceWrapper","text":"Utilities for using AlphaZero.jl on RL environments that implement CommonRLInterface.jl.\n\n\n\n\n\n","category":"module"},{"location":"reference/game_interface/#AlphaZero.CommonRLInterfaceWrapper.Env","page":"Game Interface","title":"AlphaZero.CommonRLInterfaceWrapper.Env","text":"Env(rlenv::CommonRLInterface.AbstractEnv; <kwargs>) <: AbstractGameEnv\n\nWrap an environment implementing the interface defined in CommonRLInterface.jl into an AbstractGameEnv.\n\nRequirements\n\nThe following optional methods must be implemented for rlenv:\n\nclone\nstate\nsetstate!\nvalid_action_mask\nplayer\nplayers\n\nKeyword arguments\n\nThe following optional functions from GameInterface are not present in CommonRLInterface.jl and can be provided as keyword arguments:\n\nvectorize_state: must be provided unless states already have type Array{<:Number}\nheuristic_value\nsymmetries\nrender\naction_string\nparse_action\nread_state\n\nIf f is not provided, the default implementation calls GI.f(::CommonRLInterface.AbstractEnv, ...).\n\n\n\n\n\n","category":"type"},{"location":"reference/game_interface/#AlphaZero.CommonRLInterfaceWrapper.Spec","page":"Game Interface","title":"AlphaZero.CommonRLInterfaceWrapper.Spec","text":"Spec(rlenv::RL.AbstractEnv; kwargs...) = spec(Env(rlenv; kwargs...))\n\n\n\n\n\n","category":"type"},{"location":"reference/player/#player","page":"Players and Simulations","title":"Players and Simulations","text":"","category":"section"},{"location":"reference/player/","page":"Players and Simulations","title":"Players and Simulations","text":"CurrentModule = AlphaZero","category":"page"},{"location":"reference/player/#Player-Interface","page":"Players and Simulations","title":"Player Interface","text":"","category":"section"},{"location":"reference/player/","page":"Players and Simulations","title":"Players and Simulations","text":"AbstractPlayer\nthink\nselect_move\nreset_player!\nplayer_temperature","category":"page"},{"location":"reference/player/#AlphaZero.AbstractPlayer","page":"Players and Simulations","title":"AlphaZero.AbstractPlayer","text":"AbstractPlayer\n\nAbstract type for a game player.\n\n\n\n\n\n","category":"type"},{"location":"reference/player/#AlphaZero.think","page":"Players and Simulations","title":"AlphaZero.think","text":"think(::AbstractPlayer, game)\n\nReturn a probability distribution over available actions as a (actions, π) pair.\n\n\n\n\n\n","category":"function"},{"location":"reference/player/#AlphaZero.select_move","page":"Players and Simulations","title":"AlphaZero.select_move","text":"select_move(player::AbstractPlayer, game, turn_number)\n\nReturn a single action. A default implementation is provided that samples an action according to the distribution computed by think, with a temperature given by player_temperature.\n\n\n\n\n\n","category":"function"},{"location":"reference/player/#AlphaZero.reset_player!","page":"Players and Simulations","title":"AlphaZero.reset_player!","text":"reset_player!(::AbstractPlayer)\n\nReset the internal memory of a player (e.g. the MCTS tree). The default implementation does nothing.\n\n\n\n\n\n","category":"function"},{"location":"reference/player/#AlphaZero.player_temperature","page":"Players and Simulations","title":"AlphaZero.player_temperature","text":"player_temperature(::AbstractPlayer, game, turn_number)\n\nReturn the player temperature, given the number of actions that have been played before by both players in the current game.\n\nA default implementation is provided that always returns 1.\n\n\n\n\n\n","category":"function"},{"location":"reference/player/#Player-Instances","page":"Players and Simulations","title":"Player Instances","text":"","category":"section"},{"location":"reference/player/","page":"Players and Simulations","title":"Players and Simulations","text":"AlphaZeroPlayer\nMctsPlayer\nRandomPlayer\nNetworkPlayer\nEpsilonGreedyPlayer\nPlayerWithTemperature\nTwoPlayers","category":"page"},{"location":"reference/player/#AlphaZero.AlphaZeroPlayer","page":"Players and Simulations","title":"AlphaZero.AlphaZeroPlayer","text":"AlphaZeroPlayer(::Env; [timeout, mcts_params, use_gpu])\n\nCreate an AlphaZero player from the current training environment.\n\nNote that the returned player may be slow as it does not batch MCTS requests.\n\n\n\n\n\n","category":"function"},{"location":"reference/player/#AlphaZero.MctsPlayer","page":"Players and Simulations","title":"AlphaZero.MctsPlayer","text":"MctsPlayer{MctsEnv} <: AbstractPlayer\n\nA player that selects actions using MCTS.\n\nConstructors\n\nMctsPlayer(mcts::MCTS.Env; τ, niters, timeout=nothing)\n\nConstruct a player from an MCTS environment. When computing each move:\n\nif timeout is provided, MCTS simulations are executed for timeout seconds by groups of niters\notherwise, niters MCTS simulations are run\n\nThe temperature parameter τ can be either a real number or a AbstractSchedule.\n\nMctsPlayer(game_spec::AbstractGameSpec, oracle,\n           params::MctsParams; timeout=nothing)\n\nConstruct an MCTS player from an oracle and an MctsParams structure.\n\n\n\n\n\n","category":"type"},{"location":"reference/player/#AlphaZero.RandomPlayer","page":"Players and Simulations","title":"AlphaZero.RandomPlayer","text":"RandomPlayer <: AbstractPlayer\n\nA player that picks actions uniformly at random.\n\n\n\n\n\n","category":"type"},{"location":"reference/player/#AlphaZero.NetworkPlayer","page":"Players and Simulations","title":"AlphaZero.NetworkPlayer","text":"NetworkPlayer{Net} <: AbstractPlayer\n\nA player that uses the policy output by a neural network directly, instead of relying on MCTS. The given neural network must be in test mode.\n\n\n\n\n\n","category":"type"},{"location":"reference/player/#AlphaZero.EpsilonGreedyPlayer","page":"Players and Simulations","title":"AlphaZero.EpsilonGreedyPlayer","text":"EpsilonGreedyPlayer{Player} <: AbstractPlayer\n\nA wrapper on a player that makes it choose a random move with a fixed ϵ probability.\n\n\n\n\n\n","category":"type"},{"location":"reference/player/#AlphaZero.PlayerWithTemperature","page":"Players and Simulations","title":"AlphaZero.PlayerWithTemperature","text":"PlayerWithTemperature{Player} <: AbstractPlayer\n\nA wrapper on a player that enables overwriting the temperature schedule.\n\n\n\n\n\n","category":"type"},{"location":"reference/player/#AlphaZero.TwoPlayers","page":"Players and Simulations","title":"AlphaZero.TwoPlayers","text":"TwoPlayers <: AbstractPlayer\n\nIf white and black are two AbstractPlayer, then TwoPlayers(white, black) is a player that behaves as white when white is to play and as black when black is to play.\n\n\n\n\n\n","category":"type"},{"location":"reference/player/#Game-Simulations","page":"Players and Simulations","title":"Game Simulations","text":"","category":"section"},{"location":"reference/player/#Simulation-traces","page":"Players and Simulations","title":"Simulation traces","text":"","category":"section"},{"location":"reference/player/","page":"Players and Simulations","title":"Players and Simulations","text":"Trace\nBase.push!(::Trace, π, r, s)\n","category":"page"},{"location":"reference/player/#AlphaZero.Trace","page":"Players and Simulations","title":"AlphaZero.Trace","text":"Trace{State}\n\nAn object that collects all states visited during a game, along with the rewards obtained at each step and the successive player policies to be used as targets for the neural network.\n\nConstructor\n\nTrace(initial_state)\n\n\n\n\n\n","category":"type"},{"location":"reference/player/#Base.push!-Tuple{Trace, Any, Any, Any}","page":"Players and Simulations","title":"Base.push!","text":"Base.push!(t::Trace, π, r, s)\n\nAdd a (target policy, reward, new state) quadruple to a trace.\n\n\n\n\n\n","category":"method"},{"location":"reference/player/#Playing-a-single-game","page":"Players and Simulations","title":"Playing a single game","text":"","category":"section"},{"location":"reference/player/","page":"Players and Simulations","title":"Players and Simulations","text":"play_game","category":"page"},{"location":"reference/player/#AlphaZero.play_game","page":"Players and Simulations","title":"AlphaZero.play_game","text":"play_game(gspec::AbstractGameSpec, player; flip_probability=0.) :: Trace\n\nSimulate a game by an AbstractPlayer.\n\nFor two-player games, please use TwoPlayers.\nIf the flip_probability argument is set to p, the board is flipped randomly at every turn with probability p, using GI.apply_random_symmetry!.\n\n\n\n\n\n","category":"function"},{"location":"reference/player/#Playing-multiple-games-in-a-distibuted-fashion","page":"Players and Simulations","title":"Playing multiple games in a distibuted fashion","text":"","category":"section"},{"location":"reference/player/","page":"Players and Simulations","title":"Players and Simulations","text":"Simulator\nrecord_trace\nsimulate\nsimulate_distributed","category":"page"},{"location":"reference/player/#AlphaZero.Simulator","page":"Players and Simulations","title":"AlphaZero.Simulator","text":"Simulator(make_player, make_oracles, measure)\n\nA distributed simulator that encapsulates the details of running simulations across multiple threads and multiple machines.\n\nArguments\n\nmake_oracles: a function that takes no argument and returns   the oracles used by the player, which can be either nothing,   a single oracle or a pair of oracles.\nmake_player: a function that takes as an argument the result of make_oracles   and builds a player from it. In practice, an oracle returned by make_oracles   may be replaced by a BatchedOracle before it is passed to make_player, which   is why these two functions are specified separately.\nmeasure(trace, colors_flipped, player): the function that is used to   take measurements after each game simulation.\n\n\n\n\n\n","category":"type"},{"location":"reference/player/#AlphaZero.record_trace","page":"Players and Simulations","title":"AlphaZero.record_trace","text":"record_trace\n\nA measurement function to be passed to a Simulator that produces named tuples with two fields: trace::Trace and colors_flipped::Bool.\n\n\n\n\n\n","category":"function"},{"location":"reference/player/#AlphaZero.simulate","page":"Players and Simulations","title":"AlphaZero.simulate","text":"simulate(::Simulator, ::AbstractGameSpec; ::SimParams; <kwargs>)\n\nPlay a series of games using a given Simulator.\n\nKeyword Arguments\n\ngame_simulated is called every time a game simulation is completed (with no arguments)\n\nReturn\n\nReturn a vector of objects computed by simulator.measure.\n\n\n\n\n\n","category":"function"},{"location":"reference/player/#AlphaZero.simulate_distributed","page":"Players and Simulations","title":"AlphaZero.simulate_distributed","text":"simulate_distributed(::Simulator, ::AbstractGameSpec, ::SimParams; <kwargs>)\n\nIdentical to simulate but splits the work across all available distributed workers, whose number is given by Distributed.nworkers().\n\n\n\n\n\n","category":"function"},{"location":"reference/player/#Utilities-for-playing-interactive-games","page":"Players and Simulations","title":"Utilities for playing interactive games","text":"","category":"section"},{"location":"reference/player/","page":"Players and Simulations","title":"Players and Simulations","text":"Human\ninteractive!","category":"page"},{"location":"reference/player/#AlphaZero.Human","page":"Players and Simulations","title":"AlphaZero.Human","text":"Human <: AbstractPlayer\n\nHuman player that queries the standard input for actions.\n\nDoes not implement think but instead implements select_move directly.\n\n\n\n\n\n","category":"type"},{"location":"reference/player/#AlphaZero.interactive!","page":"Players and Simulations","title":"AlphaZero.interactive!","text":"interactive!(game)\ninteractive!(gspec)\ninteractive!(game, player)\ninteractive!(gspec, player)\ninteractive!(game, white, black)\ninteractive!(gspec, white, black)\n\nLaunch a possibly interactive game session.\n\nThis function takes either an AbstractGameSpec or AbstractGameEnv as an argument.\n\n\n\n\n\n","category":"function"},{"location":"reference/memory/#memory","page":"Memory Buffer","title":"Memory Buffer","text":"","category":"section"},{"location":"reference/memory/","page":"Memory Buffer","title":"Memory Buffer","text":"CurrentModule = AlphaZero","category":"page"},{"location":"reference/memory/","page":"Memory Buffer","title":"Memory Buffer","text":"TrainingSample\nMemoryBuffer\nget_experience(::MemoryBuffer)\npush_trace!","category":"page"},{"location":"reference/memory/#AlphaZero.TrainingSample","page":"Memory Buffer","title":"AlphaZero.TrainingSample","text":"TrainingSample{State}\n\nType of a training sample. A sample features the following fields:\n\ns::State is the state\nπ::Vector{Float64} is the recorded MCTS policy for this position\nz::Float64 is the discounted reward cumulated from state s\nt::Float64 is the (average) number of moves remaining before the end of the game\nn::Int is the number of times the state s was recorded\n\nAs revealed by the last field n, several samples that correspond to the same state can be merged, in which case the π, z and t fields are averaged together.\n\n\n\n\n\n","category":"type"},{"location":"reference/memory/#AlphaZero.MemoryBuffer","page":"Memory Buffer","title":"AlphaZero.MemoryBuffer","text":"MemoryBuffer(game_spec, size, experience=[])\n\nA circular buffer to hold memory samples.\n\n\n\n\n\n","category":"type"},{"location":"reference/memory/#AlphaZero.get_experience-Tuple{MemoryBuffer}","page":"Memory Buffer","title":"AlphaZero.get_experience","text":"get_experience(::MemoryBuffer) :: Vector{<:TrainingSample}\n\nReturn all samples in the memory buffer.\n\n\n\n\n\n","category":"method"},{"location":"reference/memory/#AlphaZero.push_trace!","page":"Memory Buffer","title":"AlphaZero.push_trace!","text":"push_trace!(mem::MemoryBuffer, trace::Trace, gamma)\n\nCollect samples out of a game trace and add them to the memory buffer.\n\nHere, gamma is the reward discount factor.\n\n\n\n\n\n","category":"function"},{"location":"tutorial/alphazero_intro/#alphazero_intro","page":"Introduction to AlphaZero","title":"Introduction to AlphaZero","text":"","category":"section"},{"location":"tutorial/alphazero_intro/","page":"Introduction to AlphaZero","title":"Introduction to AlphaZero","text":"The AlphaZero algorithm elegantly combines search and learning, which are described in Rich Sutton's essay \"The Bitter Lesson\" as the two fundamental pillars of AI. It augments a tree search procedure with two learnt heuristics: one to evaluate board positions and one to concentrate branching on moves that are not obviously wrong.","category":"page"},{"location":"tutorial/alphazero_intro/","page":"Introduction to AlphaZero","title":"Introduction to AlphaZero","text":"When training starts, both heuristics are initialized randomly and tree search has only access to a meaningful signal at the level of final states, where the game outcome is known. These heuristics are then improved iteratively through self-play. More specifically:","category":"page"},{"location":"tutorial/alphazero_intro/","page":"Introduction to AlphaZero","title":"Introduction to AlphaZero","text":"The heuristics are implemented by a two-headed neural network. Given a board position as an input, it estimates the probability for each player to ultimately win the game. It also provides a quantitative estimate of the relative quality of all available moves in the form of a probability distribution.\nThe search component is powered by Monte-Carlo Tree Search (MCTS), which implements a good compromise between breadth-first and depth-first search and provides a principled way to manage the uncertainty introduced by the heuristics. Also, given a position, it does not return a single choice for a best move but rather a probability distribution over available moves.\nAt each training iteration, AlphaZero plays a series of games against itself. The network is then updated so that it makes more accurate predictions about the outcome of these games. Also, the network's policy heuristic is updated to match the output of MCTS on all encountered positions. This way, MCTS can be seen as a powerful policy improvement operator.","category":"page"},{"location":"tutorial/alphazero_intro/","page":"Introduction to AlphaZero","title":"Introduction to AlphaZero","text":"For more details, we recommend the following resources.","category":"page"},{"location":"tutorial/alphazero_intro/#External-resources","page":"Introduction to AlphaZero","title":"External resources","text":"","category":"section"},{"location":"tutorial/alphazero_intro/","page":"Introduction to AlphaZero","title":"Introduction to AlphaZero","text":"A short and effective introduction to AlphaZero is Surag Nair's  excellent tutorial.\nOur JuliaCon 2021 talk  features a ten-minute introduction to AlphaZero and discusses some research challenges  of using it to solve problems beyond board games.\nA good resource to learn about Monte Carlo Tree Search (MCTS) is this  Int8 tutorial.\nThen, DeepMind's original  Nature paper  is a nice read.\nFinally, this series of posts  from Oracle has been an important source of inspiration for AlphaZero.jl.  It provides useful details on implementing asynchronous MCTS, along with  an interesting discussion on hyperparameters tuning.","category":"page"},{"location":"reference/scripts/#scripts","page":"Quick Scripts","title":"Quick Scripts","text":"","category":"section"},{"location":"reference/scripts/","page":"Quick Scripts","title":"Quick Scripts","text":"CurrentModule = AlphaZero","category":"page"},{"location":"reference/scripts/","page":"Quick Scripts","title":"Quick Scripts","text":"The AlphaZero.Scripts module provides a quick way to execute common tasks with a single line of code. For example, starting or resuming a training session for the connect-four example becomes as simple as executing the following command line:","category":"page"},{"location":"reference/scripts/","page":"Quick Scripts","title":"Quick Scripts","text":"julia --project -e 'using AlphaZero; Scripts.train(\"connect-four\")'","category":"page"},{"location":"reference/scripts/","page":"Quick Scripts","title":"Quick Scripts","text":"The first argument of every script specifies what experiment to load. This can be specified as an object of type Experiment or as a string from keys(Examples.experiment).","category":"page"},{"location":"reference/scripts/#Scripts-Description","page":"Quick Scripts","title":"Scripts Description","text":"","category":"section"},{"location":"reference/scripts/","page":"Quick Scripts","title":"Quick Scripts","text":"Scripts.test_game\nScripts.train\nScripts.play\nScripts.explore","category":"page"},{"location":"reference/scripts/#AlphaZero.Scripts.test_game","page":"Quick Scripts","title":"AlphaZero.Scripts.test_game","text":"test_game(::AbstractGameSpec)\ntest_game(experiment)\n\nPerform some sanity checks regarding the compliance of a game with the AlphaZero.jl Game Interface.\n\n\n\n\n\n","category":"function"},{"location":"reference/scripts/#AlphaZero.Scripts.train","page":"Quick Scripts","title":"AlphaZero.Scripts.train","text":"train(experiment; [dir, autosave, save_intermediate])\n\nStart or resume a training session.\n\nThe optional keyword arguments are passed directly to the Session constructor.\n\n\n\n\n\n","category":"function"},{"location":"reference/scripts/#AlphaZero.Scripts.play","page":"Quick Scripts","title":"AlphaZero.Scripts.play","text":"play(experiment; [dir])\n\nPlay an interactive game against the current agent.\n\n\n\n\n\n","category":"function"},{"location":"reference/scripts/#AlphaZero.Scripts.explore","page":"Quick Scripts","title":"AlphaZero.Scripts.explore","text":"explore(experiment; [dir])\n\nUse the interactive explorer to visualize the current agent.\n\n\n\n\n\n","category":"function"},{"location":"reference/scripts/","page":"Quick Scripts","title":"Quick Scripts","text":"Scripts.dummy_run\nScripts.test_grad_updates","category":"page"},{"location":"reference/scripts/#AlphaZero.Scripts.dummy_run","page":"Quick Scripts","title":"AlphaZero.Scripts.dummy_run","text":"dummy_run(experiment; [dir, nostdout])\n\nLaunch a training session where hyperparameters are altered so that training finishes as quickly as possible.\n\nThis is useful to ensure the absence of runtime errors before a real training session is started.\n\n\n\n\n\n","category":"function"},{"location":"reference/scripts/#AlphaZero.Scripts.test_grad_updates","page":"Quick Scripts","title":"AlphaZero.Scripts.test_grad_updates","text":"test_grad_updates(experiment; [num_games])\n\nRun a gradient update phase using dummy data.\n\nThis is useful to ensure that the chosen hyperparameters do not lead to Out of Memory errors during training.\n\n\n\n\n\n","category":"function"},{"location":"reference/experiment/#experiment","page":"Game Interface","title":"Game Interface","text":"","category":"section"},{"location":"reference/experiment/","page":"Game Interface","title":"Game Interface","text":"CurrentModule = AlphaZero.Experiments","category":"page"},{"location":"reference/experiment/","page":"Game Interface","title":"Game Interface","text":"Experiment","category":"page"},{"location":"reference/experiment/#AlphaZero.Experiments.Experiment","page":"Game Interface","title":"AlphaZero.Experiments.Experiment","text":"Experiment\n\nA structure that contains the information necessary to replicate a training session.\n\nConstructor\n\nExperiment(gspec, params, mknet, netparams, benchmarks)\n\ngspec is the specification of the game to be played\nparams has type Params\nmknet is a neural network constructor taking arguments (netparams, gspec)\nnetparams are the neural network hyperparameters\nbenchmark is a vector of Benchmark.Duel to be used as a benchmark to track training progress.                                                                                       \n\n\n\n\n\n","category":"type"},{"location":"reference/ui/#ui","page":"User Interface","title":"User Interface","text":"","category":"section"},{"location":"reference/ui/","page":"User Interface","title":"User Interface","text":"CurrentModule = AlphaZero","category":"page"},{"location":"reference/ui/","page":"User Interface","title":"User Interface","text":"UserInterface","category":"page"},{"location":"reference/ui/#AlphaZero.UserInterface","page":"User Interface","title":"AlphaZero.UserInterface","text":"The default user interface for AlphaZero.\n\nThe user interface is fully separated from the core algorithm and can therefore be replaced easily.\n\n\n\n\n\n","category":"module"},{"location":"reference/ui/","page":"User Interface","title":"User Interface","text":"CurrentModule = AlphaZero.UserInterface","category":"page"},{"location":"reference/ui/#session","page":"User Interface","title":"Session","text":"","category":"section"},{"location":"reference/ui/","page":"User Interface","title":"User Interface","text":"Session","category":"page"},{"location":"reference/ui/#AlphaZero.UserInterface.Session","page":"User Interface","title":"AlphaZero.UserInterface.Session","text":"Session{Env}\n\nA wrapper on an AlphaZero environment that adds features such as:\n\nLogging and plotting\nLoading and saving of environments\n\nIn particular, it implements the Handlers interface.\n\nPublic fields\n\nenv::Env is the environment wrapped by the session\nreport is the current session report, with type SessionReport\n\n\n\n\n\n","category":"type"},{"location":"reference/ui/","page":"User Interface","title":"User Interface","text":"(Image: Session CLI (first iteration))","category":"page"},{"location":"reference/ui/","page":"User Interface","title":"User Interface","text":"Session(::Experiment) # Strangely, this includes all constructors...\nresume!\nsave\nSessionReport","category":"page"},{"location":"reference/ui/#AlphaZero.UserInterface.Session-Tuple{Experiment}","page":"User Interface","title":"AlphaZero.UserInterface.Session","text":"Session(::Experiment; <optional kwargs>)\n\nCreate a new session from an experiment.\n\nOptional keyword arguments\n\ndir=\"sessions/<experiment-name>\": session directory in which all files and reports   are saved.\nautosave=true: if set to false, the session won't be saved automatically nor   any file will be generated\nnostdout=false: disables logging on the standard output when set to true\nsave_intermediate=false: if set to true (along with autosave), all   intermediate training environments are saved on disk so that   the whole training process can be analyzed later. This can   consume a lot of disk space.\n\n\n\n\n\n","category":"method"},{"location":"reference/ui/#AlphaZero.UserInterface.resume!","page":"User Interface","title":"AlphaZero.UserInterface.resume!","text":"resume!(session::Session)\n\nResume a previously created or loaded session. The user can interrupt training by sending a SIGKILL signal.\n\n\n\n\n\n","category":"function"},{"location":"reference/ui/#AlphaZero.UserInterface.save","page":"User Interface","title":"AlphaZero.UserInterface.save","text":"save(session::Session)\n\nSave a session on disk.\n\nThis function is called automatically by resume! after each training iteration if the session was created with autosave=true.\n\n\n\n\n\n","category":"function"},{"location":"reference/ui/#AlphaZero.UserInterface.SessionReport","page":"User Interface","title":"AlphaZero.UserInterface.SessionReport","text":"SessionReport\n\nThe full collection of statistics and benchmark results collected during a training session.\n\nFields\n\niterations: vector of n iteration reports with type   Report.Iteration\nbenchmark: vector of n+1 benchmark reports with type   Report.Benchmark\n\n\n\n\n\n","category":"type"},{"location":"reference/ui/#explorer","page":"User Interface","title":"Explorer","text":"","category":"section"},{"location":"reference/ui/","page":"User Interface","title":"User Interface","text":"(Image: Explorer)","category":"page"},{"location":"reference/ui/","page":"User Interface","title":"User Interface","text":"explore","category":"page"},{"location":"reference/ui/#AlphaZero.UserInterface.explore","page":"User Interface","title":"AlphaZero.UserInterface.explore","text":"explore(::AbstractPlayer, ::AbstractGameEnv;  [memory])\nexplore(::AbstractPlayer, ::AbstractGameSpec; [memory])\n\nStart a command interpreter to explore the internals of a player through interactive play.\n\nThe memory argument is an optional reference to a memory buffer. When provided, additional state statistics are displayed.\n\nCommands\n\nThe following commands are currently implemented:\n\ndo [action]: make the current player perform action. By default, the action of highest score is played.\nexplore [num_sims]: run num_sims MCTS simulations from the current state (for MCTS players only).\ngo: query the user for a state description and go to this state.\nflip: flip the board according to a random symmetry.\nundo: undo the effect of the previous command.\nrestart: restart the explorer.\n\n\n\n\n\n","category":"function"},{"location":"contributing/guide/#contributions_guide","page":"Contribution Guide","title":"Contribution Guide","text":"","category":"section"},{"location":"contributing/guide/","page":"Contribution Guide","title":"Contribution Guide","text":"Contributions to AlphaZero.jl are most welcome. Here are some contribution ideas:","category":"page"},{"location":"contributing/guide/","page":"Contribution Guide","title":"Contribution Guide","text":"Solve new games\nHelp with hyperparameter tuning\nImprove the user interface\nWrite tutorials or other learning resources based on this package","category":"page"},{"location":"contributing/guide/","page":"Contribution Guide","title":"Contribution Guide","text":"We also believe that AlphaZero.jl can be made even faster without adding too much complexity to the codebase. Here are suggestions to make this happen:","category":"page"},{"location":"contributing/guide/","page":"Contribution Guide","title":"Contribution Guide","text":"Accelerate network inference by adding support for FP16 or Int8 quantization\nAccelerate network inference with Torch.jl\nEnable data generation, network updates and checkpoint evaluations to be run in parallel","category":"page"},{"location":"contributing/guide/","page":"Contribution Guide","title":"Contribution Guide","text":"Finally, there are many small improvements and variations that can be built on top of this implementation and that would make for nice ML projects. Here are a few examples:","category":"page"},{"location":"contributing/guide/","page":"Contribution Guide","title":"Contribution Guide","text":"Implement and compare different First-Play Urgency strategies\nImplement tools to automate and help with hyperparameter tuning\nAdd a resignation mechanism to speed-up self-play\nGive more weight to recent samples during learning\nUse rollouts in addition to the network's value head to evaluate positions (as is done by AlphaGo Lee)\nUse supervised learning to initialize the network based on a set of games played by humans\nImplement the alternate training target proposed here\nImplement some of the improvements introduced in the KataGo paper","category":"page"},{"location":"contributing/guide/","page":"Contribution Guide","title":"Contribution Guide","text":"You may also want to have a look at our JSOC (Julia Summer of Code) project page.","category":"page"},{"location":"contributing/guide/","page":"Contribution Guide","title":"Contribution Guide","text":"Please do not hesitate to open a Github issue to share any idea, feedback or suggestion.","category":"page"},{"location":"contributing/guide/","page":"Contribution Guide","title":"Contribution Guide","text":"","category":"page"},{"location":"contributing/guide/#Solve-new-games","page":"Contribution Guide","title":"Solve new games","text":"","category":"section"},{"location":"contributing/guide/","page":"Contribution Guide","title":"Contribution Guide","text":"The simplest way to contribute to AlphaZero.jl is to demonstrate it on new games. Interesting candidates include: Othello, Gobblet, Go 9x9, Chess... A nice first-time contribution may also be to provide an example of using AlphaZero.jl in conjunction with OpenSpiel.jl.","category":"page"},{"location":"contributing/guide/","page":"Contribution Guide","title":"Contribution Guide","text":"","category":"page"},{"location":"contributing/guide/#Help-with-hyperparameter-tuning","page":"Contribution Guide","title":"Help with hyperparameter tuning","text":"","category":"section"},{"location":"contributing/guide/","page":"Contribution Guide","title":"Contribution Guide","text":"A good place to start would be to experiment with the parameters of the Connect Four agent discussed in the tutorial, as it went through little tuning and can probably be improved significantly. Any kind of hyperparameters study would be extremely valuable in getting a better understanding of AlphaZero's training process.","category":"page"},{"location":"contributing/guide/","page":"Contribution Guide","title":"Contribution Guide","text":"More generally, as a training session can take hours or days, it is hard for a single person to fine-tune AlphaZero's many hyperparameters. In an effort to tackle more and more ambitious games, it would be useful to come up with a collaborative process for running tuning experiments and share the resulting insights.","category":"page"},{"location":"contributing/guide/","page":"Contribution Guide","title":"Contribution Guide","text":"","category":"page"},{"location":"contributing/guide/#improve_ui","page":"Contribution Guide","title":"Improve the user interface","text":"","category":"section"},{"location":"contributing/guide/","page":"Contribution Guide","title":"Contribution Guide","text":"An effort has been made in designing AlphaZero.jl to separate the user interface code from the core logic (see AlphaZero.Handlers). We would be interested in seeing alternative user interfaces being developed. In particular, using something like TensorBoardLogger or Dash for logging and/or profiling might be nice.","category":"page"},{"location":"#AlphaZero.jl","page":"Home","title":"AlphaZero.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package provides a generic, simple and fast implementation of Deepmind's AlphaZero algorithm:","category":"page"},{"location":"","page":"Home","title":"Home","text":"The core algorithm is only 2,000 lines of pure, hackable Julia code.\nGeneric interfaces make it easy to add support for new games or new learning frameworks.\nBeing between one and two orders of magnitude faster than its Python alternatives, this implementation enables solving nontrivial games on a standard desktop computer with a GPU.\nThe same agent can be trained on a cluster of machines as easily as on a single computer and without modifying a single line of code.","category":"page"},{"location":"#Why-should-I-care-about-AlphaZero?","page":"Home","title":"Why should I care about AlphaZero?","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Beyond its much publicized success in attaining superhuman level at games such as Chess and Go, DeepMind's AlphaZero algorithm illustrates a more general methodology of combining learning and search to explore large combinatorial spaces effectively. We believe that this methodology can have exciting applications in many different research areas.","category":"page"},{"location":"#What-does-make-this-implementation-fast-and-why-does-it-matter?","page":"Home","title":"What does make this implementation fast and why does it matter?","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Because AlphaZero is resource-hungry, successful open-source implementations (such as Leela Zero) are written in low-level languages (such as C++) and optimized for highly distributed computing environments. This makes them hardly accessible for students, researchers and hackers.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Many simple Python implementations can be found on Github, but none of them is able to beat a reasonable baseline on games such as Othello or Connect Four. As an illustration, the benchmark in the README of the most popular of them only features a random baseline, along with a greedy baseline that does not appear to be significantly stronger.","category":"page"},{"location":"","page":"Home","title":"Home","text":"AlphaZero.jl is designed to be as simple as those Python implementations. In addition, it is between one and two orders of magnitude faster, making it possible to solve nontrivial games on a standard desktop computer with a GPU. This gain comes mostly from two sources:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Julia's inherent speed: most machine learning algorithms do not suffer much from being written in Python as most of the computation happens within heavily optimized matrix manipulation routines. This is not the case with AlphaZero, where tree search is also a possible bottleneck.\nAn asynchronous simulation mechanism that enables batching requests to the neural network across several simulation threads, thereby maximizing GPU utilization.","category":"page"},{"location":"#Supporting-and-Citing","page":"Home","title":"Supporting and Citing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you want to support this project and help it gain visibility, please consider starring the repository. Doing well on such metrics may also help us secure academic funding in the future. Also, if you use this software as part of your research, we would appreciate that you include the following citation in your paper.","category":"page"},{"location":"#Acknowledgements","page":"Home","title":"Acknowledgements","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This material is based upon work supported by the United States Air Force and DARPA under Contract No. FA9550-16-1-0288 and FA8750-18-C-0092. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the United States Air Force and DARPA.","category":"page"}]
}
