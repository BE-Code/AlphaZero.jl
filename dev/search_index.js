var documenterSearchIndex = {"docs":
[{"location":"reference/networks_library/#networks_library-1","page":"Networks Library","title":"Networks Library","text":"","category":"section"},{"location":"reference/networks_library/#","page":"Networks Library","title":"Networks Library","text":"CurrentModule = AlphaZero","category":"page"},{"location":"reference/networks_library/#","page":"Networks Library","title":"Networks Library","text":"KNets","category":"page"},{"location":"reference/networks_library/#AlphaZero.KNets","page":"Networks Library","title":"AlphaZero.KNets","text":"This module provides utilities to build neural networks with Knet, along with a library of standard architectures.\n\n\n\n\n\n","category":"module"},{"location":"reference/networks_library/#Knet-Utilities-1","page":"Networks Library","title":"Knet Utilities","text":"","category":"section"},{"location":"reference/networks_library/#","page":"Networks Library","title":"Networks Library","text":"KNets.KNetwork\nKNets.TwoHeadNetwork","category":"page"},{"location":"reference/networks_library/#AlphaZero.KNets.KNetwork","page":"Networks Library","title":"AlphaZero.KNets.KNetwork","text":"KNetwork{Game} <: AbstractNetwork{Game}\n\nAbstract type for neural networks implemented using the Knet framework.\n\nSubtypes are expected to be expressed as the composition of Flux-like layers that implement a functor interface through functions children and mapchildren.\nA custom implementation of regularized_params_ must also be implemented for layers containing parameters that are subject to regularization.\n\nProvided that the above holds, KNetwork implements the full network interface with the following exceptions: Network.HyperParams, Network.hyperparams, Network.forward and Network.on_gpu.\n\n\n\n\n\n","category":"type"},{"location":"reference/networks_library/#AlphaZero.KNets.TwoHeadNetwork","page":"Networks Library","title":"AlphaZero.KNets.TwoHeadNetwork","text":"TwoHeadNetwork{Game} <: KNetwork{G}\n\nAn abstract type for two-head neural networks implemented with Knet.\n\nSubtypes are assumed to have the following fields: hyper, common, vhead and phead. Based on those, an implementation is provided for Network.hyperparams, Network.forward and Network.on_gpu, leaving only Network.HyperParams to be implemented.\n\n\n\n\n\n","category":"type"},{"location":"reference/networks_library/#Networks-Library-1","page":"Networks Library","title":"Networks Library","text":"","category":"section"},{"location":"reference/networks_library/#Convolutional-ResNet-1","page":"Networks Library","title":"Convolutional ResNet","text":"","category":"section"},{"location":"reference/networks_library/#","page":"Networks Library","title":"Networks Library","text":"KNets.ResNet\nKNets.ResNetHP","category":"page"},{"location":"reference/networks_library/#AlphaZero.KNets.ResNet","page":"Networks Library","title":"AlphaZero.KNets.ResNet","text":"ResNet{Game} <: TwoHeadNetwork{Game}\n\nThe convolutional residual network architecture that is used in the original AlphaGo Zero paper.\n\n\n\n\n\n","category":"type"},{"location":"reference/networks_library/#AlphaZero.KNets.ResNetHP","page":"Networks Library","title":"AlphaZero.KNets.ResNetHP","text":"ResNetHP\n\nHyperparameters for the convolutional resnet architecture.\n\nParameter Type Default\nnum_blocks Int -\nnum_filters Int -\nconv_kernel_size Tuple{Int, Int} -\nnum_policy_head_filters Int 2\nnum_value_head_filters Int 1\nbatch_norm_momentum Float32 0.6f0\n\nThe trunk of the two-head network consists of num_blocks consecutive blocks. Each block features two convolutional layers with num_filters filters and with kernel size conv_kernel_size. Note that both kernel dimensions must be odd.\n\nDuring training, the network is evaluated in training mode on the whole dataset to compute the loss before it is switched to test model, using big batches. Therefore, it makes sense to use a low batch norm momentum.\n\nAlphaGo Zero Parameters\n\nThe network in the original paper from Deepmind features 20 blocks with 256 filters per convolutional layer.\n\n\n\n\n\n","category":"type"},{"location":"reference/networks_library/#Simple-Network-1","page":"Networks Library","title":"Simple Network","text":"","category":"section"},{"location":"reference/networks_library/#","page":"Networks Library","title":"Networks Library","text":"KNets.SimpleNet\nKNets.SimpleNetHP","category":"page"},{"location":"reference/networks_library/#AlphaZero.KNets.SimpleNet","page":"Networks Library","title":"AlphaZero.KNets.SimpleNet","text":"SimpleNet{Game} <: TwoHeadNetwork{Game}\n\nA simple two-headed architecture with only dense layers.\n\n\n\n\n\n","category":"type"},{"location":"reference/networks_library/#AlphaZero.KNets.SimpleNetHP","page":"Networks Library","title":"AlphaZero.KNets.SimpleNetHP","text":"SimpleNetHP\n\nHyperparameters for the simplenet architecture.\n\nParameter Description\nwidth :: Int Number of neurons on each dense layer\ndepth_common :: Int Number of dense layers in the trunk\ndepth_phead = 1 Number of hidden layers in the actions head\ndepth_vhead = 1 Number of hidden layers in the value  head\nuse_batch_norm = false Use batch normalization between each layer\nbatch_norm_momentum = 0.6f0 Momentum of batch norm statistics updates\n\n\n\n\n\n","category":"type"},{"location":"reference/benchmark/#benchmark-1","page":"Benchmark","title":"Benchmark","text":"","category":"section"},{"location":"reference/benchmark/#","page":"Benchmark","title":"Benchmark","text":"CurrentModule = AlphaZero","category":"page"},{"location":"reference/benchmark/#","page":"Benchmark","title":"Benchmark","text":"Benchmark\nBenchmark.Report","category":"page"},{"location":"reference/benchmark/#AlphaZero.Benchmark","page":"Benchmark","title":"AlphaZero.Benchmark","text":"Utilities to evaluate players against one another.\n\nTypically, between each training iteration, different players relying on the current neural network compete against a set of baselines.\n\n\n\n\n\n","category":"module"},{"location":"reference/benchmark/#AlphaZero.Benchmark.Report","page":"Benchmark","title":"AlphaZero.Benchmark.Report","text":"Benchmark.Report = Vector{Benchmark.DuelOutcome}\n\nA benchmark report is a vector of Benchmark.DuelOutcome objects.\n\n\n\n\n\n","category":"type"},{"location":"reference/benchmark/#Duels-1","page":"Benchmark","title":"Duels","text":"","category":"section"},{"location":"reference/benchmark/#","page":"Benchmark","title":"Benchmark","text":"Benchmark.Duel\nBenchmark.DuelOutcome","category":"page"},{"location":"reference/benchmark/#AlphaZero.Benchmark.Duel","page":"Benchmark","title":"AlphaZero.Benchmark.Duel","text":"Benchmark.Duel(player, baseline; num_games)\n\nSpecify a duel that consists in num_games games between player and baseline, each of them of type Benchmark.Player.\n\nOptional keyword arguments\n\nreset_every: if set, the MCTS tree is reset every reset_mcts_every games   to avoid running out of memory\ncolor_policy has type ColorPolicy and is ALTERNATE_COLORS   by default\n\n\n\n\n\n","category":"type"},{"location":"reference/benchmark/#AlphaZero.Benchmark.DuelOutcome","page":"Benchmark","title":"AlphaZero.Benchmark.DuelOutcome","text":"Benchmark.DuelOutcome\n\nThe outcome of a duel between two players.\n\nFields\n\nplayer and baseline are String fields containing the names of   both players involved in the duel\navgz is the average reward collected by player\nrewards is a vector containing all rewards collected by player   (one per game played)\ntime is the computing time spent running the duel, in seconds\n\n\n\n\n\n","category":"type"},{"location":"reference/benchmark/#Players-1","page":"Benchmark","title":"Players","text":"","category":"section"},{"location":"reference/benchmark/#","page":"Benchmark","title":"Benchmark","text":"Benchmark.Player\nBenchmark.Full\nBenchmark.NetworkOnly\nBenchmark.MctsRollouts\nBenchmark.MinMaxTS\nBenchmark.Solver","category":"page"},{"location":"reference/benchmark/#AlphaZero.Benchmark.Player","page":"Benchmark","title":"AlphaZero.Benchmark.Player","text":"Benchmark.Player\n\nAbstract type to specify a player that can be featured in a benchmark duel.\n\nSubtypes must implement the following functions:\n\nBenchmark.instantiate(player, nn): instantiate the player specification   into an AbstractPlayer given a neural network\nBenchmark.name(player): return a String describing the player\n\n\n\n\n\n","category":"type"},{"location":"reference/benchmark/#AlphaZero.Benchmark.Full","page":"Benchmark","title":"AlphaZero.Benchmark.Full","text":"Benchmark.Full(params) <: Benchmark.Player\n\nFull AlphaZero player that combines MCTS with the learnt network.\n\nArgument params has type MctsParams.\n\n\n\n\n\n","category":"type"},{"location":"reference/benchmark/#AlphaZero.Benchmark.NetworkOnly","page":"Benchmark","title":"AlphaZero.Benchmark.NetworkOnly","text":"Benchmark.NetworkOnly(;use_gpu=true) <: Benchmark.Player\n\nPlayer that uses the policy output by the learnt network directly, instead of relying on MCTS.\n\n\n\n\n\n","category":"type"},{"location":"reference/benchmark/#AlphaZero.Benchmark.MctsRollouts","page":"Benchmark","title":"AlphaZero.Benchmark.MctsRollouts","text":"Benchmark.MctsRollouts(params) <: Benchmark.Player\n\nPure MCTS baseline that uses rollouts to evaluate new positions.\n\nArgument params has type MctsParams.\n\n\n\n\n\n","category":"type"},{"location":"reference/benchmark/#AlphaZero.Benchmark.MinMaxTS","page":"Benchmark","title":"AlphaZero.Benchmark.MinMaxTS","text":"Benchmark.MinMaxTS(;depth, τ=0.) <: Benchmark.Player\n\nMinmax baseline, which relies on MinMax.Player.\n\n\n\n\n\n","category":"type"},{"location":"reference/benchmark/#AlphaZero.Benchmark.Solver","page":"Benchmark","title":"AlphaZero.Benchmark.Solver","text":"Benchmark.Solver(;ϵ) <: Benchmark.Player\n\nPerfect solver that plays randomly with probability ϵ.\n\n\n\n\n\n","category":"type"},{"location":"reference/benchmark/#Minmax-Baseline-1","page":"Benchmark","title":"Minmax Baseline","text":"","category":"section"},{"location":"reference/benchmark/#","page":"Benchmark","title":"Benchmark","text":"MinMax\nMinMax.Player","category":"page"},{"location":"reference/benchmark/#AlphaZero.MinMax","page":"Benchmark","title":"AlphaZero.MinMax","text":"A simple implementation of the minmax tree search algorithm, to be used as a baseline against AlphaZero. Heuristic board values are provided by the GameInterface.heuristic_value function.\n\n\n\n\n\n","category":"module"},{"location":"reference/benchmark/#AlphaZero.MinMax.Player","page":"Benchmark","title":"AlphaZero.MinMax.Player","text":"MinMax.Player{Game} <: AbstractPlayer{Game}\n\nA stochastic minmax player, to be used as a baseline.\n\nMinMax.Player{Game}(;depth, τ=0.)\n\nThe minmax player explores the game tree exhaustively at depth depth to build an estimate of the Q-value of each available action. Then, it chooses an action as follows:\n\nIf there are winning moves (with value Inf), one of them is picked uniformly at random.\nIf all moves are losing (with value -Inf), one of them is picked uniformly at random.\n\nOtherwise,\n\nIf the temperature τ is zero, a move is picked uniformly among those with maximal Q-value (there is usually only one choice).\nIf the temperature τ is nonzero, the probability of choosing action a is proportional to e^fracq_aCτ where q_a is the Q value of action a and C is the maximum absolute value of all finite Q values, making the decision invariant to rescaling of GameInterface.heuristic_value.\n\n\n\n\n\n","category":"type"},{"location":"contributing/#Contributions-Guide-1","page":"Contributions Guide","title":"Contributions Guide","text":"","category":"section"},{"location":"reference/network/#network_interface-1","page":"Network Interface","title":"Network Interface","text":"","category":"section"},{"location":"reference/network/#","page":"Network Interface","title":"Network Interface","text":"CurrentModule = AlphaZero.Network","category":"page"},{"location":"reference/network/#","page":"Network Interface","title":"Network Interface","text":"Network","category":"page"},{"location":"reference/network/#AlphaZero.Network","page":"Network Interface","title":"AlphaZero.Network","text":"A generic, framework agnostic interface for neural networks.\n\n\n\n\n\n","category":"module"},{"location":"reference/network/#Mandatory-Interface-1","page":"Network Interface","title":"Mandatory Interface","text":"","category":"section"},{"location":"reference/network/#","page":"Network Interface","title":"Network Interface","text":"AbstractNetwork\nHyperParams\nhyperparams\nforward\ntrain!\nset_test_mode!\nparams\nregularized_params","category":"page"},{"location":"reference/network/#AlphaZero.Network.AbstractNetwork","page":"Network Interface","title":"AlphaZero.Network.AbstractNetwork","text":"AbstractNetwork{Game} <: MCTS.Oracle{Game}\n\nAbstract base type for a neural network.\n\nConstructor\n\nAny subtype Network must implement the following constructor:\n\nNetwork(hyperparams)\n\nwhere the expected type of hyperparams is given by HyperParams(Network).\n\n\n\n\n\n","category":"type"},{"location":"reference/network/#AlphaZero.Network.HyperParams","page":"Network Interface","title":"AlphaZero.Network.HyperParams","text":"HyperParams(::Type{<:AbstractNetwork})\n\nReturn the hyperparameter type associated with a given network type.\n\n\n\n\n\n","category":"function"},{"location":"reference/network/#AlphaZero.Network.hyperparams","page":"Network Interface","title":"AlphaZero.Network.hyperparams","text":"hyperparams(::AbstractNetwork)\n\nReturn the hyperparameters of a network.\n\n\n\n\n\n","category":"function"},{"location":"reference/network/#AlphaZero.Network.forward","page":"Network Interface","title":"AlphaZero.Network.forward","text":"forward(::AbstractNetwork, boards)\n\nCompute the forward pass of a network on a batch of inputs.\n\nExpect a Float32 tensor boards whose batch dimension is the last one.\n\nReturn a (P, V) triple where:\n\nP is a matrix of size (num_actions, batch_size). It is allowed to put weight on invalid actions (see evaluate).\nV is a row vector of size (1, batch_size)\n\n\n\n\n\n","category":"function"},{"location":"reference/network/#AlphaZero.Network.train!","page":"Network Interface","title":"AlphaZero.Network.train!","text":"train!(::AbstractNetwork, opt::OptimiserSpec, loss, data)\n\nUpdate a given network to fit some data.\n\nopt::OptimiserSpec specified which optimiser to use\nloss is a function that maps a batch of samples to a tracked real\ndata is an iterator over minibatches\n\n\n\n\n\n","category":"function"},{"location":"reference/network/#AlphaZero.Network.set_test_mode!","page":"Network Interface","title":"AlphaZero.Network.set_test_mode!","text":"set_test_mode!(mode=true)\n\nPut a network in test mode or in training mode. This is relevant for networks featuring layers such as batch normalization layers.\n\n\n\n\n\n","category":"function"},{"location":"reference/network/#AlphaZero.Network.params","page":"Network Interface","title":"AlphaZero.Network.params","text":"params(::AbstractNetwork)\n\nReturn the collection of trainable parameters of a network.\n\n\n\n\n\n","category":"function"},{"location":"reference/network/#AlphaZero.Network.regularized_params","page":"Network Interface","title":"AlphaZero.Network.regularized_params","text":"regularized_params(::AbstractNetwork)\n\nReturn the collection of regularized parameters of a network. This usually excludes neuron's biases.\n\n\n\n\n\n","category":"function"},{"location":"reference/network/#Conversion-and-Copy-1","page":"Network Interface","title":"Conversion and Copy","text":"","category":"section"},{"location":"reference/network/#","page":"Network Interface","title":"Network Interface","text":"Base.copy(::AbstractNetwork)\nto_gpu\nto_cpu\non_gpu\nconvert_input\nconvert_output","category":"page"},{"location":"reference/network/#Base.copy-Tuple{AbstractNetwork}","page":"Network Interface","title":"Base.copy","text":"Base.copy(::AbstractNetwork)\n\nReturn a copy of the given network.\n\n\n\n\n\n","category":"method"},{"location":"reference/network/#AlphaZero.Network.to_gpu","page":"Network Interface","title":"AlphaZero.Network.to_gpu","text":"to_gpu(::AbstractNetwork)\n\nReturn a copy of the given network that has been transferred to the GPU if one is available. Otherwise, return the given network untouched.\n\n\n\n\n\n","category":"function"},{"location":"reference/network/#AlphaZero.Network.to_cpu","page":"Network Interface","title":"AlphaZero.Network.to_cpu","text":"to_cpu(::AbstractNetwork)\n\nReturn a copy of the given network that has been transferred to the CPU or return the given network untouched if it is already on CPU.\n\n\n\n\n\n","category":"function"},{"location":"reference/network/#AlphaZero.Network.on_gpu","page":"Network Interface","title":"AlphaZero.Network.on_gpu","text":"on_gpu(::AbstractNetwork) :: Bool\n\nTest whether or not a network is located on GPU.\n\n\n\n\n\n","category":"function"},{"location":"reference/network/#AlphaZero.Network.convert_input","page":"Network Interface","title":"AlphaZero.Network.convert_input","text":"convert_input(::AbstractNetwork, input)\n\nConvert an array (or number) to the right format so that it can be used as an input by a given network.\n\n\n\n\n\n","category":"function"},{"location":"reference/network/#AlphaZero.Network.convert_output","page":"Network Interface","title":"AlphaZero.Network.convert_output","text":"convert_output(::AbstractNetwork, output)\n\nConvert an array (or number) produced by a neural network to a standard CPU array (or number) type.\n\n\n\n\n\n","category":"function"},{"location":"reference/network/#Misc-1","page":"Network Interface","title":"Misc","text":"","category":"section"},{"location":"reference/network/#","page":"Network Interface","title":"Network Interface","text":"gc","category":"page"},{"location":"reference/network/#AlphaZero.Network.gc","page":"Network Interface","title":"AlphaZero.Network.gc","text":"gc(::AbstractNetwork)\n\nPerform full garbage collection and empty the GPU memory pool.\n\n\n\n\n\n","category":"function"},{"location":"reference/network/#Derived-Functions-1","page":"Network Interface","title":"Derived Functions","text":"","category":"section"},{"location":"reference/network/#Evaluation-Function-1","page":"Network Interface","title":"Evaluation Function","text":"","category":"section"},{"location":"reference/network/#","page":"Network Interface","title":"Network Interface","text":"evaluate","category":"page"},{"location":"reference/network/#AlphaZero.Network.evaluate","page":"Network Interface","title":"AlphaZero.Network.evaluate","text":"evaluate(network::AbstractNetwork, boards, action_masks)\n\nEvaluate a batch of board positions. This function is a wrapper on forward that puts a zero weight on invalid actions.\n\nArguments\n\nboards is a tensor whose last dimension has size bach_size\naction_masks is a binary matrix of size (num_actions, batch_size)\n\nReturn\n\nReturn a (P, V, Pinv) triple where:\n\nP is a matrix of size (num_actions, batch_size).\nV is a row vector of size (1, batch_size).\nPinv is a row vector of size (1, batch_size)  that indicates the total probability weight put by the network  on invalid actions for each sample.\n\nAll tensors manipulated by this function have elements of type Float32.\n\n\n\n\n\n","category":"function"},{"location":"reference/network/#Oracle-Interface-1","page":"Network Interface","title":"Oracle Interface","text":"","category":"section"},{"location":"reference/network/#","page":"Network Interface","title":"Network Interface","text":"All subtypes of AbstractNetwork implement the MCTS.Oracle interface through functions:","category":"page"},{"location":"reference/network/#","page":"Network Interface","title":"Network Interface","text":"MCTS.evaluate\nMCTS.evaluate_batch.","category":"page"},{"location":"reference/network/#","page":"Network Interface","title":"Network Interface","text":"Since evaluating a neural network on single samples at a time is slow, the latter should be used whenever possible.","category":"page"},{"location":"reference/network/#Misc-2","page":"Network Interface","title":"Misc","text":"","category":"section"},{"location":"reference/network/#","page":"Network Interface","title":"Network Interface","text":"num_parameters\nnum_regularized_parameters\nmean_weight\ncopy(::AbstractNetwork)","category":"page"},{"location":"reference/network/#AlphaZero.Network.num_parameters","page":"Network Interface","title":"AlphaZero.Network.num_parameters","text":"num_parameters(::AbstractNetwork)\n\nReturn the total number of parameters of a network.\n\n\n\n\n\n","category":"function"},{"location":"reference/network/#AlphaZero.Network.num_regularized_parameters","page":"Network Interface","title":"AlphaZero.Network.num_regularized_parameters","text":"num_regularized_parameters(::AbstractNetwork)\n\nReturn the total number of regularized parameters of a network.\n\n\n\n\n\n","category":"function"},{"location":"reference/network/#AlphaZero.Network.mean_weight","page":"Network Interface","title":"AlphaZero.Network.mean_weight","text":"mean_weight(::AbstractNetwork)\n\nReturn the mean absolute value of the regularized parameters of a network.\n\n\n\n\n\n","category":"function"},{"location":"reference/network/#AlphaZero.Network.copy-Tuple{AbstractNetwork}","page":"Network Interface","title":"AlphaZero.Network.copy","text":"copy(::AbstractNetwork; on_gpu, test_mode)\n\nA copy function that also handles CPU/GPU transfers and test/train mode switches.\n\n\n\n\n\n","category":"method"},{"location":"reference/network/#Optimiser-Specification-1","page":"Network Interface","title":"Optimiser Specification","text":"","category":"section"},{"location":"reference/network/#","page":"Network Interface","title":"Network Interface","text":"OptimiserSpec\nMomentum\nCyclicMomentum","category":"page"},{"location":"reference/network/#AlphaZero.Network.OptimiserSpec","page":"Network Interface","title":"AlphaZero.Network.OptimiserSpec","text":"OptimiserSpec\n\nAbstract type for an optimiser specification.\n\n\n\n\n\n","category":"type"},{"location":"reference/network/#AlphaZero.Network.Momentum","page":"Network Interface","title":"AlphaZero.Network.Momentum","text":"Momentum(; lr, momentum)\n\nSGD optimiser with momentum.\n\n\n\n\n\n","category":"type"},{"location":"reference/network/#AlphaZero.Network.CyclicMomentum","page":"Network Interface","title":"AlphaZero.Network.CyclicMomentum","text":"CyclicMomentum(; lr_low, lr_high, momentum_low, momentum_high)\n\nSGD optimiser with a cyclic momentum and learning rate.\n\nDuring an epoch, the learning rate goes from lr_low to lr_high and then back to lr_low.\nThe momentum evolves in the opposite way, from high values to low values and then back to high values.\n\n\n\n\n\n","category":"type"},{"location":"reference/params/#params-1","page":"Training Parameters","title":"Training Parameters","text":"","category":"section"},{"location":"reference/params/#","page":"Training Parameters","title":"Training Parameters","text":"CurrentModule = AlphaZero","category":"page"},{"location":"reference/params/#General-1","page":"Training Parameters","title":"General","text":"","category":"section"},{"location":"reference/params/#","page":"Training Parameters","title":"Training Parameters","text":"Params","category":"page"},{"location":"reference/params/#AlphaZero.Params","page":"Training Parameters","title":"AlphaZero.Params","text":"Params\n\nThe AlphaZero parameters.\n\nParameter Type Default\nself_play SelfPlayParams -\nlearning LearningParams -\narena ArenaParams -\nmemory_analysis Union{Nothing, MemAnalysisParams} nothing\nnum_iters Int -\nmem_buffer_size PLSchedule{Int} -\nternary_rewards Bool false\n\nExplanation\n\nThe AlphaZero training process consists in num_iters iterations. Each iteration can be decomposed into a self-play phase (see SelfPlayParams) and a learning phase (see LearningParams).\n\nternary_rewards: set to true if the rewards issued by the game environment always belong to -1 0 1 so that the logging and profiling tools can take advantage of this property.\nmem_buffer_size: size schedule of the memory buffer, in terms of number of samples. It is typical to start with a small memory buffer that is grown progressively so as to wash out the initial low-quality self-play data more quickly.\n\nAlphaGo Zero Parameters\n\nIn the original AlphaGo Zero paper:\n\nAbout 5 millions games of self-play are played across 200 iterations.\nThe memory buffer contains 500K games, which makes about 100M samples as an average game of Go lasts about 200 turns.\n\n\n\n\n\n","category":"type"},{"location":"reference/params/#Self-Play-1","page":"Training Parameters","title":"Self-Play","text":"","category":"section"},{"location":"reference/params/#","page":"Training Parameters","title":"Training Parameters","text":"SelfPlayParams","category":"page"},{"location":"reference/params/#AlphaZero.SelfPlayParams","page":"Training Parameters","title":"AlphaZero.SelfPlayParams","text":"SelfPlayParams\n\nParameters governing self-play.\n\nParameter Type Default\nmcts MctsParams -\nnum_games Int -\nreset_mcts_every Union{Int, Nothing} nothing\ngc_every Union{Int, Nothing} nothing\n\nExplanation\n\nThe gc_every field, when set, forces a full garbage collection and an emptying of the GPU memory pool periodically, the period being specified in terms of a fixed number of games.\nTo avoid running out of memory, the MCTS tree is reset every reset_mcts_every games (or never if nothing is passed).\n\nAlphaGo Zero Parameters\n\nIn the original AlphaGo Zero paper, num_games = 25_000 (5 millions games of self-play across 200 iterations).\n\n\n\n\n\n","category":"type"},{"location":"reference/params/#Learning-1","page":"Training Parameters","title":"Learning","text":"","category":"section"},{"location":"reference/params/#","page":"Training Parameters","title":"Training Parameters","text":"LearningParams\nSamplesWeighingPolicy","category":"page"},{"location":"reference/params/#AlphaZero.LearningParams","page":"Training Parameters","title":"AlphaZero.LearningParams","text":"LearningParams\n\nParameters governing the learning phase of a training iteration, where the neural network is updated to fit the data in the memory buffer.\n\nParameter Type Default\nuse_gpu Bool true\ngc_every Union{Nothing, Int} nothing\nsamples_weighing_policy SamplesWeighingPolicy -\noptimiser OptimiserSpec -\nl2_regularization Float32 -\nnonvalidity_penalty Float32 1f0\nbatch_size Int -\nloss_computation_batch_size Int -\ncheckpoints Vector{Int} -\n\nDescription\n\nThe neural network gets to see the whole content of the memory buffer at each learning epoch, for maximum(checkpoints) epochs in total. After each epoch whose number is in checkpoints, the current network is evaluated against the best network so far (see ArenaParams).\n\nnonvalidity_penalty is the multiplicative constant of a loss term that  corresponds to the average probability weight that the network puts on  invalid actions.\nbatch_size is the batch size used for gradient descent.\nloss_computation_batch_size is the batch size that is used to compute the loss between each epochs.\n\nAlphaGo Zero Parameters\n\nIn the original AlphaGo Zero paper:\n\nThe batch size for gradient updates is 2048.\nThe L2 regularization parameter is set to 10^-4.\nCheckpoints are produced every 1000 training steps, which corresponds to seeing about 20% of the samples in the memory buffer: (1000  2048)  10^7   02.\nIt is unclear how many checkpoints are taken or how many training steps are performed in total.\n\n\n\n\n\n","category":"type"},{"location":"reference/params/#AlphaZero.SamplesWeighingPolicy","page":"Training Parameters","title":"AlphaZero.SamplesWeighingPolicy","text":"SamplesWeighingPolicy\n\nDuring self-play, early board positions are possibly encountered many times across several games. The corresponding samples are then merged together and given a weight W that is a nondecreasing function of the number n of merged samples:\n\nCONSTANT_WEIGHT: W(n) = 1\nLOG_WEIGHT: W(n) = log_2(n) + 1\nLINEAR_WEIGHT: W(n) = n\n\n\n\n\n\n","category":"type"},{"location":"reference/params/#Arena-1","page":"Training Parameters","title":"Arena","text":"","category":"section"},{"location":"reference/params/#","page":"Training Parameters","title":"Training Parameters","text":"ArenaParams","category":"page"},{"location":"reference/params/#AlphaZero.ArenaParams","page":"Training Parameters","title":"AlphaZero.ArenaParams","text":"ArenaParams\n\nParameters that govern the evaluation process that compares a new neural network to the current best.\n\nParameter Type Default\nmcts MctsParams -\nnum_games Int -\nreset_mcts_every Union{Int, Nothing} nothing\nupdate_threshold Float64 -\n\nExplanation\n\nThe two competing networks are instantiated into two MCTS players of parameter mcts and then play num_games games, exchanging color after each game.\nThe new network is to replace the current best one if its average collected reward is greater or equal than update_threshold.\nTo avoid running out of memory, the MCTS trees of both player are reset every reset_mcts_every games (or never if nothing is passed).\n\nRemarks\n\nSee necessary_samples to make an informed choice for num_games.\n\nAlphaGo Zero Parameters\n\nIn the original AlphaGo Zero paper, 400 games are played to evaluate a network and the update_threshold parameter is set to a value that corresponds to a 55% win rate.\n\n\n\n\n\n","category":"type"},{"location":"reference/params/#Memory-Analysis-1","page":"Training Parameters","title":"Memory Analysis","text":"","category":"section"},{"location":"reference/params/#","page":"Training Parameters","title":"Training Parameters","text":"MemAnalysisParams","category":"page"},{"location":"reference/params/#AlphaZero.MemAnalysisParams","page":"Training Parameters","title":"AlphaZero.MemAnalysisParams","text":"MemAnalysisParams\n\nParameters governing the analysis of the memory buffer (for debugging and profiling purposes).\n\nParameter Type Default\nnum_game_stages Int -\n\nExplanation\n\nThe memory analysis consists in partitioning the memory buffer in num_game_stages parts of equal size, according to the number of remaining moves until the end of the game for each sample. Then, the quality of the predictions of the current neural network is evaluated on each subset (see Report.Memory).\n\nThis is useful to get an idea of how the neural network performance varies depending on the game stage (typically, good value estimates for endgame board positions are available earlier in the training process than good values for middlegame positions).\n\n\n\n\n\n","category":"type"},{"location":"reference/params/#MCTS-1","page":"Training Parameters","title":"MCTS","text":"","category":"section"},{"location":"reference/params/#","page":"Training Parameters","title":"Training Parameters","text":"MctsParams","category":"page"},{"location":"reference/params/#AlphaZero.MctsParams","page":"Training Parameters","title":"AlphaZero.MctsParams","text":"Parameters of an MCTS player.\n\nParameter Type Default\nnum_workers Int 1\nuse_gpu Bool false\nnum_iters_per_turn Int -\ncpuct Float64 1.\ntemperature StepSchedule{Float64} StepSchedule(1.)\ndirichlet_noise_ϵ Float64 -\ndirichlet_noise_α Float64 -\n\nExplanation\n\nAn MCTS player picks actions as follows. Given a game state, it launches num_iters_per_turn MCTS iterations that are executed asynchronously on num_workers workers, with UCT exploration constant cpuct.\n\nThen, an action is picked according to the distribution π where π_i  n_i^τ with n_i the number of time that the i^textth action was visited and τ the temperature parameter.\n\nIt is typical to use a high value of the temperature parameter τ during the first moves of a game to increase exploration and then switch to a small value. Therefore, temperature has type StepSchedule.\n\nFor information on parameters cpuct, dirichlet_noise_ϵ and dirichlet_noise_α, see MCTS.Env.\n\nAlphaGo Zero Parameters\n\nIn the original AlphaGo Zero paper:\n\nThe number of MCTS iterations per move is 1600, which corresponds to 0.4s of computation time.\nThe temperature is set to 1 for the 30 first moves and then to an infinitesimal value.\nThe ϵ parameter for the Dirichlet noise is set to 025 and the α parameter to 003, which is consistent with the heuristic of using α = 10n with n the maximum number of possibles moves, which is 19  19 + 1 = 362 in the case of Go.\n\n\n\n\n\n","category":"type"},{"location":"reference/params/#Utilities-1","page":"Training Parameters","title":"Utilities","text":"","category":"section"},{"location":"reference/params/#","page":"Training Parameters","title":"Training Parameters","text":"necessary_samples\nAbstractSchedule\nStepSchedule\nPLSchedule\nCyclicSchedule","category":"page"},{"location":"reference/params/#AlphaZero.necessary_samples","page":"Training Parameters","title":"AlphaZero.necessary_samples","text":"necessary_samples(ϵ, β) = log(1 / β) / (2 * ϵ^2)\n\nCompute the number of times N that a random variable X sim textBer(p) has to be sampled so that if the empirical average of X is greather than 12 + ϵ, then p  12 with probability at least 1-β.\n\nThis bound is based on Hoeffding's inequality .\n\n\n\n\n\n","category":"function"},{"location":"reference/params/#AlphaZero.AbstractSchedule","page":"Training Parameters","title":"AlphaZero.AbstractSchedule","text":"AbstractSchedule{R}\n\nAbstract type for a parameter schedule, which represents a function from nonnegative integers to numbers of type R. Subtypes must implement the getindex operator.\n\n\n\n\n\n","category":"type"},{"location":"reference/params/#AlphaZero.StepSchedule","page":"Training Parameters","title":"AlphaZero.StepSchedule","text":"StepSchedule{R} <: AbstractSchedule{R}\n\nType for step function schedules.\n\nConstructors\n\nStepSchedule(cst)\n\nReturn a schedule with a constant value cst.\n\nStepSchedule(;start, change_at, values)\n\nReturn a schedule that has initial value start. For all i, the schedule takes value values[i] at step change_at[i].\n\n\n\n\n\n","category":"type"},{"location":"reference/params/#AlphaZero.PLSchedule","page":"Training Parameters","title":"AlphaZero.PLSchedule","text":"PLSchedule{R} <: AbstractSchedule{R}\n\nType for piecewise linear schedules.\n\nConstructors\n\nPLSchedule(cst)\n\nReturn a schedule with a constant value cst.\n\nPLSchedule(xs, ys)\n\nReturn a piecewise linear schedule such that:\n\nFor all i, (xs[i], ys[i]) belongs to the schedule's graph.\nBefore xs[1], the schedule has value ys[1].\nAfter xs[end], the schedule has value ys[end].\n\n\n\n\n\n","category":"type"},{"location":"reference/params/#AlphaZero.CyclicSchedule","page":"Training Parameters","title":"AlphaZero.CyclicSchedule","text":"CyclicSchedule(start, mid; n, xmax)\n\nReturn a PLSchedule that describes a cycle from start to mid and then back to start.\n\n\n\n\n\n","category":"function"},{"location":"reference/session/#sessions-1","page":"Sessions","title":"Sessions","text":"","category":"section"},{"location":"reference/session/#","page":"Sessions","title":"Sessions","text":"CurrentModule = AlphaZero","category":"page"},{"location":"reference/session/#","page":"Sessions","title":"Sessions","text":"Session\nSession(::Env) # Strangely, this includes all constructors...","category":"page"},{"location":"reference/session/#AlphaZero.Session","page":"Sessions","title":"AlphaZero.Session","text":"Session{Env}\n\nA wrapper on an AlphaZero environment that adds features such as:\n\nLogging and plotting\nLoading and saving of environments\n\nIn particular, it implements the Handlers interface.\n\nPublic fields\n\nenv::Env is the environment wrapped by the session\nreport is the current session report, with type SessionReport\n\n\n\n\n\n","category":"type"},{"location":"reference/session/#AlphaZero.Session-Tuple{Env}","page":"Sessions","title":"AlphaZero.Session","text":"Session(::Type{Game}, ::Type{Net}, params, netparams) where {Game, Net}\n\nCreate a new session using the given parameters, or load it from disk if it already exists.\n\nArguments\n\nGame is the type ot the game that is being learnt\nNet is the type of the network that is being used\nparams has type Params\nnetparams has type Network.HyperParams(Net)\n\nOptional keyword arguments\n\ndir: session directory in which all files and reports are saved; this   argument is either a string or nothing (default), in which case the   session won't be saved automatically and no file will be generated\nautosave: if set to false, the session won't be saved automatically nor   any file will be generated (default is true)\nnostdout: disables logging on the standard output when set to true   (default is false)\nbenchmark: vector of Benchmark.Duel to be used as a benchmark   (default is [])\nload_saved_params: if set to true, load the training parameters from   the session directory (if present) rather than using the params   argument (default is false)\n\n\n\n\n\nSession(::Type{Game}, ::Type{Network}, dir::String) where {Game, Net}\n\nLoad an existing session from a directory.\n\nThis constructor accepts the optional keyword arguments autosave, nostdout and benchmark.\n\n\n\n\n\nSession(env::Env[, dir])\n\nCreate a session from an initial environment.\n\nThe iteration counter of the environment must be equal to 0\nIf a session directory is provided, this directory must not exist yet\n\nThis constructor features the optional keyword arguments autosave, nostdout and benchmark.\n\n\n\n\n\n","category":"method"},{"location":"reference/session/#Session-Reports-1","page":"Sessions","title":"Session Reports","text":"","category":"section"},{"location":"reference/session/#","page":"Sessions","title":"Sessions","text":"SessionReport","category":"page"},{"location":"reference/session/#AlphaZero.SessionReport","page":"Sessions","title":"AlphaZero.SessionReport","text":"SessionReport\n\nThe full collection of benchmarks and statistics collected during a training session.\n\nFields\n\niterations: vector of n iteration reports with type   Report.Iteration\nbenchmark: vector of n+1 benchmark reports with type   Benchmark.Report\n\n\n\n\n\n","category":"type"},{"location":"reference/reports/#reports-1","page":"Training Reports","title":"Training Reports","text":"","category":"section"},{"location":"reference/reports/#","page":"Training Reports","title":"Training Reports","text":"CurrentModule = AlphaZero","category":"page"},{"location":"reference/reports/#","page":"Training Reports","title":"Training Reports","text":"Report","category":"page"},{"location":"reference/reports/#AlphaZero.Report","page":"Training Reports","title":"AlphaZero.Report","text":"Analytical reports generated during training, for debugging and hyperparameters tuning.\n\n\n\n\n\n","category":"module"},{"location":"reference/reports/#Learning-Phase-1","page":"Training Reports","title":"Learning Phase","text":"","category":"section"},{"location":"reference/reports/#","page":"Training Reports","title":"Training Reports","text":"Report.Loss\nReport.LearningStatus\nReport.Checkpoint\nReport.Epoch\nReport.Learning","category":"page"},{"location":"reference/reports/#AlphaZero.Report.Loss","page":"Training Reports","title":"AlphaZero.Report.Loss","text":"Report.Loss\n\nDecomposition of the loss in a sum of terms (all have type Float32).\n\nL is the total loss: L == Lp + Lv + Lreg + Linv\nLp is the policy cross-entropy loss term\nLv is the average value mean square error\nLreg is the L2 regularization loss term\nLinv is the loss term penalizing the average weight put by the network on invalid actions\n\n\n\n\n\n","category":"type"},{"location":"reference/reports/#AlphaZero.Report.LearningStatus","page":"Training Reports","title":"AlphaZero.Report.LearningStatus","text":"Report.LearningStatus\n\nStatistics about the performance of the neural network on a subset of the memory buffer.\n\nloss: detailed loss on the samples, as an object of type   Report.Loss\nHp: average entropy of the π component of samples (MCTS policy);   this quantity is independent of the network and therefore constant   during a learning iteration\nHpnet: average entropy of the network's prescribed policy on the samples\n\n\n\n\n\n","category":"type"},{"location":"reference/reports/#AlphaZero.Report.Checkpoint","page":"Training Reports","title":"AlphaZero.Report.Checkpoint","text":"Report.Checkpoint\n\nReport generated after a checkpoint evaluation.\n\nepoch_id: number of epochs after which the checkpoint was computed\nreward: average reward collected by the contender network\nnn_replaced: true if the current best neural network was updated after   the checkpoint\n\n\n\n\n\n","category":"type"},{"location":"reference/reports/#AlphaZero.Report.Epoch","page":"Training Reports","title":"AlphaZero.Report.Epoch","text":"Report.Epoch\n\nReport generated after each learning epoch.\n\nstatus_after: learning status after the epoch, as an object of type   Report.LearningStatus\n\n\n\n\n\n","category":"type"},{"location":"reference/reports/#AlphaZero.Report.Learning","page":"Training Reports","title":"AlphaZero.Report.Learning","text":"Report.Learning\n\nReport generated at the end of the learning phase of an iteration.\n\ntime_convert, time_loss, time_train and time_eval are the   amounts of time (in seconds) spent at converting the samples,   computing losses, performing gradient updates and evaluating checkpoints   respectively\ninitial_status: status before the learning phase, as an object of type   Report.LearningStatus\nepochs: vector of Report.Epoch reports\ncheckpoints: vector of Report.Checkpoint reports\nnn_replaced: true if the best neural network was replaced\n\n\n\n\n\n","category":"type"},{"location":"reference/reports/#Memory-Analysis-Phase-1","page":"Training Reports","title":"Memory Analysis Phase","text":"","category":"section"},{"location":"reference/reports/#","page":"Training Reports","title":"Training Reports","text":"Report.Samples\nReport.StageSamples\nReport.Memory","category":"page"},{"location":"reference/reports/#AlphaZero.Report.Samples","page":"Training Reports","title":"AlphaZero.Report.Samples","text":"Report.Samples\n\nStatistics about a set of samples, as collected during memory analysis.\n\nnum_samples: total number of samples\nnum_boards: number of distinct board positions\nWtot: total weight of the samples\nstatus: Report.LearningStatus statistics of the current network   on the samples\n\n\n\n\n\n","category":"type"},{"location":"reference/reports/#AlphaZero.Report.StageSamples","page":"Training Reports","title":"AlphaZero.Report.StageSamples","text":"Report.StageSamples\n\nStatistics for the samples corresponding to a particular game stage, as collected during memory analysis.\n\nmean_remaining_length: average number of remaining moves until the   end of the game; this quantity defines a game stage\nsamples_stats: samples statistics, as an object of type   Report.Samples\n\n\n\n\n\n","category":"type"},{"location":"reference/reports/#AlphaZero.Report.Memory","page":"Training Reports","title":"AlphaZero.Report.Memory","text":"Report.Memory\n\nReport generated by the memory analysis phase of an iteration. It features samples statistics for\n\nthe whole memory buffer (all_samples :: Report.Samples)\nthe samples collected during the last self-play iteration  (latest_batch :: Report.Samples)\nthe subsets of the memory buffer corresponding to different game stages:  (per_game_stage :: Vector{Report.StageSamples})\n\nSee MemAnalysisParams.\n\n\n\n\n\n","category":"type"},{"location":"reference/reports/#Self-Play-Phase-1","page":"Training Reports","title":"Self-Play Phase","text":"","category":"section"},{"location":"reference/reports/#","page":"Training Reports","title":"Training Reports","text":"Report.SelfPlay","category":"page"},{"location":"reference/reports/#AlphaZero.Report.SelfPlay","page":"Training Reports","title":"AlphaZero.Report.SelfPlay","text":"Report.SelfPlay\n\nReport generated after the self-play phase of an iteration.\n\ninference_time_ratio: see MCTS.inference_time_ratio\nsamples_gen_speed: average number of samples generated per second\naverage_exploration_depth: see MCTS.average_exploration_depth\nmcts_memory_footprint: estimation of the maximal memory footprint of the   MCTS tree during self-play, as computed by   MCTS.approximate_memory_footprint\nmemory_size: number of samples in the memory buffer at the end of the   self-play phase\nmemory_num_distinct_boards: number of distinct board positions in the   memory buffer at the end of the self-play phase\n\n\n\n\n\n","category":"type"},{"location":"reference/reports/#Training-1","page":"Training Reports","title":"Training","text":"","category":"section"},{"location":"reference/reports/#","page":"Training Reports","title":"Training Reports","text":"Report.Initial\nReport.Iteration\nReport.Perfs","category":"page"},{"location":"reference/reports/#AlphaZero.Report.Initial","page":"Training Reports","title":"AlphaZero.Report.Initial","text":"Report.Initial\n\nReport summarizing the configuration of an agent before training starts.\n\nnum_network_parameters: see Network.num_parameters\nnum_network_regularized_parameters:   see Network.num_regularized_parameters\nmcts_footprint_per_node: see MCTS.memory_footprint_per_node\n\n\n\n\n\n","category":"type"},{"location":"reference/reports/#AlphaZero.Report.Iteration","page":"Training Reports","title":"AlphaZero.Report.Iteration","text":"Report.Iteration\n\nReport generated after each training iteration.\n\nperfs_self_play, perfs_memory_analysis and perfs_learning are   performance reports for the different phases of the iteration, as   objects of type Report.Perfs\nself_play, memory, learning have types Report.SelfPlay,   Report.SelfPlay and Report.Learning respectively\n\n\n\n\n\n","category":"type"},{"location":"reference/reports/#AlphaZero.Report.Perfs","page":"Training Reports","title":"AlphaZero.Report.Perfs","text":"Report.Perfs\n\nPerformances report for a subroutine.\n\ntime: total time spent, in seconds\nallocated: amount of memory allocated, in bytes\ngc_time: total amount of time spent in the garbage collector\n\n\n\n\n\n","category":"type"},{"location":"reference/environment/#environment-1","page":"Environment","title":"Environment","text":"","category":"section"},{"location":"reference/environment/#","page":"Environment","title":"Environment","text":"CurrentModule = AlphaZero","category":"page"},{"location":"reference/environment/#","page":"Environment","title":"Environment","text":"Env{Game, Network, Board}\nHandlers\nget_experience\ninitial_report\ntrain!(env::Env)","category":"page"},{"location":"reference/environment/#AlphaZero.Env","page":"Environment","title":"AlphaZero.Env","text":"Env{Game, Network, Board}\n\nType for an AlphZero environment.\n\nThe environment features the current best neural network, a memory buffer and an iteration counter.\n\nConstructor\n\nEnv{Game}(params, network, experience=[], itc=0)\n\nConstruct a new AlphaZero environment.\n\nGame is the type of the game being played\nparams has type Params\nnetwork is the initial neural network and has type AbstractNetwork\nexperience is the initial content of the memory buffer  as a vector of TrainingSample\nitc is the value of the iteration counter (0 at the start of training)\n\n\n\n\n\n","category":"type"},{"location":"reference/environment/#AlphaZero.Handlers","page":"Environment","title":"AlphaZero.Handlers","text":"Handlers\n\nNamespace for the callback functions that are used during training. This enables logging, saving and plotting to be implemented separately. An example handler object is Session.\n\nAll callback functions take a handler object h as their first argument and sometimes a second argment r that consists in a report.\n\nCallback Comment\niteration_started(h) called at the beggining of an iteration\nself_play_started(h) called once per iter before self play starts\ngame_played(h) called after each game of self play\nself_play_finished(h, r) sends report: Report.SelfPlay\nmemory_analyzed(h, r) sends report: Report.Memory\nlearning_started(h, r) sends report: Report.LearningStatus\nlearning_epoch(h, r) sends report: Report.Epoch\ncheckpoint_started(h) called before a checkpoint evaluation starts\ncheckpoint_game_played(h) called after each arena game\ncheckpoint_finished(h, r) sends report: Report.Checkpoint\nlearning_finished(h, r) sends report: Report.Learning\niteration_finished(h, r) sends report: Report.Iteration\ntraining_finished(h) called once at the end of training\n\n\n\n\n\n","category":"module"},{"location":"reference/environment/#AlphaZero.get_experience","page":"Environment","title":"AlphaZero.get_experience","text":"get_experience(env::Env)\n\nReturn the content of the agent's memory as a vector of TrainingSample.\n\n\n\n\n\n","category":"function"},{"location":"reference/environment/#AlphaZero.initial_report","page":"Environment","title":"AlphaZero.initial_report","text":"initial_report(env::Env)\n\nReturn a report summarizing the configuration of agent before training starts, as an object of type Report.Initial.\n\n\n\n\n\n","category":"function"},{"location":"reference/environment/#AlphaZero.train!-Tuple{Env}","page":"Environment","title":"AlphaZero.train!","text":"train!(env::Env, handler=nothing)\n\nStart or resume the training of an AlphaZero agent.\n\nA handler object can be passed that implements a subset of the callback functions defined in Handlers.\n\n\n\n\n\n","category":"method"},{"location":"reference/mcts/#MCTS-1","page":"MCTS","title":"MCTS","text":"","category":"section"},{"location":"reference/mcts/#","page":"MCTS","title":"MCTS","text":"CurrentModule = AlphaZero.MCTS","category":"page"},{"location":"reference/mcts/#","page":"MCTS","title":"MCTS","text":"MCTS","category":"page"},{"location":"reference/mcts/#AlphaZero.MCTS","page":"MCTS","title":"AlphaZero.MCTS","text":"A generic, standalone implementation of asynchronous Monte Carlo Tree Search. It can be used on any game that implements the GameInterface interface and with any external oracle.\n\n\n\n\n\n","category":"module"},{"location":"reference/mcts/#Oracles-1","page":"MCTS","title":"Oracles","text":"","category":"section"},{"location":"reference/mcts/#","page":"MCTS","title":"MCTS","text":"Oracle\nevaluate\nevaluate_batch\nRolloutOracle","category":"page"},{"location":"reference/mcts/#AlphaZero.MCTS.Oracle","page":"MCTS","title":"AlphaZero.MCTS.Oracle","text":"MCTS.Oracle{Game}\n\nAbstract base type for an oracle. Oracles must implement MCTS.evaluate and MCTS.evaluate_batch.\n\n\n\n\n\n","category":"type"},{"location":"reference/mcts/#AlphaZero.MCTS.evaluate","page":"MCTS","title":"AlphaZero.MCTS.evaluate","text":"MCTS.evaluate(oracle::Oracle, board, available_actions)\n\nEvaluate a single board position (assuming white is playing).\n\nReturn a pair (P, V) where:\n\nP is a probability vector on available actions\nV is a scalar estimating the value or win probability for white.\n\n\n\n\n\n","category":"function"},{"location":"reference/mcts/#AlphaZero.MCTS.evaluate_batch","page":"MCTS","title":"AlphaZero.MCTS.evaluate_batch","text":"MCTS.evaluate_batch(oracle::Oracle, batch)\n\nEvaluate a batch of board positions.\n\nExpect a vector of (board, available_actions) pairs and return a vector of (P, V) pairs.\n\nA default implementation is provided that calls MCTS.evaluate sequentially on each position.\n\n\n\n\n\n","category":"function"},{"location":"reference/mcts/#AlphaZero.MCTS.RolloutOracle","page":"MCTS","title":"AlphaZero.MCTS.RolloutOracle","text":"MCTS.RolloutOracle{Game} <: MCTS.Oracle{Game}\n\nThis oracle estimates the value of a position by simulating a random game from it (a rollout). Moreover, it puts a uniform prior on available actions. Therefore, it can be used to implement the \"vanilla\" MCTS algorithm.\n\n\n\n\n\n","category":"type"},{"location":"reference/mcts/#Environment-1","page":"MCTS","title":"Environment","text":"","category":"section"},{"location":"reference/mcts/#","page":"MCTS","title":"MCTS","text":"Env\nexplore!\npolicy\nreset!","category":"page"},{"location":"reference/mcts/#AlphaZero.MCTS.Env","page":"MCTS","title":"AlphaZero.MCTS.Env","text":"MCTS.Env{Game}(oracle; <keyword args>) where Game\n\nCreate and initialize an MCTS environment with a given oracle.\n\nKeyword Arguments\n\nnworkers=1: numbers of asynchronous workers (see below)\nfill_batches=false: if true, a constant batch size is enforced for  evaluation requests, by completing batches with dummy entries if necessary\ncpuct=1.: exploration constant in the UCT formula\nnoise_ϵ=0., noise_α=1.: parameters for the dirichlet exploration noise  (see below)\n\nAsynchronous MCTS\n\nIf nworkers == 1, MCTS is run in a synchronous fashion and the oracle is invoked through MCTS.evaluate.\nIf nworkers > 1, nworkers asynchronous workers are spawned, along with an additional task to serve board evaluation requests. Such requests are processed by batches of size nworkers using MCTS.evaluate_batch.\n\nDirichlet Noise\n\nA naive way to ensure exploration during training is to adopt an ϵ-greedy policy, playing a random move at every turn instead of using the policy prescribed by MCTS.policy with probability ϵ. The problem with this naive strategy is that it may lead the player to make terrible moves at critical moments, thereby biasing the policy evaluation mechanism.\n\nA superior alternative is to add a random bias to the neural prior for the root node during MCTS exploration: instead of considering the policy p output by the neural network in the UCT formula, one uses (1-ϵ)p + ϵη where η is drawn once per call to MCTS.explore! from a Dirichlet distribution of parameter α.\n\n\n\n\n\n","category":"type"},{"location":"reference/mcts/#AlphaZero.MCTS.explore!","page":"MCTS","title":"AlphaZero.MCTS.explore!","text":"MCTS.explore!(env, state, nsims)\n\nRun nsims MCTS iterations from state.\n\n\n\n\n\n","category":"function"},{"location":"reference/mcts/#AlphaZero.MCTS.policy","page":"MCTS","title":"AlphaZero.MCTS.policy","text":"MCTS.policy(env, state; τ=1.)\n\nReturn the recommended stochastic policy on state, with temperature parameter equal to τ. If τ is zero, all the weight goes to the action with the highest visits count.\n\nA call to this function must always be preceded by a call to MCTS.explore!.\n\n\n\n\n\n","category":"function"},{"location":"reference/mcts/#AlphaZero.MCTS.reset!","page":"MCTS","title":"AlphaZero.MCTS.reset!","text":"MCTS.reset!(env)\n\nEmpty the MCTS tree.\n\n\n\n\n\n","category":"function"},{"location":"reference/mcts/#Profiling-Utilities-1","page":"MCTS","title":"Profiling Utilities","text":"","category":"section"},{"location":"reference/mcts/#","page":"MCTS","title":"MCTS","text":"inference_time_ratio\nmemory_footprint_per_node\napproximate_memory_footprint\naverage_exploration_depth","category":"page"},{"location":"reference/mcts/#AlphaZero.MCTS.inference_time_ratio","page":"MCTS","title":"AlphaZero.MCTS.inference_time_ratio","text":"MCTS.inference_time_ratio(env)\n\nReturn the ratio of time spent by MCTS.explore! on position evaluation (through functions MCTS.evaluate or MCTS.evaluate_batch) since the environment's creation.\n\n\n\n\n\n","category":"function"},{"location":"reference/mcts/#AlphaZero.MCTS.memory_footprint_per_node","page":"MCTS","title":"AlphaZero.MCTS.memory_footprint_per_node","text":"MCTS.memory_footprint_per_node(env)\n\nReturn an estimate of the memory footprint of a single node of the MCTS tree (in bytes).\n\n\n\n\n\n","category":"function"},{"location":"reference/mcts/#AlphaZero.MCTS.approximate_memory_footprint","page":"MCTS","title":"AlphaZero.MCTS.approximate_memory_footprint","text":"MCTS.approximate_memory_footprint(env)\n\nReturn an estimate of the memory footprint of the MCTS tree (in bytes).\n\n\n\n\n\n","category":"function"},{"location":"reference/mcts/#AlphaZero.MCTS.average_exploration_depth","page":"MCTS","title":"AlphaZero.MCTS.average_exploration_depth","text":"MCTS.average_exploration_depth(env)\n\nReturn the average number of nodes that are traversed during an MCTS iteration, not counting the root.\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#game_interface-1","page":"Game Interface","title":"Game Interface","text":"","category":"section"},{"location":"reference/game_interface/#","page":"Game Interface","title":"Game Interface","text":"CurrentModule = AlphaZero.GameInterface","category":"page"},{"location":"reference/game_interface/#","page":"Game Interface","title":"Game Interface","text":"GameInterface","category":"page"},{"location":"reference/game_interface/#AlphaZero.GameInterface","page":"Game Interface","title":"AlphaZero.GameInterface","text":"A generic interface for two-players, symmetric, zero-sum games.\n\n\n\n\n\n","category":"module"},{"location":"reference/game_interface/#Interface-Summary-1","page":"Game Interface","title":"Interface Summary","text":"","category":"section"},{"location":"reference/game_interface/#","page":"Game Interface","title":"Game Interface","text":"Types, traits and constructors\nGame()\nGame(board)\nGame(board, white_playing)\nBase.copy(game)\nBoard(Game)\nAction(Game)\nGame functions\nwhite_playing(state)\nwhite_reward(state)\nboard(state)\nboard_symmetric(state)\navailable_actions(state)\nplay!(state, action)\nMachine learning interface\nvectorize_board(Game, board)\nnum_actions(Game)\naction_id(Game, action)\naction(Game, action_id)\nInterface for interactive tools\naction_string(Game, action)\nparse_action(Game, str)\nread_state(Game)\nprint_state(state)","category":"page"},{"location":"reference/game_interface/#Types-1","page":"Game Interface","title":"Types","text":"","category":"section"},{"location":"reference/game_interface/#","page":"Game Interface","title":"Game Interface","text":"AbstractGame\nBase.copy(::AbstractGame)\nBoard\nAction","category":"page"},{"location":"reference/game_interface/#AlphaZero.GameInterface.AbstractGame","page":"Game Interface","title":"AlphaZero.GameInterface.AbstractGame","text":"AbstractGame\n\nAbstract base type for a game state.\n\nConstructors\n\nAny subtype Game must implement the following constructors:\n\nGame()\n\nReturn the initial state of the game.\n\nGame(board, white_playing=true)\n\nReturn the unique state specified by a board and a current player. By convention, the first player to play is called white and the other is called black.\n\n\n\n\n\n","category":"type"},{"location":"reference/game_interface/#Base.copy-Tuple{AlphaZero.GameInterface.AbstractGame}","page":"Game Interface","title":"Base.copy","text":"Base.copy(::AbstractGame)\n\nReturn a fresh copy of a game state.\n\n\n\n\n\n","category":"method"},{"location":"reference/game_interface/#AlphaZero.GameInterface.Board","page":"Game Interface","title":"AlphaZero.GameInterface.Board","text":"Board(Game::Type{<:AbstractGame})\n\nReturn the board type corresponding to Game.\n\nBoard objects must be persistent or appear as such.\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#AlphaZero.GameInterface.Action","page":"Game Interface","title":"AlphaZero.GameInterface.Action","text":"Action(Game::Type{<:AbstractGame})\n\nReturn the action type corresponding to Game.\n\nActions must be \"symmetric\" in the following sense:\n\navailable_actions(s) ==\n  available_actions(Game(board_symmetric(s), !white_playing(s)))\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#Game-Functions-1","page":"Game Interface","title":"Game Functions","text":"","category":"section"},{"location":"reference/game_interface/#","page":"Game Interface","title":"Game Interface","text":"white_playing\nwhite_reward\nboard\nboard_symmetric\navailable_actions\nplay!\nheuristic_value","category":"page"},{"location":"reference/game_interface/#AlphaZero.GameInterface.white_playing","page":"Game Interface","title":"AlphaZero.GameInterface.white_playing","text":"white_playing(state::AbstractGame) :: Bool\n\nReturn true if white is to play and false otherwise.\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#AlphaZero.GameInterface.white_reward","page":"Game Interface","title":"AlphaZero.GameInterface.white_reward","text":"white_reward(state::AbstractGame)\n\nReturn nothing if the game hasn't ended. Otherwise, return a reward for the white player as a number between -1 and 1.\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#AlphaZero.GameInterface.board","page":"Game Interface","title":"AlphaZero.GameInterface.board","text":"board(state::AbstractGame)\n\nReturn the game board.\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#AlphaZero.GameInterface.board_symmetric","page":"Game Interface","title":"AlphaZero.GameInterface.board_symmetric","text":"board_symmetric(state::AbstractGame)\n\nReturn the symmetric of the game board (where the roles of black and white are swapped).\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#AlphaZero.GameInterface.available_actions","page":"Game Interface","title":"AlphaZero.GameInterface.available_actions","text":"available_actions(state::AbstractGame)\n\nReturn the vector of all available actions, which must be nonempty if isnothing(white_reward(state)).\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#AlphaZero.GameInterface.play!","page":"Game Interface","title":"AlphaZero.GameInterface.play!","text":"play!(state::AbstractGame, action)\n\nUpdate the game state by making the current player perform action.\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#AlphaZero.GameInterface.heuristic_value","page":"Game Interface","title":"AlphaZero.GameInterface.heuristic_value","text":"heuristic_value(state::AbstractGame)\n\nReturn a heuristic estimate of the state value for the current player.\n\nThe given state must be nonfinal and returned values must belong to the (- ) interval. Also, implementations of this function must be antisymmetric in the sense that:\n\nheuristic_value(s) ==\n  - heuristic_value(Game(board_symmetric(s), white_playing(s)))\n\nThis function is not needed by AlphaZero but it is useful for building baselines such as minmax players.\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#Machine-Learning-Interface-1","page":"Game Interface","title":"Machine Learning Interface","text":"","category":"section"},{"location":"reference/game_interface/#","page":"Game Interface","title":"Game Interface","text":"vectorize_board\nnum_actions\naction_id\naction","category":"page"},{"location":"reference/game_interface/#AlphaZero.GameInterface.vectorize_board","page":"Game Interface","title":"AlphaZero.GameInterface.vectorize_board","text":"vectorize_board(::Type{<:AbstractGame}, board) :: Vector{Float32}\n\nReturn a vectorized representation of a board.\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#AlphaZero.GameInterface.num_actions","page":"Game Interface","title":"AlphaZero.GameInterface.num_actions","text":"num_actions(::Type{<:AbstractGame}) :: Int\n\nReturn the total number of actions for a game.\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#AlphaZero.GameInterface.action_id","page":"Game Interface","title":"AlphaZero.GameInterface.action_id","text":"action_id(G::Type{<:AbstractGame}, action) :: Int\n\nMap each action to a unique number in the range 1:num_actions(G).\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#AlphaZero.GameInterface.action","page":"Game Interface","title":"AlphaZero.GameInterface.action","text":"action(::Type{<:AbstractGame}, Int)\n\nInverse function of action_id.\n\nMap an action identifier to an actual action.\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#Interface-for-Interactive-Tools-1","page":"Game Interface","title":"Interface for Interactive Tools","text":"","category":"section"},{"location":"reference/game_interface/#","page":"Game Interface","title":"Game Interface","text":"action_string\nparse_action\nread_state\nprint_state","category":"page"},{"location":"reference/game_interface/#AlphaZero.GameInterface.action_string","page":"Game Interface","title":"AlphaZero.GameInterface.action_string","text":"action_string(::Type{<:AbstractGame}, action) :: String\n\nReturn a human-readable string representing the provided action.\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#AlphaZero.GameInterface.parse_action","page":"Game Interface","title":"AlphaZero.GameInterface.parse_action","text":"parse_action(::Type{<:AbstractGame}, str::String)\n\nReturn the action described by string str or nothing if str does not denote a valid action.\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#AlphaZero.GameInterface.read_state","page":"Game Interface","title":"AlphaZero.GameInterface.read_state","text":"read_state(::Type{G}) where G <: AbstractGame :: Union{G, Nothing}\n\nRead a state description from the standard input. Return the corresponding state or nothing in case of an invalid input.\n\n\n\n\n\n","category":"function"},{"location":"reference/game_interface/#AlphaZero.GameInterface.print_state","page":"Game Interface","title":"AlphaZero.GameInterface.print_state","text":"print_state(state::AbstractGame)\n\nPrint a state on the standard output.\n\n\n\n\n\n","category":"function"},{"location":"reference/player/#player-1","page":"Players","title":"Players","text":"","category":"section"},{"location":"reference/player/#","page":"Players","title":"Players","text":"CurrentModule = AlphaZero","category":"page"},{"location":"reference/player/#Player-Interface-1","page":"Players","title":"Player Interface","text":"","category":"section"},{"location":"reference/player/#","page":"Players","title":"Players","text":"AbstractPlayer\nthink\nselect_move\nreset_player!","category":"page"},{"location":"reference/player/#AlphaZero.AbstractPlayer","page":"Players","title":"AlphaZero.AbstractPlayer","text":"AbstractPlayer{Game}\n\nAbstract type for a player of Game.\n\n\n\n\n\n","category":"type"},{"location":"reference/player/#AlphaZero.think","page":"Players","title":"AlphaZero.think","text":"think(::AbstractPlayer, state, turn=nothing)\n\nReturn a probability distribution over actions as a (actions, π) pair.\n\nThe turn argument, if provided, indicates the number of actions that have been played before by both players in the current game. It is useful as during self-play, AlphaZero typically drops its temperature parameter after a fixed number of turns.\n\n\n\n\n\n","category":"function"},{"location":"reference/player/#AlphaZero.select_move","page":"Players","title":"AlphaZero.select_move","text":"select_move(player::AbstractPlayer, state, turn=nothing)\n\nReturn a single action. A default implementation is provided that samples an action according to the distribution computed by think.\n\n\n\n\n\n","category":"function"},{"location":"reference/player/#AlphaZero.reset_player!","page":"Players","title":"AlphaZero.reset_player!","text":"reset_player!(::AbstractPlayer)\n\nReset the internal memory of a player (e.g. the MCTS tree). The default implementation does nothing.\n\n\n\n\n\n","category":"function"},{"location":"reference/player/#Player-Instances-1","page":"Players","title":"Player Instances","text":"","category":"section"},{"location":"reference/player/#","page":"Players","title":"Players","text":"MctsPlayer\nRandomPlayer\nNetworkPlayer\nEpsilonGreedyPlayer","category":"page"},{"location":"reference/player/#AlphaZero.MctsPlayer","page":"Players","title":"AlphaZero.MctsPlayer","text":"MctsPlayer{Game, MctsEnv} <: AbstractPlayer{Game}\n\nA player that selects actions using MCTS.\n\nConstructors\n\nMctsPlayer(mcts::MCTS.Env; τ, niters, timeout=nothing)\n\nConstruct a player from an MCTS environment. When computing each move:\n\nif timeout is provided, MCTS simulations are executed for timeout seconds by groups of niters\notherwise, niters MCTS simulations are run\n\nThe temperature parameter τ can be either a real number or a StepSchedule.\n\nMctsPlayer(oracle::MCTS.Oracle, params::MctsParams)\n\nConstruct an MCTS player from an oracle and an MctsParams structure. If the oracle is a network, this constructor handles copying it, putting it in test mode and copying it on the GPU (if necessary).\n\n\n\n\n\n","category":"type"},{"location":"reference/player/#AlphaZero.RandomPlayer","page":"Players","title":"AlphaZero.RandomPlayer","text":"RandomPlayer{Game} <: AbstractPlayer{Game}\n\nA player that picks actions uniformly at random.\n\n\n\n\n\n","category":"type"},{"location":"reference/player/#AlphaZero.NetworkPlayer","page":"Players","title":"AlphaZero.NetworkPlayer","text":"NetworkPlayer{Game, Net} <: AbstractPlayer{Game}\n\nA player that uses the policy output by a neural network directly, instead of relying on MCTS.\n\n\n\n\n\n","category":"type"},{"location":"reference/player/#AlphaZero.EpsilonGreedyPlayer","page":"Players","title":"AlphaZero.EpsilonGreedyPlayer","text":"EpsilonGreedyPlayer{Game, Player} <: AbstractPlayer{Game}\n\nA wrapper on a player that makes it choose a random move with a fixed ϵ probability.\n\n\n\n\n\n","category":"type"},{"location":"reference/player/#Derived-Functions-1","page":"Players","title":"Derived Functions","text":"","category":"section"},{"location":"reference/player/#","page":"Players","title":"Players","text":"play\nself_play!\npit\nColorPolicy\ninteractive!\nHuman","category":"page"},{"location":"reference/player/#AlphaZero.play","page":"Players","title":"AlphaZero.play","text":"play(white, black, memory=nothing)\n\nPlay a game between two AbstractPlayer and return the reward obtained by white.\n\nIf the memory argument is provided, samples are automatically collected from this game (see MemoryBuffer).\n\n\n\n\n\n","category":"function"},{"location":"reference/player/#AlphaZero.self_play!","page":"Players","title":"AlphaZero.self_play!","text":"self_play!(player, memory) = play(player, player, memory)\n\nPlay a game against oneself while collecting samples.\n\n\n\n\n\n","category":"function"},{"location":"reference/player/#AlphaZero.pit","page":"Players","title":"AlphaZero.pit","text":"pit(handler, baseline, contender, ngames)\n\nEvaluate two AbstractPlayer against each other on a series of games.\n\nArguments\n\nhandler: this function is called after each simulated  game with two arguments: the game number i and the collected reward z  for the contender player\nngames: number of games to play\n\nOptional keyword arguments\n\nreset_every: if set, players are reset every reset_every games\ncolor_policy: determine the ColorPolicy, which is ALTERNATE_COLORS by default\n\n\n\n\n\n","category":"function"},{"location":"reference/player/#AlphaZero.ColorPolicy","page":"Players","title":"AlphaZero.ColorPolicy","text":"@enum ColorPolicy ALTERNATE_COLORS BASELINE_WHITE CONTENDER_WHITE\n\nPolicy for attributing colors in a duel between a baseline and a contender.\n\n\n\n\n\n","category":"type"},{"location":"reference/player/#AlphaZero.interactive!","page":"Players","title":"AlphaZero.interactive!","text":"interactive!(game, white, black)\n\nLaunch an interactive session for game::AbstractGame between players white and black. Both players have type AbstractPlayer and one of them is typically Human.\n\n\n\n\n\n","category":"function"},{"location":"reference/player/#AlphaZero.Human","page":"Players","title":"AlphaZero.Human","text":"Human{Game} <: AbstractPlayer{Game}\n\nHuman player that queries the standard input for actions.\n\nDoes not implement think but instead implements select_move directly.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/connect-four/#connect-four-1","page":"Learning to Play Connect Four","title":"Learning to Play Connect Four","text":"","category":"section"},{"location":"tutorial/connect-four/#","page":"Learning to Play Connect Four","title":"Learning to Play Connect Four","text":"In this section, we discuss how to use AlphaZero.jl to learn to play Connect Four without any form of supervision or prior knowledge. Although the game has been solved exactly with Alpha-beta pruning using domain-specific heuristics and optimizations, it is still a great challenge for reinforcement learning.[1]","category":"page"},{"location":"tutorial/connect-four/#","page":"Learning to Play Connect Four","title":"Learning to Play Connect Four","text":"[1]: To the best of our knowledge, none of the many existing Python implementations of AlphaZero are able to learn a player that beats a simple minmax baseline (that plans at depth at least 2) on a single desktop computer.","category":"page"},{"location":"tutorial/connect-four/#Setup-1","page":"Learning to Play Connect Four","title":"Setup","text":"","category":"section"},{"location":"tutorial/connect-four/#","page":"Learning to Play Connect Four","title":"Learning to Play Connect Four","text":"To replicate the experiment in this tutorial, we recommend having a CUDA compatible GPU with 6GB of memory or more. Each training iteration took about one hour and a half on a standard desktop computer with an Intel Core i5 9600K processor and an Nvidia RTX 2070 GPU.","category":"page"},{"location":"tutorial/connect-four/#","page":"Learning to Play Connect Four","title":"Learning to Play Connect Four","text":"note: Note\nTo get optimal performances, it is also recommended to use AlphaZero.jl with Julia 1.4 (nightly), which includes a critical feature that enables CuArrays to force incremental GC collections.","category":"page"},{"location":"tutorial/connect-four/#","page":"Learning to Play Connect Four","title":"Learning to Play Connect Four","text":"To download AlphaZero.jl and start a new training session, just run the following:","category":"page"},{"location":"tutorial/connect-four/#","page":"Learning to Play Connect Four","title":"Learning to Play Connect Four","text":"git clone https://github.com/jonathan-laurent/AlphaZero.jl.git\ncd AlphaZero.jl\njulia --project -e \"import Pkg; Pkg.instantiate()\"\njulia --project --color=yes scripts/alphazero.jl --game connect-four train","category":"page"},{"location":"tutorial/connect-four/#","page":"Learning to Play Connect Four","title":"Learning to Play Connect Four","text":"Instead of using the the alphazero.jl script, one can also run the following into the Julia REPL:","category":"page"},{"location":"tutorial/connect-four/#","page":"Learning to Play Connect Four","title":"Learning to Play Connect Four","text":"ENV[\"CUARRAYS_MEMORY_POOL\"] = \"split\"\n\nusing AlphaZero\n\ninclude(\"games/connect-four/main.jl\")\nusing .ConnectFour: Game, Training\n\nconst SESSION_DIR = \"sessions/connect-four\"\n\nsession = AlphaZero.Session(\n    Game,\n    Training.Network{ConnectFour.Game},\n    Training.params,\n    Training.netparams,\n    benchmark=Training.benchmark,\n    dir=SESSION_DIR)\n\nresume!(session)","category":"page"},{"location":"tutorial/connect-four/#","page":"Learning to Play Connect Four","title":"Learning to Play Connect Four","text":"The first line configures CuArrays to use a splitting memory pool, which performs better than the default binned pool on AlphaZero's workload as it does not require to run the garbage collector as frequently. Then, a new AlphaZero session is created with the following arguments:","category":"page"},{"location":"tutorial/connect-four/#","page":"Learning to Play Connect Four","title":"Learning to Play Connect Four","text":"Argument Description\nGame Game type, which implements the game interface.\nTraining.Network Network type, which implements the network interface.\nTraining.params Training parameters.\nTraining.netparams Network hyperparameters.\nTraining.benchmark Benchmark that is run between training iterations.\nSESSION_DIR Directory in which all session files are saved.","category":"page"},{"location":"tutorial/connect-four/#","page":"Learning to Play Connect Four","title":"Learning to Play Connect Four","text":"In the following sections, we discuss some of those arguments in more details.","category":"page"},{"location":"tutorial/connect-four/#Neural-Network-1","page":"Learning to Play Connect Four","title":"Neural Network","text":"","category":"section"},{"location":"tutorial/connect-four/#","page":"Learning to Play Connect Four","title":"Learning to Play Connect Four","text":"Here, we use the same architecture that is deployed in AlphaGo Zero, namely a two-headed convolutional resnet with batch normalization. However, our network is is smaller in size as it only features 7 blocks (instead of 20) and 128 convolutional filters per layer (instead of 256), resulting in about 2.5M parameters (instead of 90M).","category":"page"},{"location":"tutorial/connect-four/#Training-Parameters-1","page":"Learning to Play Connect Four","title":"Training Parameters","text":"","category":"section"},{"location":"tutorial/connect-four/#","page":"Learning to Play Connect Four","title":"Learning to Play Connect Four","text":"Here, we simulate 5000 games of   self-play per iteration, using 400 MCTS iterations per move. For comparison,   the original AlphaGo Zero plays 25,000 games of self-play per iteration,   using 1600 MCTS iterations per move.","category":"page"},{"location":"tutorial/connect-four/#","page":"Learning to Play Connect Four","title":"Learning to Play Connect Four","text":"Implementation Games per iteration Moves per Game Sims per move Inference cost\nAlphaGo Zero 25,000 200 1600 x40\nAlphaZero.jl 5,000 30 400 x1\nPython impl 100 30 25 x1","category":"page"},{"location":"tutorial/connect-four/#Benchmarks-1","page":"Learning to Play Connect Four","title":"Benchmarks","text":"","category":"section"},{"location":"tutorial/connect-four/#Results-1","page":"Learning to Play Connect Four","title":"Results","text":"","category":"section"},{"location":"tutorial/connect-four/#","page":"Learning to Play Connect Four","title":"Learning to Play Connect Four","text":"(Image: Session CLI)","category":"page"},{"location":"reference/memory/#memory-1","page":"Memory Buffer","title":"Memory Buffer","text":"","category":"section"},{"location":"reference/memory/#","page":"Memory Buffer","title":"Memory Buffer","text":"CurrentModule = AlphaZero","category":"page"},{"location":"reference/memory/#","page":"Memory Buffer","title":"Memory Buffer","text":"TrainingSample\nMemoryBuffer","category":"page"},{"location":"reference/memory/#AlphaZero.TrainingSample","page":"Memory Buffer","title":"AlphaZero.TrainingSample","text":"TrainingSample{Board}\n\nType of a training sample. A sample features the following fields:\n\nb::Board is the board position (by convention, white is to play)\nπ::Vector{Float64} is the recorded MCTS policy for this position\nz::Float64 is the reward collected at the end of the game\nt::Float64 is the number of moves remaining before the end of the game\nn::Int is the number of times the board position b was recorded\n\nAs revealed by the last field n, several samples that correspond to the same board position can be merged, in which case the π, z and t fields are averaged together.\n\n\n\n\n\n","category":"type"},{"location":"reference/memory/#AlphaZero.MemoryBuffer","page":"Memory Buffer","title":"AlphaZero.MemoryBuffer","text":"MemoryBuffer{Board}\n\nA circular buffer to hold memory samples.\n\nHow to use\n\nUse new_batch!(mem) to start a new batch, typically once per iteration before self-play.\nUse push_sample!(mem, board, policy, white_playing, turn) to record a sample during a game, where turn is the number of actions that have been played by both players since the start of the game.\nUse push_game!(mem, white_reward, game_length) when a game terminates for which samples have been collected.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/alphazero_intro/#Introduction-to-AlphaZero-1","page":"Introduction to AlphaZero","title":"Introduction to AlphaZero","text":"","category":"section"},{"location":"tutorial/alphazero_intro/#","page":"Introduction to AlphaZero","title":"Introduction to AlphaZero","text":"AlphaZero elegantly combines search and learning, which are described in Rich Sutton's essay   \"The Bitter Lesson\" as the two fundamental pillars of AI. It augments a tree search procedure with two learnt heuristics: one to evaluate board positions and one to concentrate branching on moves that are not obviously wrong.","category":"page"},{"location":"tutorial/alphazero_intro/#","page":"Introduction to AlphaZero","title":"Introduction to AlphaZero","text":"When training starts, both heuristics are initialized randomly and tree search has only access to a meaningful signal at the level of final states, where the game outcome is known. These heuristics are then improved iteratively through self-play. More specifically:","category":"page"},{"location":"tutorial/alphazero_intro/#","page":"Introduction to AlphaZero","title":"Introduction to AlphaZero","text":"The heuristics are implemented by a two-headed neural network. Given a board position as an input, it estimates the probability for each player to ultimately win the game. It also provides a quantitative estimate of the relative quality of all available moves in the form of a probability distribution.\nThe search component is powered by Monte-Carlo Tree Search (MCTS), which implements a good compromise between breadth-first and depth-first search and provides a principled way to manage the uncertainty introduced by the heuristics. Also, given a position, it does not return a single choice for a best move but rather a probability distribution over available moves.\nAt each training iteration, AlphaZero plays a series of games against itself. The network is then updated so that it makes more accurate predictions about the outcome of these games. Also, the network's policy heuristic is updated to match the output of MCTS on all encountered positions. This way, MCTS can be seen as a powerful policy improvement operator.","category":"page"},{"location":"tutorial/alphazero_intro/#","page":"Introduction to AlphaZero","title":"Introduction to AlphaZero","text":"For more details, we recommend the following resources.","category":"page"},{"location":"tutorial/alphazero_intro/#External-resources-1","page":"Introduction to AlphaZero","title":"External resources","text":"","category":"section"},{"location":"tutorial/alphazero_intro/#","page":"Introduction to AlphaZero","title":"Introduction to AlphaZero","text":"A good resource to learn about Monte Carlo Tree Search (MCTS) is this  Int8 tutorial.\nA simple and effective introduction to AlphaZero is Surag Nair's  excellent tutorial.\nThen, DeepMind's original  Nature paper  is a nice read.\nFinally, this series of posts  from Oracle has been an important source of inspiration for AlphaZero.jl.  It provides useful details on implementing asynchronous MCTS, along with  an interesting discussion on hyperparameters tuning.","category":"page"},{"location":"reference/overview/#overview-1","page":"Codebase Overview","title":"Codebase Overview","text":"","category":"section"},{"location":"#AlphaZero.jl-1","page":"Home","title":"AlphaZero.jl","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"This package provides a generic, simple and fast implementation of Deepmind's AlphaZero algorithm:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"The core algorithm is only 2,000 lines of pure, hackable Julia code.\nGeneric interfaces make it easy to add support for new games or new learning frameworks.\nBeing about two orders of magnitude faster than competing alternatives written in Python, this implementation enables to solve nontrivial games on a standard desktop computer with a GPU.","category":"page"},{"location":"#Why-should-I-care-about-AlphaZero?-1","page":"Home","title":"Why should I care about AlphaZero?","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Beyond its much publicized success in attaining superhuman level at games such as Chess and Go, DeepMind's AlphaZero algorithm illustrates a more general methodology of combining learning and search to explore large combinatorial spaces effectively. We believe that this methodology can have exciting applications in many different research areas.","category":"page"},{"location":"#What-does-make-this-implementation-fast-and-why-does-it-matter?-1","page":"Home","title":"What does make this implementation fast and why does it matter?","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Because AlphaZero is resource-hungry, successful open-source implementations (such as   Leela Zero) are written in low-level languages (such as C++) and optimized to work on large, distributed clusters. This makes them hardly accessible for researchers and hackers.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Many simple Python implementations can be found on Github, but none of them is able to beat a reasonable baseline on games such as Connect Four or Othello. As an illustration, the benchmark in the README of the most popular of them only features a random baseline, along with a greedy baseline that does not appear to be significantly stronger.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"AlphaZero.jl is designed to be as simple as those implementations. In addition, it is about two orders of magnitude faster, making it possible to solve nontrivial games on a standard desktop computer with a GPU. This gain comes mostly from two sources:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Julia's inherent speed: most machine learning algorithms do not suffer much from being written in Python as most of the computation happens within heavily optimized matrix manipulation routines. This is not the case with AlphaZero, where tree search is also a possible bottleneck.\nAn asynchronous MCTS implementation: even more importantly, a key aspect in making MCTS scale is to enable several workers to explore the search tree asynchronously. This is a huge win even on a single machine, as it enables to perform neural-network inference on large batches rather than evaluating board positions separately, thereby maximizing the GPU utilization.","category":"page"},{"location":"tutorial/add_game/#Adding-new-games-1","page":"Adding new games","title":"Adding new games","text":"","category":"section"}]
}
