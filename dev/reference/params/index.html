<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Training Parameters · AlphaZero</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="AlphaZero logo"/></a><div class="docs-package-name"><span class="docs-autofit">AlphaZero</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Guided Tour</span><ul><li><a class="tocitem" href="../../tutorial/alphazero_intro/">Introduction to AlphaZero</a></li><li><a class="tocitem" href="../../tutorial/package_overview/">Package Overview</a></li><li><a class="tocitem" href="../../tutorial/connect_four/">Training a Connect Four Agent</a></li><li><a class="tocitem" href="../../tutorial/own_game/">Solving Your Own Games</a></li></ul></li><li><span class="tocitem">Reference</span><ul><li class="is-active"><a class="tocitem" href>Training Parameters</a><ul class="internal"><li><a class="tocitem" href="#General"><span>General</span></a></li><li><a class="tocitem" href="#Self-Play"><span>Self-Play</span></a></li><li><a class="tocitem" href="#Learning"><span>Learning</span></a></li><li><a class="tocitem" href="#Arena"><span>Arena</span></a></li><li><a class="tocitem" href="#Memory-Analysisautodocsautodocs"><span>Memory Analysisautodocsautodocs</span></a></li><li><a class="tocitem" href="#MCTS"><span>MCTS</span></a></li><li><a class="tocitem" href="#Simulations"><span>Simulations</span></a></li><li><a class="tocitem" href="#Utilities"><span>Utilities</span></a></li></ul></li><li><a class="tocitem" href="../game_interface/">Game Interface</a></li><li><a class="tocitem" href="../mcts/">MCTS</a></li><li><a class="tocitem" href="../network/">Network Interface</a></li><li><a class="tocitem" href="../networks_library/">Networks Library</a></li><li><a class="tocitem" href="../player/">Players and Simulations</a></li><li><a class="tocitem" href="../memory/">Memory Buffer</a></li><li><a class="tocitem" href="../environment/">Environment</a></li><li><a class="tocitem" href="../benchmark/">Benchmark</a></li><li><a class="tocitem" href="../reports/">Training Reports</a></li><li><a class="tocitem" href="../experiment/">Game Interface</a></li><li><a class="tocitem" href="../ui/">User Interface</a></li><li><a class="tocitem" href="../scripts/">Quick Scripts</a></li></ul></li><li><span class="tocitem">Contributing</span><ul><li><a class="tocitem" href="../../contributing/guide/">Contribution Guide</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Reference</a></li><li class="is-active"><a href>Training Parameters</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Training Parameters</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/jonathan-laurent/AlphaZero.jl/blob/master/docs/src/reference/params.md#L" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="params"><a class="docs-heading-anchor" href="#params">Training Parameters</a><a id="params-1"></a><a class="docs-heading-anchor-permalink" href="#params" title="Permalink"></a></h1><h2 id="General"><a class="docs-heading-anchor" href="#General">General</a><a id="General-1"></a><a class="docs-heading-anchor-permalink" href="#General" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="AlphaZero.Params" href="#AlphaZero.Params"><code>AlphaZero.Params</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Params</code></pre><p>The AlphaZero training hyperparameters.</p><table><tr><th style="text-align: left">Parameter</th><th style="text-align: left">Type</th><th style="text-align: left">Default</th></tr><tr><td style="text-align: left"><code>self_play</code></td><td style="text-align: left"><a href="#AlphaZero.SelfPlayParams"><code>SelfPlayParams</code></a></td><td style="text-align: left">-</td></tr><tr><td style="text-align: left"><code>learning</code></td><td style="text-align: left"><a href="#AlphaZero.LearningParams"><code>LearningParams</code></a></td><td style="text-align: left">-</td></tr><tr><td style="text-align: left"><code>arena</code></td><td style="text-align: left"><code>Union{Nothing, ArenaParams</code>}</td><td style="text-align: left">-</td></tr><tr><td style="text-align: left"><code>memory_analysis</code></td><td style="text-align: left"><code>Union{Nothing, MemAnalysisParams}</code></td><td style="text-align: left"><code>nothing</code></td></tr><tr><td style="text-align: left"><code>num_iters</code></td><td style="text-align: left"><code>Int</code></td><td style="text-align: left">-</td></tr><tr><td style="text-align: left"><code>use_symmetries</code></td><td style="text-align: left"><code>Bool</code></td><td style="text-align: left"><code>false</code></td></tr><tr><td style="text-align: left"><code>ternary_rewards</code></td><td style="text-align: left"><code>Bool</code></td><td style="text-align: left"><code>false</code></td></tr><tr><td style="text-align: left"><code>mem_buffer_size</code></td><td style="text-align: left"><code>PLSchedule{Int}</code></td><td style="text-align: left">-</td></tr></table><p><strong>Explanation</strong></p><p>The AlphaZero training process consists in <code>num_iters</code> iterations. Each iteration can be decomposed into a self-play phase (see <a href="#AlphaZero.SelfPlayParams"><code>SelfPlayParams</code></a>) and a learning phase (see <a href="#AlphaZero.LearningParams"><code>LearningParams</code></a>).</p><ul><li><code>ternary_rewards</code>: set to <code>true</code> if the rewards issued by  the game environment always belong to <span>$\{-1, 0, 1\}$</span> so that  the logging and profiling tools can take advantage of this property.</li><li><code>use_symmetries</code>: if set to <code>true</code>, board symmetries are used for  data augmentation before learning.</li><li><code>mem_buffer_size</code>: size schedule of the memory buffer, in terms of number  of samples. It is typical to start with a small memory buffer that is grown  progressively so as to wash out the initial low-quality self-play data  more quickly.</li><li><code>memory_analysis</code>: parameters for the memory analysis step that is  performed at each iteration (see <a href="#AlphaZero.MemAnalysisParams"><code>MemAnalysisParams</code></a>), or  <code>nothing</code> if no analysis is to be performed.</li></ul><p><strong>AlphaGo Zero Parameters</strong></p><p>In the original AlphaGo Zero paper:</p><ul><li>About 5 millions games of self-play are played across 200 iterations.</li><li>The memory buffer contains 500K games, which makes about 100M samples as an average game of Go lasts about 200 turns.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jonathan-laurent/AlphaZero.jl/blob/e639138a13d59202d35a363187f0146220edab02/src/params.jl#LL277-L319">source</a></section></article><h2 id="Self-Play"><a class="docs-heading-anchor" href="#Self-Play">Self-Play</a><a id="Self-Play-1"></a><a class="docs-heading-anchor-permalink" href="#Self-Play" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="AlphaZero.SelfPlayParams" href="#AlphaZero.SelfPlayParams"><code>AlphaZero.SelfPlayParams</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">SelfPlayParams</code></pre><p>Parameters governing self-play.</p><table><tr><th style="text-align: left">Parameter</th><th style="text-align: left">Type</th><th style="text-align: left">Default</th></tr><tr><td style="text-align: left"><code>mcts</code></td><td style="text-align: left"><a href="#AlphaZero.MctsParams"><code>MctsParams</code></a></td><td style="text-align: left">-</td></tr><tr><td style="text-align: left"><code>sim</code></td><td style="text-align: left"><a href="#AlphaZero.SimParams"><code>SimParams</code></a></td><td style="text-align: left">-</td></tr></table><p><strong>AlphaGo Zero Parameters</strong></p><p>In the original AlphaGo Zero paper, <code>sim.num_games=25_000</code> (5 millions games of self-play across 200 iterations).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jonathan-laurent/AlphaZero.jl/blob/e639138a13d59202d35a363187f0146220edab02/src/params.jl#LL145-L159">source</a></section></article><h2 id="Learning"><a class="docs-heading-anchor" href="#Learning">Learning</a><a id="Learning-1"></a><a class="docs-heading-anchor-permalink" href="#Learning" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="AlphaZero.LearningParams" href="#AlphaZero.LearningParams"><code>AlphaZero.LearningParams</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">LearningParams</code></pre><p>Parameters governing the learning phase of a training iteration, where the neural network is updated to fit the data in the memory buffer.</p><table><tr><th style="text-align: left">Parameter</th><th style="text-align: left">Type</th><th style="text-align: left">Default</th></tr><tr><td style="text-align: left"><code>use_gpu</code></td><td style="text-align: left"><code>Bool</code></td><td style="text-align: left"><code>false</code></td></tr><tr><td style="text-align: left"><code>use_position_averaging</code></td><td style="text-align: left"><code>Bool</code></td><td style="text-align: left"><code>true</code></td></tr><tr><td style="text-align: left"><code>samples_weighing_policy</code></td><td style="text-align: left"><a href="#AlphaZero.SamplesWeighingPolicy"><code>SamplesWeighingPolicy</code></a></td><td style="text-align: left">-</td></tr><tr><td style="text-align: left"><code>optimiser</code></td><td style="text-align: left"><a href="../network/#AlphaZero.Network.OptimiserSpec"><code>OptimiserSpec</code></a></td><td style="text-align: left">-</td></tr><tr><td style="text-align: left"><code>l2_regularization</code></td><td style="text-align: left"><code>Float32</code></td><td style="text-align: left">-</td></tr><tr><td style="text-align: left"><code>rewards_renormalization</code></td><td style="text-align: left"><code>Float32</code></td><td style="text-align: left"><code>1f0</code></td></tr><tr><td style="text-align: left"><code>nonvalidity_penalty</code></td><td style="text-align: left"><code>Float32</code></td><td style="text-align: left"><code>1f0</code></td></tr><tr><td style="text-align: left"><code>batch_size</code></td><td style="text-align: left"><code>Int</code></td><td style="text-align: left">-</td></tr><tr><td style="text-align: left"><code>loss_computation_batch_size</code></td><td style="text-align: left"><code>Int</code></td><td style="text-align: left">-</td></tr><tr><td style="text-align: left"><code>min_checkpoints_per_epoch</code></td><td style="text-align: left"><code>Float64</code></td><td style="text-align: left">-</td></tr><tr><td style="text-align: left"><code>max_batches_per_checkpoint</code></td><td style="text-align: left"><code>Int</code></td><td style="text-align: left">-</td></tr><tr><td style="text-align: left"><code>num_checkpoints</code></td><td style="text-align: left"><code>Int</code></td><td style="text-align: left">-</td></tr></table><p><strong>Description</strong></p><p>The neural network goes through <code>num_checkpoints</code> series of <code>n</code> updates using batches of size <code>batch_size</code> drawn from memory, where <code>n</code> is defined as follows:</p><pre><code class="language-none">n = min(max_batches_per_checkpoint, ntotal ÷ min_checkpoints_per_epoch)</code></pre><p>with <code>ntotal</code> the total number of batches in memory. Between each series, the current network is evaluated against the best network so far (see <a href="#AlphaZero.ArenaParams"><code>ArenaParams</code></a>).</p><ul><li><code>nonvalidity_penalty</code> is the multiplicative constant of a loss term that  corresponds to the average probability weight that the network puts on  invalid actions.</li><li><code>batch_size</code> is the batch size used for gradient descent.</li><li><code>loss_computation_batch_size</code> is the batch size that is used to compute the loss between each epochs.</li><li>All rewards are divided by <code>rewards_renormalization</code> before the MSE loss is computed.</li><li>If <code>use_position_averaging</code> is set to true, samples in memory that correspond to the same board position are averaged together. The merged sample is reweighted according to <code>samples_weighing_policy</code>.</li></ul><p><strong>AlphaGo Zero Parameters</strong></p><p>In the original AlphaGo Zero paper:</p><ul><li>The batch size for gradient updates is <span>$2048$</span>.</li><li>The L2 regularization parameter is set to <span>$10^{-4}$</span>.</li><li>Checkpoints are produced every 1000 training steps, which corresponds to seeing about 20% of the samples in the memory buffer: <span>$(1000 × 2048) / 10^7  ≈ 0.2$</span>.</li><li>It is unclear how many checkpoints are taken or how many training steps are performed in total.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jonathan-laurent/AlphaZero.jl/blob/e639138a13d59202d35a363187f0146220edab02/src/params.jl#LL179-L234">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AlphaZero.SamplesWeighingPolicy" href="#AlphaZero.SamplesWeighingPolicy"><code>AlphaZero.SamplesWeighingPolicy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">SamplesWeighingPolicy</code></pre><p>During self-play, early board positions are possibly encountered many times across several games. The corresponding samples can be merged together and given a weight <span>$W$</span> that is a nondecreasing function of the number <span>$n$</span> of merged samples:</p><ul><li><code>CONSTANT_WEIGHT</code>: <span>$W(n) = 1$</span></li><li><code>LOG_WEIGHT</code>: <span>$W(n) = \log_2(n) + 1$</span></li><li><code>LINEAR_WEIGHT</code>: <span>$W(n) = n$</span></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jonathan-laurent/AlphaZero.jl/blob/e639138a13d59202d35a363187f0146220edab02/src/params.jl#LL165-L176">source</a></section></article><h2 id="Arena"><a class="docs-heading-anchor" href="#Arena">Arena</a><a id="Arena-1"></a><a class="docs-heading-anchor-permalink" href="#Arena" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="AlphaZero.ArenaParams" href="#AlphaZero.ArenaParams"><code>AlphaZero.ArenaParams</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ArenaParams</code></pre><p>Parameters that govern the evaluation process that compares the current neural network with the best one seen so far (which is used to generate data).</p><table><tr><th style="text-align: left">Parameter</th><th style="text-align: left">Type</th><th style="text-align: left">Default</th></tr><tr><td style="text-align: left"><code>mcts</code></td><td style="text-align: left"><a href="#AlphaZero.MctsParams"><code>MctsParams</code></a></td><td style="text-align: left">-</td></tr><tr><td style="text-align: left"><code>sim</code></td><td style="text-align: left"><a href="#AlphaZero.SimParams"><code>SimParams</code></a></td><td style="text-align: left">-</td></tr><tr><td style="text-align: left"><code>update_threshold</code></td><td style="text-align: left"><code>Float64</code></td><td style="text-align: left">-</td></tr></table><p><strong>Explanation (two-player games)</strong></p><ul><li>The two competing networks are instantiated into two MCTS players of parameter <code>mcts</code> and then play <code>sim.num_games</code> games.</li><li>The evaluated network replaces the current best one if its average collected reward is greater or equal than <code>update_threshold</code>.</li></ul><p><strong>Explanation (single-player games)</strong></p><ul><li>The two competing networks play <code>sim.num_games</code> games each.</li><li>The evaluated network replaces the current best one if its average collected rewards exceeds the average collected reward of the old one by <code>update_threshold</code> at least. </li></ul><p><strong>Remarks</strong></p><ul><li>See <a href="#AlphaZero.necessary_samples"><code>necessary_samples</code></a> to make an informed choice for <code>sim.num_games</code>.</li></ul><p><strong>AlphaGo Zero Parameters</strong></p><p>In the original AlphaGo Zero paper, 400 games are played to evaluate a network and the <code>update_threshold</code> parameter is set to a value that corresponds to a 55% win rate.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jonathan-laurent/AlphaZero.jl/blob/e639138a13d59202d35a363187f0146220edab02/src/params.jl#LL103-L138">source</a></section></article><h2 id="Memory-Analysisautodocsautodocs"><a class="docs-heading-anchor" href="#Memory-Analysisautodocsautodocs">Memory Analysisautodocsautodocs</a><a id="Memory-Analysisautodocsautodocs-1"></a><a class="docs-heading-anchor-permalink" href="#Memory-Analysisautodocsautodocs" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="AlphaZero.MemAnalysisParams" href="#AlphaZero.MemAnalysisParams"><code>AlphaZero.MemAnalysisParams</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MemAnalysisParams</code></pre><p>Parameters governing the analysis of the memory buffer (for debugging and profiling purposes).</p><table><tr><th style="text-align: left">Parameter</th><th style="text-align: left">Type</th><th style="text-align: left">Default</th></tr><tr><td style="text-align: left"><code>num_game_stages</code></td><td style="text-align: left"><code>Int</code></td><td style="text-align: left">-</td></tr></table><p><strong>Explanation</strong></p><p>The memory analysis consists in partitioning the memory buffer in <code>num_game_stages</code> parts of equal size, according to the number of remaining moves until the end of the game for each sample. Then, the quality of the predictions of the current neural network is evaluated on each subset (see <a href="../reports/#AlphaZero.Report.Memory"><code>Report.Memory</code></a>).</p><p>This is useful to get an idea of how the neural network performance varies depending on the game stage (typically, good value estimates for endgame board positions are available earlier in the training process than good values for middlegame positions).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jonathan-laurent/AlphaZero.jl/blob/e639138a13d59202d35a363187f0146220edab02/src/params.jl#LL250-L272">source</a></section></article><h2 id="MCTS"><a class="docs-heading-anchor" href="#MCTS">MCTS</a><a id="MCTS-1"></a><a class="docs-heading-anchor-permalink" href="#MCTS" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="AlphaZero.MctsParams" href="#AlphaZero.MctsParams"><code>AlphaZero.MctsParams</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Parameters of an MCTS player.</p><table><tr><th style="text-align: left">Parameter</th><th style="text-align: left">Type</th><th style="text-align: left">Default</th></tr><tr><td style="text-align: left"><code>num_iters_per_turn</code></td><td style="text-align: left"><code>Int</code></td><td style="text-align: left">-</td></tr><tr><td style="text-align: left"><code>gamma</code></td><td style="text-align: left"><code>Float64</code></td><td style="text-align: left"><code>1.</code></td></tr><tr><td style="text-align: left"><code>cpuct</code></td><td style="text-align: left"><code>Float64</code></td><td style="text-align: left"><code>1.</code></td></tr><tr><td style="text-align: left"><code>temperature</code></td><td style="text-align: left"><code>AbstractSchedule{Float64}</code></td><td style="text-align: left"><code>ConstSchedule(1.)</code></td></tr><tr><td style="text-align: left"><code>dirichlet_noise_ϵ</code></td><td style="text-align: left"><code>Float64</code></td><td style="text-align: left">-</td></tr><tr><td style="text-align: left"><code>dirichlet_noise_α</code></td><td style="text-align: left"><code>Float64</code></td><td style="text-align: left">-</td></tr><tr><td style="text-align: left"><code>prior_temperature</code></td><td style="text-align: left"><code>Float64</code></td><td style="text-align: left"><code>1.</code></td></tr></table><p><strong>Explanation</strong></p><p>An MCTS player picks an action as follows. Given a game state, it launches <code>num_iters_per_turn</code> MCTS iterations, with UCT exploration constant <code>cpuct</code>. Rewards are discounted using the <code>gamma</code> factor.</p><p>Then, an action is picked according to the distribution <span>$π$</span> where <span>$π_i ∝ n_i^τ$</span> with <span>$n_i$</span> the number of times that the <span>$i^{\text{th}}$</span> action was visited and <span>$τ$</span> the <code>temperature</code> parameter.</p><p>It is typical to use a high value of the temperature parameter <span>$τ$</span> during the first moves of a game to increase exploration and then switch to a small value. Therefore, <code>temperature</code> is am <a href="#AlphaZero.AbstractSchedule"><code>AbstractSchedule</code></a>.</p><p>For information on parameters <code>cpuct</code>, <code>dirichlet_noise_ϵ</code>, <code>dirichlet_noise_α</code> and <code>prior_temperature</code>, see <a href="../mcts/#AlphaZero.MCTS.Env"><code>MCTS.Env</code></a>.</p><p><strong>AlphaGo Zero Parameters</strong></p><p>In the original AlphaGo Zero paper:</p><ul><li>The discount factor <code>gamma</code> is set to 1.</li><li>The number of MCTS iterations per move is 1600, which corresponds to 0.4s of computation time.</li><li>The temperature is set to 1 for the 30 first moves and then to an infinitesimal value.</li><li>The <span>$ϵ$</span> parameter for the Dirichlet noise is set to <span>$0.25$</span> and the <span>$α$</span> parameter to <span>$0.03$</span>, which is consistent with the heuristic of using <span>$α = 10/n$</span> with <span>$n$</span> the maximum number of possibles moves, which is <span>$19 × 19 + 1 = 362$</span> in the case of Go.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jonathan-laurent/AlphaZero.jl/blob/e639138a13d59202d35a363187f0146220edab02/src/params.jl#LL5-L48">source</a></section></article><h2 id="Simulations"><a class="docs-heading-anchor" href="#Simulations">Simulations</a><a id="Simulations-1"></a><a class="docs-heading-anchor-permalink" href="#Simulations" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="AlphaZero.SimParams" href="#AlphaZero.SimParams"><code>AlphaZero.SimParams</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">SimParams</code></pre><p>Parameters for parallel game simulations.</p><p>These parameters are common to self-play data generation, neural network evaluation and benchmarking.</p><table><tr><th style="text-align: left">Parameter</th><th style="text-align: left">Type</th><th style="text-align: left">Default</th></tr><tr><td style="text-align: left"><code>num_games</code></td><td style="text-align: left"><code>Int</code></td><td style="text-align: left">-</td></tr><tr><td style="text-align: left"><code>num_workers</code></td><td style="text-align: left"><code>Int</code></td><td style="text-align: left">-</td></tr><tr><td style="text-align: left"><code>batch_size</code></td><td style="text-align: left"><code>Int</code></td><td style="text-align: left">-</td></tr><tr><td style="text-align: left"><code>use_gpu</code></td><td style="text-align: left"><code>Bool</code></td><td style="text-align: left"><code>false</code></td></tr><tr><td style="text-align: left"><code>fill_batches</code></td><td style="text-align: left"><code>Bool</code></td><td style="text-align: left"><code>true</code></td></tr><tr><td style="text-align: left"><code>flip_probability</code></td><td style="text-align: left"><code>Float64</code></td><td style="text-align: left"><code>0.</code></td></tr><tr><td style="text-align: left"><code>reset_every</code></td><td style="text-align: left"><code>Union{Nothing, Int}</code></td><td style="text-align: left"><code>1</code></td></tr><tr><td style="text-align: left"><code>alternate_colors</code></td><td style="text-align: left"><code>Float64</code></td><td style="text-align: left"><code>false</code></td></tr></table><p><strong>Explanations</strong></p><ul><li>On each machine (process), <code>num_workers</code> simulation tasks are spawned. Inference requests are processed by an inference server by batch of size <code>batch_size</code>. Note that we must have <code>batch_size &lt;= num_workers</code>.</li><li>If <code>fill_batches</code> is set to <code>true</code>, we make sure that batches sent to the neural network for inference have constant size.</li><li>Both players are reset (e.g. their MCTS trees are emptied) every <code>reset_every</code> games (or never if <code>nothing</code> is passed).</li><li>To add randomization and before every game turn, the game board is &quot;flipped&quot; according to a symmetric transformation with probability <code>flip_probability</code>.</li><li>In the case of (symmetric) two-player games and if <code>alternate_colors</code> is set to<code>true</code>, then the colors of both players are swapped between each simulated game.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jonathan-laurent/AlphaZero.jl/blob/e639138a13d59202d35a363187f0146220edab02/src/params.jl#LL59-L91">source</a></section></article><h2 id="Utilities"><a class="docs-heading-anchor" href="#Utilities">Utilities</a><a id="Utilities-1"></a><a class="docs-heading-anchor-permalink" href="#Utilities" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="AlphaZero.necessary_samples" href="#AlphaZero.necessary_samples"><code>AlphaZero.necessary_samples</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">necessary_samples(ϵ, β) = log(1 / β) / (2 * ϵ^2)</code></pre><p>Compute the number of times <span>$N$</span> that a random variable <span>$X \sim \text{Ber}(p)$</span> has to be sampled so that if the empirical average of <span>$X$</span> is greather than <span>$1/2 + ϵ$</span>, then <span>$p &gt; 1/2$</span> with probability at least <span>$1-β$</span>.</p><p>This bound is based on <a href="https://en.wikipedia.org/wiki/Hoeffding%27s_inequality">Hoeffding&#39;s inequality </a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jonathan-laurent/AlphaZero.jl/blob/e639138a13d59202d35a363187f0146220edab02/src/params.jl#LL339-L349">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AlphaZero.AbstractSchedule" href="#AlphaZero.AbstractSchedule"><code>AlphaZero.AbstractSchedule</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">AbstractSchedule{R}</code></pre><p>Abstract type for a parameter schedule, which represents a function from nonnegative integers to numbers of type <code>R</code>. Subtypes must implement the <code>getindex(s::AbstractSchedule, i::Int)</code> operator.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jonathan-laurent/AlphaZero.jl/blob/e639138a13d59202d35a363187f0146220edab02/src/schedule.jl#LL5-L11">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AlphaZero.StepSchedule" href="#AlphaZero.StepSchedule"><code>AlphaZero.StepSchedule</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">StepSchedule{R} &lt;: AbstractSchedule{R}</code></pre><p>Type for step function schedules.</p><p><strong>Constructor</strong></p><pre><code class="language-none">StepSchedule(;start, change_at, values)</code></pre><p>Return a schedule that has initial value <code>start</code>. For all <code>i</code>, the schedule takes value <code>values[i]</code> at step <code>change_at[i]</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jonathan-laurent/AlphaZero.jl/blob/e639138a13d59202d35a363187f0146220edab02/src/schedule.jl#LL95-L106">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AlphaZero.PLSchedule" href="#AlphaZero.PLSchedule"><code>AlphaZero.PLSchedule</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">PLSchedule{R} &lt;: AbstractSchedule{R}</code></pre><p>Type for piecewise linear schedules.</p><p><strong>Constructors</strong></p><pre><code class="language-none">PLSchedule(cst)</code></pre><p>Return a schedule with a constant value <code>cst</code>.</p><pre><code class="language-none">PLSchedule(xs, ys)</code></pre><p>Return a piecewise linear schedule such that:</p><ul><li>For all <code>i</code>, <code>(xs[i], ys[i])</code> belongs to the schedule&#39;s graph.</li><li>Before <code>xs[1]</code>, the schedule has value <code>ys[1]</code>.</li><li>After <code>xs[end]</code>, the schedule has value <code>ys[end]</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jonathan-laurent/AlphaZero.jl/blob/e639138a13d59202d35a363187f0146220edab02/src/schedule.jl#LL31-L48">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AlphaZero.CyclicSchedule" href="#AlphaZero.CyclicSchedule"><code>AlphaZero.CyclicSchedule</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">CyclicSchedule(base, mid, term; n, xmid=0.45, xback=0.90)</code></pre><p>Return the <a href="#AlphaZero.PLSchedule"><code>PLSchedule</code></a> that is typically used for cyclic learning rate scheduling.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jonathan-laurent/AlphaZero.jl/blob/e639138a13d59202d35a363187f0146220edab02/src/schedule.jl#LL126-L131">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../tutorial/own_game/">« Solving Your Own Games</a><a class="docs-footer-nextpage" href="../game_interface/">Game Interface »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Tuesday 11 May 2021 01:02">Tuesday 11 May 2021</span>. Using Julia version 1.6.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
